{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. CNN construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(object):\n",
    "    def __init__(self, padding_size=500, embedding_size=100, file_path='matrix_Folder/', \n",
    "                 num_classes=5, filter_sizes = [2,3,4], num_filters = 128, dropout_keep_prob = 0.5):\n",
    "        \n",
    "        self.input_x = tf.placeholder(tf.float32, [None, padding_size, embedding_size, 1], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.test = tf.placeholder(tf.float32, [None, padding_size, embedding_size, 1], name=\"testing\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        \n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        pooled_outputs = []\n",
    "        \n",
    "        # for each filter size\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                # W is the filter matrix\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.input_x,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # provide non-linearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Max pooling\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, padding_size - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        print(pooled_outputs[0])\n",
    "\n",
    "        # concatenate all the outputs into a long features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs,3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "        \n",
    "        \n",
    "        # drop out\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # output \n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "           \n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            \n",
    "            #do wx + b matrix multiplication\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "        \n",
    "        # compute loss and accuracy\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Parameters Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "dropout_keep_prob = 0.5\n",
    "split_percentage = 0.01\n",
    "\n",
    "num_checkpoints = 100\n",
    "eval_interval  = 100\n",
    "checkpoint_interval= 100\n",
    "\n",
    "Xfile = \"matrix_Folder/\"\n",
    "Yfile = \"songs.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.helper functions\n",
    "Those functions are helper functions which are used in loading data and padding vectors to make sure their size be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# those are helper functions that are used in the next cell\n",
    "\n",
    "# 1.load x and load y\n",
    "def load_X(folder_path,nums_file=25000):\n",
    "    files = []\n",
    "    for i in range(nums_file):\n",
    "        filename = folder_path + \"matrix\" + str(i) + \".txt\"\n",
    "        files.append(filename)\n",
    "            \n",
    "    return files\n",
    "\n",
    "def load_Y(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    classes = list(df['genre'])\n",
    "    genres = sorted(list(set(classes)))\n",
    "    print(genres)\n",
    "    \n",
    "    res = []\n",
    "    for i, label in enumerate(classes):\n",
    "        index = genres.index(label)\n",
    "        label = [0] * len(genres)\n",
    "        label[index] = 1\n",
    "        res.append(label)\n",
    "        \n",
    "    return res\n",
    "\n",
    "# 2.padding to make each matrix has the same size(500)\n",
    "def padding0(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        lyrics = pickle.load(f)\n",
    "        \n",
    "        if len(lyrics) <= 500:\n",
    "            padding_num = 500 - len(lyrics)\n",
    "            for j in range(padding_num):\n",
    "                zero_row = np.zeros(100).tolist()\n",
    "                lyrics.append(zero_row)\n",
    "            return np.expand_dims(np.array(lyrics), axis=2)\n",
    "        else:\n",
    "            lyrics = lyrics[0:500]\n",
    "            return np.expand_dims(np.array(lyrics), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Country', 'Jazz', 'Metal', 'Pop', 'Rock']\n",
      "Train/Test: 24750/250\n"
     ]
    }
   ],
   "source": [
    "x = load_X(Xfile)  # x is a list of filenames\n",
    "y = load_Y(Yfile)  #shape: 25000 x 5\n",
    "\n",
    "#shuffle data and split data into train/test parts\n",
    "\n",
    "temp = list(zip(x, y))\n",
    "random.shuffle(temp)\n",
    "x_shuffled, y_shuffled = zip(*temp)\n",
    "\n",
    "split_index = -1 * int(split_percentage * float(len(y)))\n",
    "\n",
    "x_train, x_test = x_shuffled[:split_index], x_shuffled[split_index:]\n",
    "y_train, y_test = y_shuffled[:split_index], y_shuffled[split_index:]\n",
    "\n",
    "\n",
    "# padding the vector\n",
    "x_test = [padding0(file) for file in x_test]\n",
    "\n",
    "print(\"Train/Test: {:d}/{:d}\".format(len(y_train), len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# functions: \n",
    "#    for a single training step/ testing step\n",
    "#    and evaluate the loss and accuracy on a batch of data and updating the model parameters.\n",
    "\n",
    "def train_step(x_batch, y_batch):\n",
    "    input_dict = {\n",
    "      cnn.input_x: x_batch,\n",
    "      cnn.input_y: y_batch,\n",
    "      cnn.dropout_keep_prob: dropout_keep_prob\n",
    "    }\n",
    "    \n",
    "    _, step, summaries, loss, accuracy = sess.run(\n",
    "        [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy], input_dict)\n",
    "\n",
    "    cur_time = datetime.datetime.now().isoformat()\n",
    "    print(\"{}: step {}, loss {:g}, acc {:g}\".format(cur_time, step, loss, accuracy))\n",
    "    res = str(cur_time) + \", \" + str(step) + \", \" + str(loss) + \", \" + str(accuracy) + '\\n'\n",
    "    \n",
    "    file2.write(res)\n",
    "    train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "    \n",
    "def test_step(x_batch, y_batch, writer=None):\n",
    "    input_dict = {\n",
    "      cnn.input_x: x_batch,\n",
    "      cnn.input_y: y_batch,\n",
    "      cnn.dropout_keep_prob: 1.0\n",
    "    }\n",
    "    \n",
    "    step, summaries, loss, accuracy = sess.run(\n",
    "        [global_step, test_summary_op, cnn.loss, cnn.accuracy], input_dict)\n",
    "\n",
    "    cur_time = datetime.datetime.now().isoformat()\n",
    "    print(\"{}: step {}, loss {:g}, acc {:g}\".format(cur_time, step, loss, accuracy))\n",
    "    res = str(cur_time) + \", \" + str(step) + \", \" + str(loss) + \", \" + str(accuracy) + '\\n'\n",
    "    file1.write(res)\n",
    "    \n",
    "    if writer:\n",
    "        writer.add_summary(summaries, step)\n",
    "\n",
    "\n",
    "#  function:\n",
    "#  generate the batch\n",
    "def batch_iterator(data, batch_size, num_epochs, shuffle=True):\n",
    "   \n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int(data_size-1)/batch_size) + 1\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        if shuffle:\n",
    "            shuffled_data  = data\n",
    "            random.shuffle(shuffled_data)\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start = batch_num * batch_size\n",
    "            end = min((batch_num + 1) * batch_size, data_size)\n",
    "            \n",
    "            x_iter  = [padding0(pair[0]) for pair in shuffled_data[start:end]]\n",
    "            y_iter  = [pair[1] for pair in shuffled_data[start:end]]\n",
    "            \n",
    "            yield list(zip(x_iter, y_iter))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv-maxpool-2/pool:0\", shape=(?, 1, 1, 128), dtype=float32)\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/hist is illegal; using conv-maxpool-2/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/sparsity is illegal; using conv-maxpool-2/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/hist is illegal; using conv-maxpool-2/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/sparsity is illegal; using conv-maxpool-2/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\n",
      "\n",
      "2018-12-07T15:50:17.840698: step 1, loss 13.6608, acc 0.234375\n",
      "2018-12-07T15:50:19.225548: step 2, loss 12.2305, acc 0.15625\n",
      "2018-12-07T15:50:20.893674: step 3, loss 9.08574, acc 0.1875\n",
      "2018-12-07T15:50:22.260312: step 4, loss 12.1897, acc 0.09375\n",
      "2018-12-07T15:50:24.084913: step 5, loss 9.2256, acc 0.234375\n",
      "2018-12-07T15:50:25.484831: step 6, loss 10.0024, acc 0.203125\n",
      "2018-12-07T15:50:27.208259: step 7, loss 7.90092, acc 0.265625\n",
      "2018-12-07T15:50:28.619827: step 8, loss 9.69722, acc 0.171875\n",
      "2018-12-07T15:50:29.984003: step 9, loss 12.1846, acc 0.203125\n",
      "2018-12-07T15:50:31.421335: step 10, loss 9.75805, acc 0.1875\n",
      "2018-12-07T15:50:32.827259: step 11, loss 8.68209, acc 0.28125\n",
      "2018-12-07T15:50:34.591366: step 12, loss 11.7993, acc 0.171875\n",
      "2018-12-07T15:50:36.119140: step 13, loss 10.4588, acc 0.265625\n",
      "2018-12-07T15:50:38.168629: step 14, loss 9.38649, acc 0.28125\n",
      "2018-12-07T15:50:39.914021: step 15, loss 8.50327, acc 0.234375\n",
      "2018-12-07T15:50:41.561364: step 16, loss 8.92529, acc 0.21875\n",
      "2018-12-07T15:50:43.432064: step 17, loss 7.34538, acc 0.25\n",
      "2018-12-07T15:50:44.832515: step 18, loss 9.32638, acc 0.171875\n",
      "2018-12-07T15:50:46.662586: step 19, loss 9.38385, acc 0.25\n",
      "2018-12-07T15:50:48.010733: step 20, loss 9.04467, acc 0.234375\n",
      "2018-12-07T15:50:49.970767: step 21, loss 8.0203, acc 0.234375\n",
      "2018-12-07T15:50:51.589750: step 22, loss 10.4231, acc 0.1875\n",
      "2018-12-07T15:50:53.120134: step 23, loss 8.62198, acc 0.21875\n",
      "2018-12-07T15:50:54.520355: step 24, loss 8.76215, acc 0.234375\n",
      "2018-12-07T15:50:56.380268: step 25, loss 8.98119, acc 0.265625\n",
      "2018-12-07T15:50:57.969828: step 26, loss 7.96722, acc 0.28125\n",
      "2018-12-07T15:50:59.572981: step 27, loss 9.26091, acc 0.1875\n",
      "2018-12-07T15:51:01.069805: step 28, loss 8.73583, acc 0.171875\n",
      "2018-12-07T15:51:03.261335: step 29, loss 8.69587, acc 0.265625\n",
      "2018-12-07T15:51:05.279110: step 30, loss 7.96622, acc 0.203125\n",
      "2018-12-07T15:51:06.725211: step 31, loss 7.58374, acc 0.1875\n",
      "2018-12-07T15:51:08.589336: step 32, loss 7.36594, acc 0.21875\n",
      "2018-12-07T15:51:09.877226: step 33, loss 8.75344, acc 0.234375\n",
      "2018-12-07T15:51:11.360631: step 34, loss 6.50299, acc 0.296875\n",
      "2018-12-07T15:51:13.057404: step 35, loss 8.26125, acc 0.203125\n",
      "2018-12-07T15:51:14.530348: step 36, loss 8.88916, acc 0.203125\n",
      "2018-12-07T15:51:16.708145: step 37, loss 8.54687, acc 0.234375\n",
      "2018-12-07T15:51:18.604403: step 38, loss 7.89622, acc 0.25\n",
      "2018-12-07T15:51:20.807212: step 39, loss 8.33659, acc 0.140625\n",
      "2018-12-07T15:51:22.469730: step 40, loss 7.91705, acc 0.25\n",
      "2018-12-07T15:51:23.740160: step 41, loss 8.63544, acc 0.15625\n",
      "2018-12-07T15:51:25.081753: step 42, loss 7.84703, acc 0.203125\n",
      "2018-12-07T15:51:27.086911: step 43, loss 7.17391, acc 0.28125\n",
      "2018-12-07T15:51:28.550103: step 44, loss 8.17005, acc 0.140625\n",
      "2018-12-07T15:51:30.072747: step 45, loss 8.41567, acc 0.15625\n",
      "2018-12-07T15:51:31.553019: step 46, loss 6.89982, acc 0.28125\n",
      "2018-12-07T15:51:33.061660: step 47, loss 6.78441, acc 0.265625\n",
      "2018-12-07T15:51:34.466262: step 48, loss 7.60108, acc 0.234375\n",
      "2018-12-07T15:51:35.879141: step 49, loss 6.79855, acc 0.296875\n",
      "2018-12-07T15:51:37.297843: step 50, loss 6.6045, acc 0.203125\n",
      "2018-12-07T15:51:38.695414: step 51, loss 6.7398, acc 0.265625\n",
      "2018-12-07T15:51:40.646869: step 52, loss 5.59231, acc 0.203125\n",
      "2018-12-07T15:51:42.334069: step 53, loss 8.26327, acc 0.234375\n",
      "2018-12-07T15:51:43.759271: step 54, loss 8.33871, acc 0.1875\n",
      "2018-12-07T15:51:45.041134: step 55, loss 6.09261, acc 0.3125\n",
      "2018-12-07T15:51:46.686623: step 56, loss 8.39747, acc 0.125\n",
      "2018-12-07T15:51:48.109581: step 57, loss 6.72551, acc 0.234375\n",
      "2018-12-07T15:51:49.431593: step 58, loss 6.0923, acc 0.28125\n",
      "2018-12-07T15:51:50.938648: step 59, loss 7.80904, acc 0.21875\n",
      "2018-12-07T15:51:52.974653: step 60, loss 5.54571, acc 0.265625\n",
      "2018-12-07T15:51:54.680852: step 61, loss 7.28888, acc 0.203125\n",
      "2018-12-07T15:51:56.203195: step 62, loss 6.47156, acc 0.3125\n",
      "2018-12-07T15:51:57.675679: step 63, loss 7.0526, acc 0.203125\n",
      "2018-12-07T15:51:59.245099: step 64, loss 5.63115, acc 0.296875\n",
      "2018-12-07T15:52:01.080423: step 65, loss 7.64181, acc 0.203125\n",
      "2018-12-07T15:52:02.744538: step 66, loss 6.84991, acc 0.203125\n",
      "2018-12-07T15:52:04.136789: step 67, loss 6.28497, acc 0.203125\n",
      "2018-12-07T15:52:05.915542: step 68, loss 6.92944, acc 0.234375\n",
      "2018-12-07T15:52:07.734215: step 69, loss 7.08736, acc 0.1875\n",
      "2018-12-07T15:52:09.317184: step 70, loss 5.91622, acc 0.328125\n",
      "2018-12-07T15:52:11.141088: step 71, loss 6.37295, acc 0.25\n",
      "2018-12-07T15:52:12.850259: step 72, loss 7.49012, acc 0.21875\n",
      "2018-12-07T15:52:14.694133: step 73, loss 6.53247, acc 0.28125\n",
      "2018-12-07T15:52:16.125521: step 74, loss 7.00501, acc 0.265625\n",
      "2018-12-07T15:52:17.759070: step 75, loss 5.19659, acc 0.375\n",
      "2018-12-07T15:52:20.013532: step 76, loss 6.10718, acc 0.21875\n",
      "2018-12-07T15:52:22.036668: step 77, loss 7.1863, acc 0.21875\n",
      "2018-12-07T15:52:23.686681: step 78, loss 6.1287, acc 0.234375\n",
      "2018-12-07T15:52:25.063444: step 79, loss 6.13032, acc 0.21875\n",
      "2018-12-07T15:52:26.790525: step 80, loss 5.83694, acc 0.21875\n",
      "2018-12-07T15:52:28.286899: step 81, loss 6.01067, acc 0.21875\n",
      "2018-12-07T15:52:31.107301: step 82, loss 6.75583, acc 0.21875\n",
      "2018-12-07T15:52:32.726862: step 83, loss 5.27095, acc 0.234375\n",
      "2018-12-07T15:52:35.175110: step 84, loss 5.66496, acc 0.25\n",
      "2018-12-07T15:52:36.856432: step 85, loss 5.9742, acc 0.1875\n",
      "2018-12-07T15:52:38.409536: step 86, loss 6.08794, acc 0.28125\n",
      "2018-12-07T15:52:40.112273: step 87, loss 4.85602, acc 0.34375\n",
      "2018-12-07T15:52:42.157924: step 88, loss 6.5106, acc 0.15625\n",
      "2018-12-07T15:52:43.664692: step 89, loss 4.74002, acc 0.34375\n",
      "2018-12-07T15:52:45.245507: step 90, loss 6.52764, acc 0.171875\n",
      "2018-12-07T15:52:47.354114: step 91, loss 6.58271, acc 0.21875\n",
      "2018-12-07T15:52:49.438359: step 92, loss 5.77085, acc 0.265625\n",
      "2018-12-07T15:52:51.102815: step 93, loss 5.72555, acc 0.28125\n",
      "2018-12-07T15:52:52.822677: step 94, loss 5.28493, acc 0.3125\n",
      "2018-12-07T15:52:55.289308: step 95, loss 6.42968, acc 0.171875\n",
      "2018-12-07T15:52:57.365567: step 96, loss 5.50922, acc 0.3125\n",
      "2018-12-07T15:52:59.142123: step 97, loss 5.9187, acc 0.25\n",
      "2018-12-07T15:53:00.776945: step 98, loss 6.221, acc 0.3125\n",
      "2018-12-07T15:53:03.019067: step 99, loss 5.66408, acc 0.28125\n",
      "2018-12-07T15:53:04.548322: step 100, loss 4.34301, acc 0.3125\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07T15:53:05.877766: step 100, loss 2.04039, acc 0.36\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-100\n",
      "\n",
      "2018-12-07T15:53:08.649570: step 101, loss 4.13138, acc 0.375\n",
      "2018-12-07T15:53:10.305376: step 102, loss 5.61799, acc 0.359375\n",
      "2018-12-07T15:53:11.818057: step 103, loss 5.14301, acc 0.234375\n",
      "2018-12-07T15:53:13.689765: step 104, loss 4.76927, acc 0.296875\n",
      "2018-12-07T15:53:15.267473: step 105, loss 5.49963, acc 0.203125\n",
      "2018-12-07T15:53:17.818065: step 106, loss 4.46147, acc 0.390625\n",
      "2018-12-07T15:53:19.674755: step 107, loss 5.41682, acc 0.234375\n",
      "2018-12-07T15:53:21.372448: step 108, loss 4.96335, acc 0.265625\n",
      "2018-12-07T15:53:22.894179: step 109, loss 4.03028, acc 0.359375\n",
      "2018-12-07T15:53:24.655535: step 110, loss 5.52626, acc 0.21875\n",
      "2018-12-07T15:53:26.219290: step 111, loss 4.36683, acc 0.328125\n",
      "2018-12-07T15:53:27.868826: step 112, loss 5.33858, acc 0.234375\n",
      "2018-12-07T15:53:29.677690: step 113, loss 4.44745, acc 0.265625\n",
      "2018-12-07T15:53:31.196174: step 114, loss 4.27662, acc 0.3125\n",
      "2018-12-07T15:53:32.751139: step 115, loss 5.05908, acc 0.1875\n",
      "2018-12-07T15:53:34.473545: step 116, loss 4.806, acc 0.28125\n",
      "2018-12-07T15:53:36.262136: step 117, loss 5.56918, acc 0.25\n",
      "2018-12-07T15:53:37.923912: step 118, loss 5.19262, acc 0.25\n",
      "2018-12-07T15:53:39.566419: step 119, loss 4.93018, acc 0.296875\n",
      "2018-12-07T15:53:41.310294: step 120, loss 5.81956, acc 0.265625\n",
      "2018-12-07T15:53:42.999323: step 121, loss 5.0287, acc 0.21875\n",
      "2018-12-07T15:53:44.878909: step 122, loss 5.93807, acc 0.1875\n",
      "2018-12-07T15:53:46.309578: step 123, loss 4.26238, acc 0.390625\n",
      "2018-12-07T15:53:48.202866: step 124, loss 5.00148, acc 0.375\n",
      "2018-12-07T15:53:49.877191: step 125, loss 4.37623, acc 0.296875\n",
      "2018-12-07T15:53:51.511322: step 126, loss 5.18849, acc 0.265625\n",
      "2018-12-07T15:53:53.017792: step 127, loss 4.78081, acc 0.25\n",
      "2018-12-07T15:53:55.058003: step 128, loss 3.89192, acc 0.34375\n",
      "2018-12-07T15:53:56.694465: step 129, loss 4.695, acc 0.265625\n",
      "2018-12-07T15:53:58.575781: step 130, loss 6.19611, acc 0.21875\n",
      "2018-12-07T15:54:00.312785: step 131, loss 3.79125, acc 0.375\n",
      "2018-12-07T15:54:02.100674: step 132, loss 4.54519, acc 0.359375\n",
      "2018-12-07T15:54:04.167774: step 133, loss 4.23505, acc 0.28125\n",
      "2018-12-07T15:54:05.937972: step 134, loss 5.45385, acc 0.265625\n",
      "2018-12-07T15:54:07.725042: step 135, loss 5.39368, acc 0.21875\n",
      "2018-12-07T15:54:09.319056: step 136, loss 5.08004, acc 0.265625\n",
      "2018-12-07T15:54:11.144742: step 137, loss 4.88586, acc 0.34375\n",
      "2018-12-07T15:54:12.964200: step 138, loss 4.51591, acc 0.296875\n",
      "2018-12-07T15:54:15.305781: step 139, loss 3.95706, acc 0.40625\n",
      "2018-12-07T15:54:17.193829: step 140, loss 4.82166, acc 0.28125\n",
      "2018-12-07T15:54:18.961393: step 141, loss 5.15083, acc 0.21875\n",
      "2018-12-07T15:54:20.719223: step 142, loss 5.57978, acc 0.21875\n",
      "2018-12-07T15:54:22.509664: step 143, loss 5.28011, acc 0.25\n",
      "2018-12-07T15:54:23.945542: step 144, loss 4.70303, acc 0.203125\n",
      "2018-12-07T15:54:25.787499: step 145, loss 4.98702, acc 0.234375\n",
      "2018-12-07T15:54:27.638126: step 146, loss 4.61133, acc 0.3125\n",
      "2018-12-07T15:54:29.436967: step 147, loss 4.69841, acc 0.203125\n",
      "2018-12-07T15:54:31.745389: step 148, loss 4.88717, acc 0.265625\n",
      "2018-12-07T15:54:33.674707: step 149, loss 4.52786, acc 0.34375\n",
      "2018-12-07T15:54:35.379316: step 150, loss 5.16323, acc 0.203125\n",
      "2018-12-07T15:54:37.017682: step 151, loss 5.16448, acc 0.296875\n",
      "2018-12-07T15:54:38.771521: step 152, loss 4.76241, acc 0.203125\n",
      "2018-12-07T15:54:40.404721: step 153, loss 3.50995, acc 0.375\n",
      "2018-12-07T15:54:42.310924: step 154, loss 4.76184, acc 0.265625\n",
      "2018-12-07T15:54:43.962640: step 155, loss 4.06652, acc 0.34375\n",
      "2018-12-07T15:54:45.684449: step 156, loss 5.12187, acc 0.25\n",
      "2018-12-07T15:54:47.249043: step 157, loss 4.34171, acc 0.25\n",
      "2018-12-07T15:54:48.970748: step 158, loss 5.16961, acc 0.25\n",
      "2018-12-07T15:54:50.665092: step 159, loss 4.5456, acc 0.296875\n",
      "2018-12-07T15:54:52.137994: step 160, loss 4.38131, acc 0.203125\n",
      "2018-12-07T15:54:53.898003: step 161, loss 4.87209, acc 0.25\n",
      "2018-12-07T15:54:55.393598: step 162, loss 3.97665, acc 0.28125\n",
      "2018-12-07T15:54:57.458442: step 163, loss 3.40324, acc 0.359375\n",
      "2018-12-07T15:54:59.321605: step 164, loss 4.24056, acc 0.25\n",
      "2018-12-07T15:55:01.191214: step 165, loss 3.99645, acc 0.28125\n",
      "2018-12-07T15:55:03.210484: step 166, loss 4.19717, acc 0.25\n",
      "2018-12-07T15:55:04.893967: step 167, loss 4.2353, acc 0.296875\n",
      "2018-12-07T15:55:07.291577: step 168, loss 3.802, acc 0.328125\n",
      "2018-12-07T15:55:08.840193: step 169, loss 4.23859, acc 0.265625\n",
      "2018-12-07T15:55:10.618862: step 170, loss 4.30971, acc 0.28125\n",
      "2018-12-07T15:55:12.259837: step 171, loss 3.08011, acc 0.3125\n",
      "2018-12-07T15:55:14.306438: step 172, loss 4.95527, acc 0.34375\n",
      "2018-12-07T15:55:15.973354: step 173, loss 3.71029, acc 0.265625\n",
      "2018-12-07T15:55:17.716249: step 174, loss 5.12545, acc 0.140625\n",
      "2018-12-07T15:55:19.358247: step 175, loss 4.36421, acc 0.28125\n",
      "2018-12-07T15:55:21.549665: step 176, loss 3.8928, acc 0.234375\n",
      "2018-12-07T15:55:23.521887: step 177, loss 4.30869, acc 0.25\n",
      "2018-12-07T15:55:25.162043: step 178, loss 3.4186, acc 0.265625\n",
      "2018-12-07T15:55:26.896975: step 179, loss 4.06792, acc 0.328125\n",
      "2018-12-07T15:55:28.537721: step 180, loss 3.43706, acc 0.375\n",
      "2018-12-07T15:55:30.194918: step 181, loss 3.94404, acc 0.234375\n",
      "2018-12-07T15:55:31.986925: step 182, loss 3.07346, acc 0.40625\n",
      "2018-12-07T15:55:33.513620: step 183, loss 3.43952, acc 0.34375\n",
      "2018-12-07T15:55:35.426451: step 184, loss 3.22317, acc 0.28125\n",
      "2018-12-07T15:55:37.013144: step 185, loss 4.38391, acc 0.296875\n",
      "2018-12-07T15:55:38.505903: step 186, loss 3.9414, acc 0.3125\n",
      "2018-12-07T15:55:40.486806: step 187, loss 3.54534, acc 0.390625\n",
      "2018-12-07T15:55:42.212097: step 188, loss 3.72151, acc 0.359375\n",
      "2018-12-07T15:55:43.806045: step 189, loss 4.04045, acc 0.3125\n",
      "2018-12-07T15:55:45.675975: step 190, loss 3.34086, acc 0.3125\n",
      "2018-12-07T15:55:48.048606: step 191, loss 3.86667, acc 0.328125\n",
      "2018-12-07T15:55:49.790377: step 192, loss 3.22667, acc 0.28125\n",
      "2018-12-07T15:55:51.590801: step 193, loss 3.99257, acc 0.21875\n",
      "2018-12-07T15:55:53.698820: step 194, loss 3.61567, acc 0.296875\n",
      "2018-12-07T15:55:55.360769: step 195, loss 4.44749, acc 0.265625\n",
      "2018-12-07T15:55:57.316454: step 196, loss 3.68347, acc 0.3125\n",
      "2018-12-07T15:55:58.909534: step 197, loss 3.92712, acc 0.296875\n",
      "2018-12-07T15:56:00.793085: step 198, loss 4.09336, acc 0.359375\n",
      "2018-12-07T15:56:02.258497: step 199, loss 4.2517, acc 0.265625\n",
      "2018-12-07T15:56:03.832956: step 200, loss 4.33026, acc 0.28125\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T15:56:05.119517: step 200, loss 1.48564, acc 0.468\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-200\n",
      "\n",
      "2018-12-07T15:56:07.221767: step 201, loss 3.73452, acc 0.34375\n",
      "2018-12-07T15:56:08.821713: step 202, loss 3.01004, acc 0.359375\n",
      "2018-12-07T15:56:10.549074: step 203, loss 4.21022, acc 0.328125\n",
      "2018-12-07T15:56:12.174673: step 204, loss 3.59526, acc 0.3125\n",
      "2018-12-07T15:56:14.322311: step 205, loss 4.02091, acc 0.296875\n",
      "2018-12-07T15:56:16.150806: step 206, loss 4.17384, acc 0.203125\n",
      "2018-12-07T15:56:17.758296: step 207, loss 4.25065, acc 0.296875\n",
      "2018-12-07T15:56:19.163489: step 208, loss 3.19729, acc 0.3125\n",
      "2018-12-07T15:56:20.770839: step 209, loss 3.70809, acc 0.21875\n",
      "2018-12-07T15:56:22.424446: step 210, loss 3.53931, acc 0.25\n",
      "2018-12-07T15:56:23.886726: step 211, loss 3.6045, acc 0.28125\n",
      "2018-12-07T15:56:25.301796: step 212, loss 3.2451, acc 0.390625\n",
      "2018-12-07T15:56:27.437201: step 213, loss 3.59471, acc 0.40625\n",
      "2018-12-07T15:56:29.578137: step 214, loss 3.44684, acc 0.265625\n",
      "2018-12-07T15:56:31.538956: step 215, loss 3.55597, acc 0.265625\n",
      "2018-12-07T15:56:34.538530: step 216, loss 4.18057, acc 0.28125\n",
      "2018-12-07T15:56:37.246863: step 217, loss 3.74024, acc 0.28125\n",
      "2018-12-07T15:56:39.921883: step 218, loss 4.73743, acc 0.234375\n",
      "2018-12-07T15:56:41.630664: step 219, loss 3.57312, acc 0.328125\n",
      "2018-12-07T15:56:43.257900: step 220, loss 2.29935, acc 0.421875\n",
      "2018-12-07T15:56:45.182810: step 221, loss 3.04415, acc 0.390625\n",
      "2018-12-07T15:56:46.761199: step 222, loss 3.10596, acc 0.375\n",
      "2018-12-07T15:56:48.432437: step 223, loss 2.9841, acc 0.359375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07T15:56:50.192240: step 224, loss 3.58898, acc 0.25\n",
      "2018-12-07T15:56:51.872007: step 225, loss 3.12353, acc 0.296875\n",
      "2018-12-07T15:56:54.033841: step 226, loss 2.93204, acc 0.390625\n",
      "2018-12-07T15:56:56.737724: step 227, loss 3.20421, acc 0.359375\n",
      "2018-12-07T15:56:58.749629: step 228, loss 3.90719, acc 0.21875\n",
      "2018-12-07T15:57:00.861211: step 229, loss 3.43613, acc 0.359375\n",
      "2018-12-07T15:57:02.896284: step 230, loss 3.59089, acc 0.34375\n",
      "2018-12-07T15:57:05.063214: step 231, loss 3.46645, acc 0.265625\n",
      "2018-12-07T15:57:07.589879: step 232, loss 3.13217, acc 0.328125\n",
      "2018-12-07T15:57:09.301558: step 233, loss 2.79403, acc 0.359375\n",
      "2018-12-07T15:57:11.780341: step 234, loss 3.19096, acc 0.28125\n",
      "2018-12-07T15:57:14.128132: step 235, loss 3.61232, acc 0.328125\n",
      "2018-12-07T15:57:15.822272: step 236, loss 3.21901, acc 0.34375\n",
      "2018-12-07T15:57:17.707017: step 237, loss 2.96953, acc 0.40625\n",
      "2018-12-07T15:57:19.950211: step 238, loss 2.69022, acc 0.40625\n",
      "2018-12-07T15:57:22.142354: step 239, loss 2.60315, acc 0.4375\n",
      "2018-12-07T15:57:23.722436: step 240, loss 3.00694, acc 0.296875\n",
      "2018-12-07T15:57:25.584066: step 241, loss 3.54069, acc 0.296875\n",
      "2018-12-07T15:57:27.281450: step 242, loss 2.42473, acc 0.4375\n",
      "2018-12-07T15:57:29.574669: step 243, loss 2.78074, acc 0.34375\n",
      "2018-12-07T15:57:31.695714: step 244, loss 3.27111, acc 0.234375\n",
      "2018-12-07T15:57:33.775245: step 245, loss 2.33212, acc 0.421875\n",
      "2018-12-07T15:57:36.102760: step 246, loss 3.42004, acc 0.28125\n",
      "2018-12-07T15:57:37.906586: step 247, loss 3.07622, acc 0.296875\n",
      "2018-12-07T15:57:40.062204: step 248, loss 3.29963, acc 0.234375\n",
      "2018-12-07T15:57:42.000384: step 249, loss 3.03298, acc 0.203125\n",
      "2018-12-07T15:57:43.576979: step 250, loss 2.77897, acc 0.359375\n",
      "2018-12-07T15:57:45.166613: step 251, loss 2.83589, acc 0.390625\n",
      "2018-12-07T15:57:46.793789: step 252, loss 3.42678, acc 0.28125\n",
      "2018-12-07T15:57:48.633418: step 253, loss 3.52363, acc 0.296875\n",
      "2018-12-07T15:57:50.615196: step 254, loss 2.41105, acc 0.375\n",
      "2018-12-07T15:57:52.771281: step 255, loss 2.75401, acc 0.328125\n",
      "2018-12-07T15:57:54.609519: step 256, loss 2.60236, acc 0.390625\n",
      "2018-12-07T15:57:56.117340: step 257, loss 3.56649, acc 0.3125\n",
      "2018-12-07T15:57:57.821757: step 258, loss 3.28312, acc 0.359375\n",
      "2018-12-07T15:57:59.373512: step 259, loss 3.17086, acc 0.296875\n",
      "2018-12-07T15:58:01.265597: step 260, loss 3.84477, acc 0.234375\n",
      "2018-12-07T15:58:02.958534: step 261, loss 3.62126, acc 0.28125\n",
      "2018-12-07T15:58:04.614777: step 262, loss 3.14525, acc 0.25\n",
      "2018-12-07T15:58:06.469694: step 263, loss 3.48977, acc 0.25\n",
      "2018-12-07T15:58:08.087767: step 264, loss 3.39044, acc 0.296875\n",
      "2018-12-07T15:58:09.509941: step 265, loss 3.00201, acc 0.296875\n",
      "2018-12-07T15:58:12.061628: step 266, loss 3.19007, acc 0.28125\n",
      "2018-12-07T15:58:13.574468: step 267, loss 2.61502, acc 0.34375\n",
      "2018-12-07T15:58:14.978818: step 268, loss 3.03611, acc 0.328125\n",
      "2018-12-07T15:58:16.612850: step 269, loss 2.47637, acc 0.359375\n",
      "2018-12-07T15:58:18.111481: step 270, loss 3.78011, acc 0.3125\n",
      "2018-12-07T15:58:19.898317: step 271, loss 2.15555, acc 0.609375\n",
      "2018-12-07T15:58:21.648363: step 272, loss 2.64884, acc 0.359375\n",
      "2018-12-07T15:58:23.167175: step 273, loss 2.92475, acc 0.28125\n",
      "2018-12-07T15:58:24.582034: step 274, loss 3.78176, acc 0.3125\n",
      "2018-12-07T15:58:26.259562: step 275, loss 3.08504, acc 0.28125\n",
      "2018-12-07T15:58:28.604865: step 276, loss 2.71803, acc 0.359375\n",
      "2018-12-07T15:58:30.610307: step 277, loss 3.44407, acc 0.328125\n",
      "2018-12-07T15:58:32.601942: step 278, loss 3.16186, acc 0.28125\n",
      "2018-12-07T15:58:34.811967: step 279, loss 3.23902, acc 0.28125\n",
      "2018-12-07T15:58:36.505918: step 280, loss 2.65683, acc 0.390625\n",
      "2018-12-07T15:58:38.056406: step 281, loss 2.25842, acc 0.390625\n",
      "2018-12-07T15:58:40.081994: step 282, loss 3.30435, acc 0.21875\n",
      "2018-12-07T15:58:42.000735: step 283, loss 3.22615, acc 0.28125\n",
      "2018-12-07T15:58:43.506308: step 284, loss 2.37119, acc 0.390625\n",
      "2018-12-07T15:58:45.155953: step 285, loss 2.72672, acc 0.34375\n",
      "2018-12-07T15:58:47.296596: step 286, loss 3.31043, acc 0.265625\n",
      "2018-12-07T15:58:49.070453: step 287, loss 2.909, acc 0.296875\n",
      "2018-12-07T15:58:50.521562: step 288, loss 3.41946, acc 0.265625\n",
      "2018-12-07T15:58:52.409918: step 289, loss 2.69174, acc 0.328125\n",
      "2018-12-07T15:58:53.913356: step 290, loss 3.11254, acc 0.25\n",
      "2018-12-07T15:58:55.849692: step 291, loss 3.11827, acc 0.265625\n",
      "2018-12-07T15:58:58.452280: step 292, loss 2.60567, acc 0.375\n",
      "2018-12-07T15:59:00.268841: step 293, loss 2.5079, acc 0.359375\n",
      "2018-12-07T15:59:02.233741: step 294, loss 2.39784, acc 0.328125\n",
      "2018-12-07T15:59:04.445376: step 295, loss 2.58432, acc 0.3125\n",
      "2018-12-07T15:59:06.277594: step 296, loss 2.47741, acc 0.375\n",
      "2018-12-07T15:59:08.050913: step 297, loss 2.44447, acc 0.390625\n",
      "2018-12-07T15:59:09.632122: step 298, loss 2.45209, acc 0.296875\n",
      "2018-12-07T15:59:11.356521: step 299, loss 2.44194, acc 0.390625\n",
      "2018-12-07T15:59:13.014265: step 300, loss 2.89381, acc 0.328125\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T15:59:14.227025: step 300, loss 1.25291, acc 0.556\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-300\n",
      "\n",
      "2018-12-07T15:59:16.430903: step 301, loss 1.95669, acc 0.5\n",
      "2018-12-07T15:59:18.601422: step 302, loss 2.67669, acc 0.34375\n",
      "2018-12-07T15:59:20.989619: step 303, loss 2.27693, acc 0.390625\n",
      "2018-12-07T15:59:22.536221: step 304, loss 2.46589, acc 0.375\n",
      "2018-12-07T15:59:24.210888: step 305, loss 2.52208, acc 0.328125\n",
      "2018-12-07T15:59:25.863337: step 306, loss 2.68919, acc 0.265625\n",
      "2018-12-07T15:59:28.004259: step 307, loss 2.13702, acc 0.40625\n",
      "2018-12-07T15:59:29.699673: step 308, loss 2.33988, acc 0.4375\n",
      "2018-12-07T15:59:31.227813: step 309, loss 2.83979, acc 0.34375\n",
      "2018-12-07T15:59:32.686730: step 310, loss 2.7758, acc 0.328125\n",
      "2018-12-07T15:59:34.245832: step 311, loss 2.86477, acc 0.265625\n",
      "2018-12-07T15:59:35.944614: step 312, loss 2.84016, acc 0.3125\n",
      "2018-12-07T15:59:37.752825: step 313, loss 2.95588, acc 0.265625\n",
      "2018-12-07T15:59:39.728027: step 314, loss 2.68161, acc 0.359375\n",
      "2018-12-07T15:59:41.475129: step 315, loss 2.86208, acc 0.40625\n",
      "2018-12-07T15:59:43.553155: step 316, loss 2.99011, acc 0.25\n",
      "2018-12-07T15:59:45.510255: step 317, loss 2.52655, acc 0.375\n",
      "2018-12-07T15:59:47.100758: step 318, loss 2.67019, acc 0.265625\n",
      "2018-12-07T15:59:48.620214: step 319, loss 2.99181, acc 0.28125\n",
      "2018-12-07T15:59:50.386925: step 320, loss 2.88227, acc 0.328125\n",
      "2018-12-07T15:59:51.801631: step 321, loss 2.5332, acc 0.296875\n",
      "2018-12-07T15:59:53.318633: step 322, loss 3.06925, acc 0.265625\n",
      "2018-12-07T15:59:54.830059: step 323, loss 2.38426, acc 0.390625\n",
      "2018-12-07T15:59:56.379996: step 324, loss 2.4075, acc 0.265625\n",
      "2018-12-07T15:59:58.577997: step 325, loss 2.09762, acc 0.390625\n",
      "2018-12-07T16:00:00.054362: step 326, loss 2.72012, acc 0.328125\n",
      "2018-12-07T16:00:01.553887: step 327, loss 2.50861, acc 0.34375\n",
      "2018-12-07T16:00:03.255572: step 328, loss 2.40422, acc 0.25\n",
      "2018-12-07T16:00:05.300162: step 329, loss 2.77291, acc 0.328125\n",
      "2018-12-07T16:00:06.868803: step 330, loss 2.46242, acc 0.359375\n",
      "2018-12-07T16:00:08.524104: step 331, loss 2.78764, acc 0.21875\n",
      "2018-12-07T16:00:09.999873: step 332, loss 2.26225, acc 0.359375\n",
      "2018-12-07T16:00:11.855343: step 333, loss 2.85126, acc 0.3125\n",
      "2018-12-07T16:00:13.546225: step 334, loss 3.18851, acc 0.28125\n",
      "2018-12-07T16:00:15.080273: step 335, loss 2.38566, acc 0.34375\n",
      "2018-12-07T16:00:17.001869: step 336, loss 2.42376, acc 0.296875\n",
      "2018-12-07T16:00:19.152163: step 337, loss 2.23516, acc 0.375\n",
      "2018-12-07T16:00:20.918700: step 338, loss 1.89168, acc 0.390625\n",
      "2018-12-07T16:00:22.604884: step 339, loss 2.40296, acc 0.328125\n",
      "2018-12-07T16:00:24.398863: step 340, loss 2.1948, acc 0.359375\n",
      "2018-12-07T16:00:26.255036: step 341, loss 2.35032, acc 0.328125\n",
      "2018-12-07T16:00:28.011497: step 342, loss 2.69199, acc 0.234375\n",
      "2018-12-07T16:00:29.548330: step 343, loss 2.44227, acc 0.296875\n",
      "2018-12-07T16:00:31.229889: step 344, loss 2.47417, acc 0.34375\n",
      "2018-12-07T16:00:33.582870: step 345, loss 2.01337, acc 0.34375\n",
      "2018-12-07T16:00:35.217400: step 346, loss 2.40821, acc 0.375\n",
      "2018-12-07T16:00:36.787374: step 347, loss 2.4502, acc 0.3125\n",
      "2018-12-07T16:00:38.630810: step 348, loss 2.7569, acc 0.296875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07T16:00:40.924849: step 349, loss 2.46542, acc 0.3125\n",
      "2018-12-07T16:00:42.844108: step 350, loss 1.99108, acc 0.390625\n",
      "2018-12-07T16:00:44.533297: step 351, loss 2.18711, acc 0.4375\n",
      "2018-12-07T16:00:46.479538: step 352, loss 1.97345, acc 0.359375\n",
      "2018-12-07T16:00:48.260548: step 353, loss 2.42396, acc 0.375\n",
      "2018-12-07T16:00:50.146562: step 354, loss 1.9724, acc 0.421875\n",
      "2018-12-07T16:00:51.817771: step 355, loss 2.12736, acc 0.328125\n",
      "2018-12-07T16:00:53.513977: step 356, loss 2.33985, acc 0.34375\n",
      "2018-12-07T16:00:55.262827: step 357, loss 2.43565, acc 0.3125\n",
      "2018-12-07T16:00:56.689519: step 358, loss 2.06174, acc 0.421875\n",
      "2018-12-07T16:00:58.823877: step 359, loss 2.58235, acc 0.328125\n",
      "2018-12-07T16:01:00.913892: step 360, loss 2.41188, acc 0.40625\n",
      "2018-12-07T16:01:02.746990: step 361, loss 2.42759, acc 0.328125\n",
      "2018-12-07T16:01:04.344005: step 362, loss 2.43725, acc 0.34375\n",
      "2018-12-07T16:01:05.918337: step 363, loss 2.51727, acc 0.296875\n",
      "2018-12-07T16:01:07.479111: step 364, loss 1.82588, acc 0.4375\n",
      "2018-12-07T16:01:09.262796: step 365, loss 2.50513, acc 0.265625\n",
      "2018-12-07T16:01:11.354891: step 366, loss 2.01017, acc 0.328125\n",
      "2018-12-07T16:01:12.764145: step 367, loss 1.96582, acc 0.421875\n",
      "2018-12-07T16:01:14.457392: step 368, loss 2.48428, acc 0.28125\n",
      "2018-12-07T16:01:16.087889: step 369, loss 2.42471, acc 0.3125\n",
      "2018-12-07T16:01:18.004285: step 370, loss 2.73498, acc 0.21875\n",
      "2018-12-07T16:01:19.603053: step 371, loss 2.35234, acc 0.234375\n",
      "2018-12-07T16:01:21.356301: step 372, loss 2.21346, acc 0.3125\n",
      "2018-12-07T16:01:22.997147: step 373, loss 2.44867, acc 0.4375\n",
      "2018-12-07T16:01:24.545348: step 374, loss 2.07685, acc 0.390625\n",
      "2018-12-07T16:01:26.282409: step 375, loss 2.15918, acc 0.328125\n",
      "2018-12-07T16:01:27.953404: step 376, loss 1.83015, acc 0.375\n",
      "2018-12-07T16:01:29.661874: step 377, loss 2.19744, acc 0.40625\n",
      "2018-12-07T16:01:31.824557: step 378, loss 2.40162, acc 0.40625\n",
      "2018-12-07T16:01:33.959779: step 379, loss 2.43213, acc 0.296875\n",
      "2018-12-07T16:01:35.555875: step 380, loss 2.59795, acc 0.25\n",
      "2018-12-07T16:01:37.363998: step 381, loss 2.3807, acc 0.265625\n",
      "2018-12-07T16:01:39.265494: step 382, loss 2.7789, acc 0.296875\n",
      "2018-12-07T16:01:40.664047: step 383, loss 1.99565, acc 0.34375\n",
      "2018-12-07T16:01:42.469115: step 384, loss 2.24072, acc 0.390625\n",
      "2018-12-07T16:01:44.030080: step 385, loss 1.94642, acc 0.375\n",
      "2018-12-07T16:01:45.697652: step 386, loss 2.19196, acc 0.390625\n",
      "2018-12-07T16:01:47.103481: step 387, loss 1.97364, acc 0.282609\n",
      "2018-12-07T16:01:48.148155: step 388, loss 2.14239, acc 0.296875\n",
      "2018-12-07T16:01:49.314014: step 389, loss 2.13032, acc 0.28125\n",
      "2018-12-07T16:01:50.333534: step 390, loss 1.47831, acc 0.546875\n",
      "2018-12-07T16:01:51.504703: step 391, loss 1.99479, acc 0.375\n",
      "2018-12-07T16:01:52.502053: step 392, loss 2.08943, acc 0.40625\n",
      "2018-12-07T16:01:53.799709: step 393, loss 2.26482, acc 0.296875\n",
      "2018-12-07T16:01:54.802311: step 394, loss 2.01808, acc 0.359375\n",
      "2018-12-07T16:01:55.841995: step 395, loss 2.28767, acc 0.359375\n",
      "2018-12-07T16:01:56.915498: step 396, loss 1.95192, acc 0.34375\n",
      "2018-12-07T16:01:58.065955: step 397, loss 1.91594, acc 0.359375\n",
      "2018-12-07T16:01:59.111044: step 398, loss 1.91116, acc 0.375\n",
      "2018-12-07T16:02:00.275983: step 399, loss 1.75143, acc 0.4375\n",
      "2018-12-07T16:02:01.594090: step 400, loss 1.79173, acc 0.5\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T16:02:02.826795: step 400, loss 1.18064, acc 0.556\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-400\n",
      "\n",
      "2018-12-07T16:02:04.982438: step 401, loss 2.07399, acc 0.328125\n",
      "2018-12-07T16:02:06.351422: step 402, loss 1.65647, acc 0.4375\n",
      "2018-12-07T16:02:07.434716: step 403, loss 1.90882, acc 0.453125\n",
      "2018-12-07T16:02:08.700014: step 404, loss 1.72795, acc 0.34375\n",
      "2018-12-07T16:02:09.987739: step 405, loss 2.20283, acc 0.3125\n",
      "2018-12-07T16:02:11.042807: step 406, loss 1.95865, acc 0.46875\n",
      "2018-12-07T16:02:12.319393: step 407, loss 1.94641, acc 0.421875\n",
      "2018-12-07T16:02:13.432793: step 408, loss 2.10965, acc 0.265625\n",
      "2018-12-07T16:02:14.597513: step 409, loss 2.15339, acc 0.28125\n",
      "2018-12-07T16:02:15.714525: step 410, loss 1.93733, acc 0.375\n",
      "2018-12-07T16:02:16.869662: step 411, loss 2.26327, acc 0.3125\n",
      "2018-12-07T16:02:18.045471: step 412, loss 1.6212, acc 0.46875\n",
      "2018-12-07T16:02:19.197591: step 413, loss 1.88389, acc 0.375\n",
      "2018-12-07T16:02:20.195252: step 414, loss 1.80074, acc 0.4375\n",
      "2018-12-07T16:02:21.231926: step 415, loss 1.95654, acc 0.375\n",
      "2018-12-07T16:02:22.288100: step 416, loss 2.21638, acc 0.34375\n",
      "2018-12-07T16:02:23.370044: step 417, loss 1.83297, acc 0.390625\n",
      "2018-12-07T16:02:24.518214: step 418, loss 1.78835, acc 0.46875\n",
      "2018-12-07T16:02:25.718222: step 419, loss 1.74008, acc 0.40625\n",
      "2018-12-07T16:02:26.773096: step 420, loss 1.95472, acc 0.4375\n",
      "2018-12-07T16:02:28.153595: step 421, loss 1.82238, acc 0.40625\n",
      "2018-12-07T16:02:29.214041: step 422, loss 1.79834, acc 0.390625\n",
      "2018-12-07T16:02:30.308518: step 423, loss 1.76604, acc 0.40625\n",
      "2018-12-07T16:02:31.347274: step 424, loss 1.78832, acc 0.4375\n",
      "2018-12-07T16:02:32.394467: step 425, loss 2.12988, acc 0.328125\n",
      "2018-12-07T16:02:33.472478: step 426, loss 1.78322, acc 0.359375\n",
      "2018-12-07T16:02:34.889371: step 427, loss 1.86107, acc 0.453125\n",
      "2018-12-07T16:02:35.909642: step 428, loss 2.07143, acc 0.359375\n",
      "2018-12-07T16:02:37.188318: step 429, loss 1.66394, acc 0.40625\n",
      "2018-12-07T16:02:38.434269: step 430, loss 1.77751, acc 0.375\n",
      "2018-12-07T16:02:39.863630: step 431, loss 1.90131, acc 0.390625\n",
      "2018-12-07T16:02:41.097218: step 432, loss 1.87397, acc 0.390625\n",
      "2018-12-07T16:02:42.256512: step 433, loss 2.04033, acc 0.296875\n",
      "2018-12-07T16:02:43.294086: step 434, loss 2.17755, acc 0.3125\n",
      "2018-12-07T16:02:44.368801: step 435, loss 1.84387, acc 0.3125\n",
      "2018-12-07T16:02:45.461340: step 436, loss 1.88052, acc 0.375\n",
      "2018-12-07T16:02:46.632203: step 437, loss 1.75876, acc 0.46875\n",
      "2018-12-07T16:02:48.103490: step 438, loss 2.00014, acc 0.359375\n",
      "2018-12-07T16:02:49.122039: step 439, loss 1.68188, acc 0.359375\n",
      "2018-12-07T16:02:50.308196: step 440, loss 1.79169, acc 0.375\n",
      "2018-12-07T16:02:51.665150: step 441, loss 1.93043, acc 0.34375\n",
      "2018-12-07T16:02:52.928757: step 442, loss 1.54609, acc 0.515625\n",
      "2018-12-07T16:02:54.027856: step 443, loss 1.75876, acc 0.375\n",
      "2018-12-07T16:02:55.229881: step 444, loss 1.92902, acc 0.234375\n",
      "2018-12-07T16:02:56.303258: step 445, loss 1.88524, acc 0.390625\n",
      "2018-12-07T16:02:57.529162: step 446, loss 2.16141, acc 0.328125\n",
      "2018-12-07T16:02:58.745936: step 447, loss 1.85366, acc 0.375\n",
      "2018-12-07T16:03:00.117286: step 448, loss 1.82602, acc 0.390625\n",
      "2018-12-07T16:03:01.279335: step 449, loss 1.48084, acc 0.453125\n",
      "2018-12-07T16:03:02.391931: step 450, loss 1.73829, acc 0.484375\n",
      "2018-12-07T16:03:03.459429: step 451, loss 1.76989, acc 0.4375\n",
      "2018-12-07T16:03:04.615148: step 452, loss 1.79734, acc 0.390625\n",
      "2018-12-07T16:03:05.786082: step 453, loss 1.81131, acc 0.40625\n",
      "2018-12-07T16:03:07.057518: step 454, loss 1.46124, acc 0.4375\n",
      "2018-12-07T16:03:08.119331: step 455, loss 1.7668, acc 0.3125\n",
      "2018-12-07T16:03:09.445313: step 456, loss 1.84782, acc 0.359375\n",
      "2018-12-07T16:03:10.661569: step 457, loss 2.01759, acc 0.3125\n",
      "2018-12-07T16:03:11.757031: step 458, loss 1.71798, acc 0.390625\n",
      "2018-12-07T16:03:12.839025: step 459, loss 1.64234, acc 0.46875\n",
      "2018-12-07T16:03:14.038688: step 460, loss 1.45776, acc 0.46875\n",
      "2018-12-07T16:03:15.105792: step 461, loss 2.03264, acc 0.390625\n",
      "2018-12-07T16:03:16.167800: step 462, loss 1.82998, acc 0.375\n",
      "2018-12-07T16:03:17.550918: step 463, loss 1.81507, acc 0.421875\n",
      "2018-12-07T16:03:18.738775: step 464, loss 1.84805, acc 0.390625\n",
      "2018-12-07T16:03:19.793394: step 465, loss 1.57405, acc 0.390625\n",
      "2018-12-07T16:03:21.157560: step 466, loss 1.91398, acc 0.328125\n",
      "2018-12-07T16:03:22.127950: step 467, loss 1.74314, acc 0.390625\n",
      "2018-12-07T16:03:23.502169: step 468, loss 2.0121, acc 0.296875\n",
      "2018-12-07T16:03:24.673190: step 469, loss 2.05068, acc 0.3125\n",
      "2018-12-07T16:03:25.693818: step 470, loss 2.07277, acc 0.34375\n",
      "2018-12-07T16:03:26.912522: step 471, loss 1.4836, acc 0.5\n",
      "2018-12-07T16:03:28.105507: step 472, loss 1.54381, acc 0.46875\n",
      "2018-12-07T16:03:29.178957: step 473, loss 2.02959, acc 0.359375\n",
      "2018-12-07T16:03:30.409348: step 474, loss 1.76793, acc 0.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07T16:03:31.643410: step 475, loss 1.73884, acc 0.390625\n",
      "2018-12-07T16:03:32.834671: step 476, loss 1.63987, acc 0.375\n",
      "2018-12-07T16:03:34.004214: step 477, loss 1.31807, acc 0.53125\n",
      "2018-12-07T16:03:35.075686: step 478, loss 1.35933, acc 0.53125\n",
      "2018-12-07T16:03:36.262215: step 479, loss 1.62795, acc 0.375\n",
      "2018-12-07T16:03:37.489474: step 480, loss 1.77043, acc 0.40625\n",
      "2018-12-07T16:03:38.609885: step 481, loss 1.572, acc 0.421875\n",
      "2018-12-07T16:03:39.645889: step 482, loss 1.53112, acc 0.46875\n",
      "2018-12-07T16:03:40.719069: step 483, loss 1.67897, acc 0.421875\n",
      "2018-12-07T16:03:41.808405: step 484, loss 2.12042, acc 0.25\n",
      "2018-12-07T16:03:42.908972: step 485, loss 1.75772, acc 0.40625\n",
      "2018-12-07T16:03:44.123589: step 486, loss 1.54729, acc 0.40625\n",
      "2018-12-07T16:03:45.160772: step 487, loss 1.6025, acc 0.484375\n",
      "2018-12-07T16:03:46.337719: step 488, loss 1.68253, acc 0.4375\n",
      "2018-12-07T16:03:47.670161: step 489, loss 1.29219, acc 0.4375\n",
      "2018-12-07T16:03:48.835469: step 490, loss 1.54622, acc 0.421875\n",
      "2018-12-07T16:03:50.198626: step 491, loss 1.74902, acc 0.328125\n",
      "2018-12-07T16:03:51.446670: step 492, loss 1.52936, acc 0.40625\n",
      "2018-12-07T16:03:52.467113: step 493, loss 1.67652, acc 0.40625\n",
      "2018-12-07T16:03:53.797507: step 494, loss 1.7982, acc 0.34375\n",
      "2018-12-07T16:03:54.955022: step 495, loss 2.14878, acc 0.28125\n",
      "2018-12-07T16:03:56.019820: step 496, loss 1.53138, acc 0.421875\n",
      "2018-12-07T16:03:57.498049: step 497, loss 1.42081, acc 0.484375\n",
      "2018-12-07T16:03:58.556473: step 498, loss 1.55711, acc 0.390625\n",
      "2018-12-07T16:03:59.965591: step 499, loss 1.75019, acc 0.375\n",
      "2018-12-07T16:04:01.058439: step 500, loss 1.66917, acc 0.40625\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T16:04:02.302113: step 500, loss 1.17727, acc 0.552\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-500\n",
      "\n",
      "2018-12-07T16:04:04.146650: step 501, loss 1.5576, acc 0.46875\n",
      "2018-12-07T16:04:05.272737: step 502, loss 1.48834, acc 0.484375\n",
      "2018-12-07T16:04:06.500049: step 503, loss 1.58669, acc 0.421875\n",
      "2018-12-07T16:04:07.626673: step 504, loss 1.93752, acc 0.484375\n",
      "2018-12-07T16:04:09.124970: step 505, loss 1.84768, acc 0.4375\n",
      "2018-12-07T16:04:10.275526: step 506, loss 1.90502, acc 0.296875\n",
      "2018-12-07T16:04:11.584700: step 507, loss 1.53005, acc 0.4375\n",
      "2018-12-07T16:04:12.664498: step 508, loss 1.71947, acc 0.40625\n",
      "2018-12-07T16:04:13.812755: step 509, loss 1.65394, acc 0.375\n",
      "2018-12-07T16:04:15.180264: step 510, loss 1.32711, acc 0.5\n",
      "2018-12-07T16:04:16.691772: step 511, loss 1.70009, acc 0.421875\n",
      "2018-12-07T16:04:18.100299: step 512, loss 1.74468, acc 0.4375\n",
      "2018-12-07T16:04:19.541633: step 513, loss 1.86785, acc 0.296875\n",
      "2018-12-07T16:04:21.024774: step 514, loss 1.6788, acc 0.328125\n",
      "2018-12-07T16:04:23.213467: step 515, loss 1.71053, acc 0.359375\n",
      "2018-12-07T16:04:24.813658: step 516, loss 1.61068, acc 0.40625\n",
      "2018-12-07T16:04:25.966874: step 517, loss 1.74522, acc 0.375\n",
      "2018-12-07T16:04:27.315906: step 518, loss 1.84203, acc 0.328125\n",
      "2018-12-07T16:04:28.457660: step 519, loss 1.94396, acc 0.453125\n",
      "2018-12-07T16:04:29.934851: step 520, loss 1.82931, acc 0.25\n",
      "2018-12-07T16:04:31.581986: step 521, loss 1.6026, acc 0.4375\n",
      "2018-12-07T16:04:32.976878: step 522, loss 1.4249, acc 0.4375\n",
      "2018-12-07T16:04:34.315153: step 523, loss 1.67351, acc 0.46875\n",
      "2018-12-07T16:04:35.604147: step 524, loss 1.48373, acc 0.46875\n",
      "2018-12-07T16:04:36.973590: step 525, loss 1.85845, acc 0.40625\n",
      "2018-12-07T16:04:38.076533: step 526, loss 1.62357, acc 0.40625\n",
      "2018-12-07T16:04:39.238300: step 527, loss 1.91351, acc 0.40625\n",
      "2018-12-07T16:04:40.310538: step 528, loss 1.51335, acc 0.46875\n",
      "2018-12-07T16:04:41.507018: step 529, loss 1.92611, acc 0.359375\n",
      "2018-12-07T16:04:42.578762: step 530, loss 1.74062, acc 0.3125\n",
      "2018-12-07T16:04:43.720738: step 531, loss 1.45744, acc 0.5\n",
      "2018-12-07T16:04:45.067854: step 532, loss 1.58003, acc 0.375\n",
      "2018-12-07T16:04:46.518677: step 533, loss 1.20946, acc 0.484375\n",
      "2018-12-07T16:04:48.250471: step 534, loss 1.24773, acc 0.453125\n",
      "2018-12-07T16:04:49.672585: step 535, loss 1.5342, acc 0.390625\n",
      "2018-12-07T16:04:51.147852: step 536, loss 1.80888, acc 0.3125\n",
      "2018-12-07T16:04:52.425867: step 537, loss 1.65111, acc 0.34375\n",
      "2018-12-07T16:04:53.670544: step 538, loss 1.26281, acc 0.453125\n",
      "2018-12-07T16:04:54.807068: step 539, loss 1.6937, acc 0.375\n",
      "2018-12-07T16:04:56.191994: step 540, loss 1.65341, acc 0.40625\n",
      "2018-12-07T16:04:57.428465: step 541, loss 1.53369, acc 0.453125\n",
      "2018-12-07T16:04:58.568584: step 542, loss 1.81295, acc 0.375\n",
      "2018-12-07T16:04:59.818842: step 543, loss 1.49153, acc 0.421875\n",
      "2018-12-07T16:05:01.321548: step 544, loss 1.7072, acc 0.34375\n",
      "2018-12-07T16:05:02.754078: step 545, loss 1.63096, acc 0.359375\n",
      "2018-12-07T16:05:03.896193: step 546, loss 1.69545, acc 0.359375\n",
      "2018-12-07T16:05:05.168679: step 547, loss 1.35336, acc 0.546875\n",
      "2018-12-07T16:05:06.310177: step 548, loss 1.59355, acc 0.359375\n",
      "2018-12-07T16:05:07.529684: step 549, loss 1.59209, acc 0.421875\n",
      "2018-12-07T16:05:08.640656: step 550, loss 1.9886, acc 0.28125\n",
      "2018-12-07T16:05:09.836876: step 551, loss 1.59747, acc 0.421875\n",
      "2018-12-07T16:05:10.941524: step 552, loss 1.74702, acc 0.390625\n",
      "2018-12-07T16:05:12.029290: step 553, loss 1.68465, acc 0.328125\n",
      "2018-12-07T16:05:13.145939: step 554, loss 1.59921, acc 0.4375\n",
      "2018-12-07T16:05:14.246578: step 555, loss 1.33585, acc 0.5\n",
      "2018-12-07T16:05:15.728908: step 556, loss 1.59595, acc 0.375\n",
      "2018-12-07T16:05:16.898372: step 557, loss 1.85662, acc 0.265625\n",
      "2018-12-07T16:05:18.318234: step 558, loss 1.67013, acc 0.484375\n",
      "2018-12-07T16:05:19.504827: step 559, loss 1.54964, acc 0.453125\n",
      "2018-12-07T16:05:20.725973: step 560, loss 1.57411, acc 0.46875\n",
      "2018-12-07T16:05:21.887585: step 561, loss 1.69405, acc 0.34375\n",
      "2018-12-07T16:05:23.132654: step 562, loss 1.57042, acc 0.421875\n",
      "2018-12-07T16:05:24.559689: step 563, loss 1.62014, acc 0.40625\n",
      "2018-12-07T16:05:25.815831: step 564, loss 1.3676, acc 0.4375\n",
      "2018-12-07T16:05:27.042960: step 565, loss 1.51476, acc 0.390625\n",
      "2018-12-07T16:05:28.259782: step 566, loss 1.7621, acc 0.3125\n",
      "2018-12-07T16:05:29.683219: step 567, loss 1.66422, acc 0.515625\n",
      "2018-12-07T16:05:30.912179: step 568, loss 1.45943, acc 0.453125\n",
      "2018-12-07T16:05:32.041370: step 569, loss 1.6668, acc 0.375\n",
      "2018-12-07T16:05:33.310644: step 570, loss 1.50579, acc 0.4375\n",
      "2018-12-07T16:05:34.574918: step 571, loss 1.44359, acc 0.4375\n",
      "2018-12-07T16:05:35.771498: step 572, loss 1.41321, acc 0.421875\n",
      "2018-12-07T16:05:37.433377: step 573, loss 1.56046, acc 0.40625\n",
      "2018-12-07T16:05:38.781038: step 574, loss 1.64186, acc 0.421875\n",
      "2018-12-07T16:05:39.897869: step 575, loss 1.58041, acc 0.390625\n",
      "2018-12-07T16:05:41.095478: step 576, loss 1.78131, acc 0.296875\n",
      "2018-12-07T16:05:42.516009: step 577, loss 1.55451, acc 0.328125\n",
      "2018-12-07T16:05:43.610800: step 578, loss 1.52394, acc 0.375\n",
      "2018-12-07T16:05:45.118586: step 579, loss 1.09565, acc 0.546875\n",
      "2018-12-07T16:05:46.232237: step 580, loss 1.87438, acc 0.296875\n",
      "2018-12-07T16:05:47.442184: step 581, loss 1.51652, acc 0.453125\n",
      "2018-12-07T16:05:48.809786: step 582, loss 1.2984, acc 0.4375\n",
      "2018-12-07T16:05:50.139028: step 583, loss 1.64929, acc 0.34375\n",
      "2018-12-07T16:05:51.637616: step 584, loss 1.53383, acc 0.421875\n",
      "2018-12-07T16:05:53.023687: step 585, loss 1.54064, acc 0.359375\n",
      "2018-12-07T16:05:54.253825: step 586, loss 1.56193, acc 0.390625\n",
      "2018-12-07T16:05:55.659403: step 587, loss 1.50592, acc 0.40625\n",
      "2018-12-07T16:05:56.805105: step 588, loss 1.54703, acc 0.390625\n",
      "2018-12-07T16:05:57.961219: step 589, loss 1.3245, acc 0.46875\n",
      "2018-12-07T16:05:59.280506: step 590, loss 1.68007, acc 0.390625\n",
      "2018-12-07T16:06:00.707871: step 591, loss 1.66141, acc 0.390625\n",
      "2018-12-07T16:06:02.006895: step 592, loss 1.75534, acc 0.375\n",
      "2018-12-07T16:06:03.235513: step 593, loss 1.43509, acc 0.46875\n",
      "2018-12-07T16:06:04.566962: step 594, loss 1.59016, acc 0.375\n",
      "2018-12-07T16:06:05.708058: step 595, loss 1.69706, acc 0.40625\n",
      "2018-12-07T16:06:06.902178: step 596, loss 1.52515, acc 0.421875\n",
      "2018-12-07T16:06:07.989856: step 597, loss 1.57094, acc 0.515625\n",
      "2018-12-07T16:06:09.283997: step 598, loss 1.54696, acc 0.453125\n",
      "2018-12-07T16:06:10.395287: step 599, loss 1.53287, acc 0.40625\n",
      "2018-12-07T16:06:11.933489: step 600, loss 1.87457, acc 0.3125\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07T16:06:13.166192: step 600, loss 1.13948, acc 0.584\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-600\n",
      "\n",
      "2018-12-07T16:06:15.162511: step 601, loss 1.33864, acc 0.40625\n",
      "2018-12-07T16:06:16.319417: step 602, loss 1.62139, acc 0.40625\n",
      "2018-12-07T16:06:17.961607: step 603, loss 1.44116, acc 0.4375\n",
      "2018-12-07T16:06:19.186070: step 604, loss 1.62148, acc 0.421875\n",
      "2018-12-07T16:06:20.626235: step 605, loss 1.21001, acc 0.5\n",
      "2018-12-07T16:06:22.319267: step 606, loss 1.41327, acc 0.421875\n",
      "2018-12-07T16:06:23.673882: step 607, loss 1.51988, acc 0.359375\n",
      "2018-12-07T16:06:24.960173: step 608, loss 1.60068, acc 0.34375\n",
      "2018-12-07T16:06:26.297262: step 609, loss 1.30202, acc 0.484375\n",
      "2018-12-07T16:06:27.857818: step 610, loss 1.60118, acc 0.390625\n",
      "2018-12-07T16:06:29.600535: step 611, loss 1.85043, acc 0.265625\n",
      "2018-12-07T16:06:30.967360: step 612, loss 1.44671, acc 0.484375\n",
      "2018-12-07T16:06:32.477693: step 613, loss 1.70777, acc 0.34375\n",
      "2018-12-07T16:06:33.717628: step 614, loss 1.34157, acc 0.46875\n",
      "2018-12-07T16:06:35.376150: step 615, loss 1.4292, acc 0.4375\n",
      "2018-12-07T16:06:36.720037: step 616, loss 1.57562, acc 0.390625\n",
      "2018-12-07T16:06:38.222811: step 617, loss 1.40487, acc 0.453125\n",
      "2018-12-07T16:06:39.838480: step 618, loss 1.52545, acc 0.375\n",
      "2018-12-07T16:06:41.195840: step 619, loss 1.58121, acc 0.375\n",
      "2018-12-07T16:06:42.438766: step 620, loss 1.31748, acc 0.453125\n",
      "2018-12-07T16:06:43.636159: step 621, loss 1.75681, acc 0.359375\n",
      "2018-12-07T16:06:45.047575: step 622, loss 1.54017, acc 0.5\n",
      "2018-12-07T16:06:46.456401: step 623, loss 1.4914, acc 0.375\n",
      "2018-12-07T16:06:47.956448: step 624, loss 1.59304, acc 0.375\n",
      "2018-12-07T16:06:49.396956: step 625, loss 1.53184, acc 0.359375\n",
      "2018-12-07T16:06:50.741756: step 626, loss 1.41634, acc 0.453125\n",
      "2018-12-07T16:06:52.052085: step 627, loss 1.43257, acc 0.453125\n",
      "2018-12-07T16:06:53.640793: step 628, loss 1.47908, acc 0.4375\n",
      "2018-12-07T16:06:55.195774: step 629, loss 1.50903, acc 0.40625\n",
      "2018-12-07T16:06:56.725691: step 630, loss 1.57668, acc 0.359375\n",
      "2018-12-07T16:06:57.949619: step 631, loss 1.37326, acc 0.453125\n",
      "2018-12-07T16:06:59.173679: step 632, loss 1.4718, acc 0.421875\n",
      "2018-12-07T16:07:00.887333: step 633, loss 1.32735, acc 0.484375\n",
      "2018-12-07T16:07:02.221455: step 634, loss 1.55203, acc 0.46875\n",
      "2018-12-07T16:07:03.474256: step 635, loss 1.43456, acc 0.421875\n",
      "2018-12-07T16:07:04.677425: step 636, loss 1.53666, acc 0.375\n",
      "2018-12-07T16:07:06.276542: step 637, loss 1.60609, acc 0.328125\n",
      "2018-12-07T16:07:07.638847: step 638, loss 1.49936, acc 0.375\n",
      "2018-12-07T16:07:08.852971: step 639, loss 1.66039, acc 0.453125\n",
      "2018-12-07T16:07:10.067850: step 640, loss 1.4839, acc 0.421875\n",
      "2018-12-07T16:07:11.304715: step 641, loss 1.32872, acc 0.40625\n",
      "2018-12-07T16:07:12.512526: step 642, loss 1.83579, acc 0.28125\n",
      "2018-12-07T16:07:14.260394: step 643, loss 1.22235, acc 0.484375\n",
      "2018-12-07T16:07:15.886325: step 644, loss 1.48334, acc 0.453125\n",
      "2018-12-07T16:07:17.498325: step 645, loss 1.3711, acc 0.484375\n",
      "2018-12-07T16:07:18.719059: step 646, loss 1.47251, acc 0.375\n",
      "2018-12-07T16:07:20.198905: step 647, loss 1.59315, acc 0.375\n",
      "2018-12-07T16:07:21.675946: step 648, loss 1.70877, acc 0.328125\n",
      "2018-12-07T16:07:23.029059: step 649, loss 1.34119, acc 0.46875\n",
      "2018-12-07T16:07:24.365415: step 650, loss 1.52356, acc 0.328125\n",
      "2018-12-07T16:07:25.855179: step 651, loss 1.22354, acc 0.578125\n",
      "2018-12-07T16:07:27.506208: step 652, loss 1.40181, acc 0.4375\n",
      "2018-12-07T16:07:28.793984: step 653, loss 1.48893, acc 0.421875\n",
      "2018-12-07T16:07:30.335520: step 654, loss 1.69586, acc 0.28125\n",
      "2018-12-07T16:07:31.686012: step 655, loss 1.3667, acc 0.390625\n",
      "2018-12-07T16:07:32.920933: step 656, loss 1.3594, acc 0.4375\n",
      "2018-12-07T16:07:34.162265: step 657, loss 1.41229, acc 0.359375\n",
      "2018-12-07T16:07:35.835142: step 658, loss 1.32969, acc 0.40625\n",
      "2018-12-07T16:07:37.166529: step 659, loss 1.49163, acc 0.375\n",
      "2018-12-07T16:07:38.538387: step 660, loss 1.25602, acc 0.515625\n",
      "2018-12-07T16:07:39.744592: step 661, loss 1.24715, acc 0.421875\n",
      "2018-12-07T16:07:41.002160: step 662, loss 1.51, acc 0.375\n",
      "2018-12-07T16:07:42.181868: step 663, loss 1.38342, acc 0.515625\n",
      "2018-12-07T16:07:44.106002: step 664, loss 1.48684, acc 0.484375\n",
      "2018-12-07T16:07:45.762111: step 665, loss 1.59064, acc 0.375\n",
      "2018-12-07T16:07:47.212003: step 666, loss 1.42692, acc 0.484375\n",
      "2018-12-07T16:07:48.648206: step 667, loss 1.50012, acc 0.421875\n",
      "2018-12-07T16:07:50.013891: step 668, loss 1.59651, acc 0.359375\n",
      "2018-12-07T16:07:51.322224: step 669, loss 1.4298, acc 0.484375\n",
      "2018-12-07T16:07:52.665822: step 670, loss 1.64727, acc 0.359375\n",
      "2018-12-07T16:07:54.108355: step 671, loss 1.58657, acc 0.375\n",
      "2018-12-07T16:07:55.515486: step 672, loss 1.58088, acc 0.4375\n",
      "2018-12-07T16:07:56.993004: step 673, loss 1.14923, acc 0.53125\n",
      "2018-12-07T16:07:58.150902: step 674, loss 1.47363, acc 0.296875\n",
      "2018-12-07T16:07:59.348115: step 675, loss 1.04689, acc 0.5625\n",
      "2018-12-07T16:08:00.808689: step 676, loss 1.39985, acc 0.53125\n",
      "2018-12-07T16:08:02.547867: step 677, loss 1.29945, acc 0.546875\n",
      "2018-12-07T16:08:03.893576: step 678, loss 1.45634, acc 0.453125\n",
      "2018-12-07T16:08:05.086046: step 679, loss 1.41245, acc 0.40625\n",
      "2018-12-07T16:08:06.638672: step 680, loss 1.45887, acc 0.453125\n",
      "2018-12-07T16:08:08.491526: step 681, loss 1.6114, acc 0.4375\n",
      "2018-12-07T16:08:10.147187: step 682, loss 1.38003, acc 0.421875\n",
      "2018-12-07T16:08:11.780105: step 683, loss 1.48434, acc 0.390625\n",
      "2018-12-07T16:08:13.303838: step 684, loss 1.45807, acc 0.46875\n",
      "2018-12-07T16:08:14.578197: step 685, loss 1.39292, acc 0.4375\n",
      "2018-12-07T16:08:16.258362: step 686, loss 1.21015, acc 0.515625\n",
      "2018-12-07T16:08:17.759692: step 687, loss 1.43417, acc 0.421875\n",
      "2018-12-07T16:08:19.190781: step 688, loss 1.49397, acc 0.421875\n",
      "2018-12-07T16:08:20.699421: step 689, loss 1.20825, acc 0.53125\n",
      "2018-12-07T16:08:21.964516: step 690, loss 1.40363, acc 0.4375\n",
      "2018-12-07T16:08:23.556805: step 691, loss 1.55956, acc 0.40625\n",
      "2018-12-07T16:08:25.186952: step 692, loss 1.70069, acc 0.390625\n",
      "2018-12-07T16:08:27.418152: step 693, loss 1.35506, acc 0.5\n",
      "2018-12-07T16:08:29.062373: step 694, loss 1.43637, acc 0.4375\n",
      "2018-12-07T16:08:30.873576: step 695, loss 1.48047, acc 0.4375\n",
      "2018-12-07T16:08:32.620148: step 696, loss 1.6293, acc 0.359375\n",
      "2018-12-07T16:08:34.447583: step 697, loss 1.64062, acc 0.375\n",
      "2018-12-07T16:08:36.315728: step 698, loss 1.43612, acc 0.421875\n",
      "2018-12-07T16:08:37.977759: step 699, loss 1.59781, acc 0.296875\n",
      "2018-12-07T16:08:39.506751: step 700, loss 1.20833, acc 0.53125\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T16:08:40.739456: step 700, loss 1.18794, acc 0.552\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-700\n",
      "\n",
      "2018-12-07T16:08:42.973821: step 701, loss 1.52225, acc 0.390625\n",
      "2018-12-07T16:08:44.843720: step 702, loss 1.52148, acc 0.359375\n",
      "2018-12-07T16:08:46.405239: step 703, loss 1.48394, acc 0.359375\n",
      "2018-12-07T16:08:48.159506: step 704, loss 1.43842, acc 0.390625\n",
      "2018-12-07T16:08:50.052000: step 705, loss 1.46277, acc 0.4375\n",
      "2018-12-07T16:08:52.127356: step 706, loss 1.28311, acc 0.53125\n",
      "2018-12-07T16:08:53.676935: step 707, loss 1.53082, acc 0.390625\n",
      "2018-12-07T16:08:55.367051: step 708, loss 1.35846, acc 0.4375\n",
      "2018-12-07T16:08:57.071795: step 709, loss 1.40046, acc 0.5\n",
      "2018-12-07T16:08:58.651329: step 710, loss 1.52141, acc 0.4375\n",
      "2018-12-07T16:09:00.488390: step 711, loss 1.38949, acc 0.421875\n",
      "2018-12-07T16:09:02.159917: step 712, loss 1.67856, acc 0.421875\n",
      "2018-12-07T16:09:04.217428: step 713, loss 1.37906, acc 0.46875\n",
      "2018-12-07T16:09:05.705636: step 714, loss 1.52521, acc 0.40625\n",
      "2018-12-07T16:09:07.468708: step 715, loss 1.15919, acc 0.53125\n",
      "2018-12-07T16:09:09.393365: step 716, loss 1.3636, acc 0.5\n",
      "2018-12-07T16:09:11.003118: step 717, loss 1.33728, acc 0.4375\n",
      "2018-12-07T16:09:12.780013: step 718, loss 1.25539, acc 0.453125\n",
      "2018-12-07T16:09:14.245497: step 719, loss 1.30352, acc 0.453125\n",
      "2018-12-07T16:09:15.806427: step 720, loss 1.32252, acc 0.5\n",
      "2018-12-07T16:09:17.690414: step 721, loss 1.18661, acc 0.515625\n",
      "2018-12-07T16:09:19.278892: step 722, loss 1.2822, acc 0.4375\n",
      "2018-12-07T16:09:20.925772: step 723, loss 1.42321, acc 0.390625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07T16:09:22.414301: step 724, loss 1.46397, acc 0.484375\n",
      "2018-12-07T16:09:24.113946: step 725, loss 1.11905, acc 0.53125\n",
      "2018-12-07T16:09:25.498800: step 726, loss 1.10852, acc 0.5625\n",
      "2018-12-07T16:09:27.336281: step 727, loss 1.44918, acc 0.484375\n",
      "2018-12-07T16:09:28.947264: step 728, loss 1.51169, acc 0.359375\n",
      "2018-12-07T16:09:30.602535: step 729, loss 1.43129, acc 0.421875\n",
      "2018-12-07T16:09:32.292820: step 730, loss 1.47496, acc 0.375\n",
      "2018-12-07T16:09:33.955771: step 731, loss 1.41018, acc 0.421875\n",
      "2018-12-07T16:09:35.524104: step 732, loss 1.17253, acc 0.53125\n",
      "2018-12-07T16:09:37.247053: step 733, loss 1.39498, acc 0.40625\n",
      "2018-12-07T16:09:38.771359: step 734, loss 1.28059, acc 0.453125\n",
      "2018-12-07T16:09:41.015587: step 735, loss 1.25669, acc 0.515625\n",
      "2018-12-07T16:09:42.817087: step 736, loss 1.3994, acc 0.5\n",
      "2018-12-07T16:09:44.238485: step 737, loss 1.51771, acc 0.390625\n",
      "2018-12-07T16:09:46.572205: step 738, loss 1.47901, acc 0.390625\n",
      "2018-12-07T16:09:48.535895: step 739, loss 1.44029, acc 0.40625\n",
      "2018-12-07T16:09:50.671960: step 740, loss 1.48367, acc 0.421875\n",
      "2018-12-07T16:09:52.183038: step 741, loss 1.27087, acc 0.5\n",
      "2018-12-07T16:09:54.038881: step 742, loss 1.29347, acc 0.484375\n",
      "2018-12-07T16:09:56.057842: step 743, loss 1.52294, acc 0.40625\n",
      "2018-12-07T16:09:57.793295: step 744, loss 1.37459, acc 0.421875\n",
      "2018-12-07T16:09:59.479967: step 745, loss 1.40261, acc 0.453125\n",
      "2018-12-07T16:10:01.126084: step 746, loss 1.44924, acc 0.453125\n",
      "2018-12-07T16:10:02.491088: step 747, loss 1.31866, acc 0.484375\n",
      "2018-12-07T16:10:04.020769: step 748, loss 1.35013, acc 0.5\n",
      "2018-12-07T16:10:05.752601: step 749, loss 1.36883, acc 0.484375\n",
      "2018-12-07T16:10:08.089684: step 750, loss 1.38525, acc 0.46875\n",
      "2018-12-07T16:10:09.877885: step 751, loss 1.22056, acc 0.515625\n",
      "2018-12-07T16:10:11.670016: step 752, loss 1.32361, acc 0.421875\n",
      "2018-12-07T16:10:13.623481: step 753, loss 1.33706, acc 0.453125\n",
      "2018-12-07T16:10:15.517671: step 754, loss 1.59081, acc 0.3125\n",
      "2018-12-07T16:10:17.184308: step 755, loss 1.37667, acc 0.4375\n",
      "2018-12-07T16:10:19.098988: step 756, loss 1.21373, acc 0.46875\n",
      "2018-12-07T16:10:21.014056: step 757, loss 1.5229, acc 0.359375\n",
      "2018-12-07T16:10:22.662829: step 758, loss 1.19295, acc 0.53125\n",
      "2018-12-07T16:10:24.569603: step 759, loss 1.12321, acc 0.484375\n",
      "2018-12-07T16:10:26.470598: step 760, loss 1.40708, acc 0.421875\n",
      "2018-12-07T16:10:28.400448: step 761, loss 1.46434, acc 0.4375\n",
      "2018-12-07T16:10:30.524355: step 762, loss 1.3857, acc 0.375\n",
      "2018-12-07T16:10:32.064766: step 763, loss 1.42107, acc 0.46875\n",
      "2018-12-07T16:10:33.835364: step 764, loss 1.25697, acc 0.578125\n",
      "2018-12-07T16:10:35.271935: step 765, loss 1.22693, acc 0.515625\n",
      "2018-12-07T16:10:36.829769: step 766, loss 1.4852, acc 0.375\n",
      "2018-12-07T16:10:38.971640: step 767, loss 1.35085, acc 0.40625\n",
      "2018-12-07T16:10:40.439388: step 768, loss 1.41192, acc 0.40625\n",
      "2018-12-07T16:10:42.000616: step 769, loss 1.24839, acc 0.484375\n",
      "2018-12-07T16:10:43.931986: step 770, loss 1.26004, acc 0.515625\n",
      "2018-12-07T16:10:45.814376: step 771, loss 1.40623, acc 0.453125\n",
      "2018-12-07T16:10:47.334739: step 772, loss 1.35463, acc 0.4375\n",
      "2018-12-07T16:10:49.249158: step 773, loss 1.40313, acc 0.359375\n",
      "2018-12-07T16:10:50.719851: step 774, loss 1.23512, acc 0.5\n",
      "2018-12-07T16:10:51.745556: step 775, loss 1.43933, acc 0.5\n",
      "2018-12-07T16:10:53.032565: step 776, loss 1.10951, acc 0.5\n",
      "2018-12-07T16:10:54.035732: step 777, loss 1.22062, acc 0.515625\n",
      "2018-12-07T16:10:55.057339: step 778, loss 1.23598, acc 0.5\n",
      "2018-12-07T16:10:56.026234: step 779, loss 1.32404, acc 0.5\n",
      "2018-12-07T16:10:57.066247: step 780, loss 1.26275, acc 0.46875\n",
      "2018-12-07T16:10:58.307918: step 781, loss 1.1812, acc 0.578125\n",
      "2018-12-07T16:10:59.471648: step 782, loss 1.18128, acc 0.515625\n",
      "2018-12-07T16:11:00.475322: step 783, loss 1.25059, acc 0.5\n",
      "2018-12-07T16:11:01.828487: step 784, loss 1.15275, acc 0.546875\n",
      "2018-12-07T16:11:02.981156: step 785, loss 1.27857, acc 0.5\n",
      "2018-12-07T16:11:03.972821: step 786, loss 1.31835, acc 0.5\n",
      "2018-12-07T16:11:05.236379: step 787, loss 1.24937, acc 0.46875\n",
      "2018-12-07T16:11:06.238489: step 788, loss 1.28927, acc 0.4375\n",
      "2018-12-07T16:11:07.319833: step 789, loss 1.34283, acc 0.484375\n",
      "2018-12-07T16:11:08.351938: step 790, loss 1.23769, acc 0.5\n",
      "2018-12-07T16:11:09.484118: step 791, loss 1.40999, acc 0.40625\n",
      "2018-12-07T16:11:10.484342: step 792, loss 1.4222, acc 0.390625\n",
      "2018-12-07T16:11:11.523453: step 793, loss 1.19715, acc 0.53125\n",
      "2018-12-07T16:11:12.936231: step 794, loss 1.29372, acc 0.453125\n",
      "2018-12-07T16:11:14.057840: step 795, loss 1.13438, acc 0.5\n",
      "2018-12-07T16:11:15.100823: step 796, loss 1.35169, acc 0.546875\n",
      "2018-12-07T16:11:16.080966: step 797, loss 1.383, acc 0.375\n",
      "2018-12-07T16:11:17.081844: step 798, loss 1.40788, acc 0.421875\n",
      "2018-12-07T16:11:18.220278: step 799, loss 1.19073, acc 0.515625\n",
      "2018-12-07T16:11:19.237948: step 800, loss 1.30033, acc 0.421875\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T16:11:20.460677: step 800, loss 1.10958, acc 0.6\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-800\n",
      "\n",
      "2018-12-07T16:11:22.290586: step 801, loss 1.21052, acc 0.53125\n",
      "2018-12-07T16:11:23.464392: step 802, loss 1.40254, acc 0.453125\n",
      "2018-12-07T16:11:24.653388: step 803, loss 1.33043, acc 0.515625\n",
      "2018-12-07T16:11:26.016122: step 804, loss 1.4127, acc 0.421875\n",
      "2018-12-07T16:11:27.078083: step 805, loss 1.12954, acc 0.53125\n",
      "2018-12-07T16:11:28.420105: step 806, loss 1.27341, acc 0.5\n",
      "2018-12-07T16:11:29.450632: step 807, loss 1.38514, acc 0.40625\n",
      "2018-12-07T16:11:30.633243: step 808, loss 1.10802, acc 0.515625\n",
      "2018-12-07T16:11:31.910190: step 809, loss 1.26246, acc 0.421875\n",
      "2018-12-07T16:11:32.889169: step 810, loss 1.14279, acc 0.546875\n",
      "2018-12-07T16:11:33.974624: step 811, loss 1.43076, acc 0.40625\n",
      "2018-12-07T16:11:35.037999: step 812, loss 1.39254, acc 0.453125\n",
      "2018-12-07T16:11:36.202509: step 813, loss 1.47625, acc 0.359375\n",
      "2018-12-07T16:11:37.262752: step 814, loss 1.2618, acc 0.453125\n",
      "2018-12-07T16:11:38.380802: step 815, loss 1.44485, acc 0.4375\n",
      "2018-12-07T16:11:39.734346: step 816, loss 1.07653, acc 0.5625\n",
      "2018-12-07T16:11:40.789796: step 817, loss 1.2383, acc 0.484375\n",
      "2018-12-07T16:11:42.428822: step 818, loss 1.43219, acc 0.453125\n",
      "2018-12-07T16:11:43.764916: step 819, loss 1.29831, acc 0.53125\n",
      "2018-12-07T16:11:44.982232: step 820, loss 1.57878, acc 0.421875\n",
      "2018-12-07T16:11:46.068960: step 821, loss 1.25069, acc 0.515625\n",
      "2018-12-07T16:11:47.139211: step 822, loss 1.34945, acc 0.484375\n",
      "2018-12-07T16:11:48.346308: step 823, loss 1.43589, acc 0.484375\n",
      "2018-12-07T16:11:49.380930: step 824, loss 1.22524, acc 0.46875\n",
      "2018-12-07T16:11:50.683406: step 825, loss 1.29621, acc 0.484375\n",
      "2018-12-07T16:11:51.743890: step 826, loss 1.33278, acc 0.46875\n",
      "2018-12-07T16:11:52.764079: step 827, loss 1.16819, acc 0.546875\n",
      "2018-12-07T16:11:54.035512: step 828, loss 1.3345, acc 0.46875\n",
      "2018-12-07T16:11:55.216633: step 829, loss 1.19695, acc 0.53125\n",
      "2018-12-07T16:11:56.616276: step 830, loss 1.30625, acc 0.4375\n",
      "2018-12-07T16:11:57.852506: step 831, loss 1.26383, acc 0.4375\n",
      "2018-12-07T16:11:59.274592: step 832, loss 1.19134, acc 0.53125\n",
      "2018-12-07T16:12:00.345086: step 833, loss 1.46385, acc 0.375\n",
      "2018-12-07T16:12:01.415643: step 834, loss 1.08175, acc 0.578125\n",
      "2018-12-07T16:12:02.592628: step 835, loss 1.25205, acc 0.40625\n",
      "2018-12-07T16:12:03.645776: step 836, loss 1.39896, acc 0.390625\n",
      "2018-12-07T16:12:04.821746: step 837, loss 1.32978, acc 0.421875\n",
      "2018-12-07T16:12:05.857702: step 838, loss 1.21199, acc 0.421875\n",
      "2018-12-07T16:12:06.929183: step 839, loss 1.19572, acc 0.5\n",
      "2018-12-07T16:12:08.035598: step 840, loss 1.30224, acc 0.5\n",
      "2018-12-07T16:12:09.172046: step 841, loss 1.28938, acc 0.484375\n",
      "2018-12-07T16:12:10.307451: step 842, loss 1.27971, acc 0.53125\n",
      "2018-12-07T16:12:11.358914: step 843, loss 1.26505, acc 0.484375\n",
      "2018-12-07T16:12:12.402984: step 844, loss 1.0056, acc 0.546875\n",
      "2018-12-07T16:12:13.654480: step 845, loss 1.21365, acc 0.46875\n",
      "2018-12-07T16:12:14.691510: step 846, loss 1.33634, acc 0.5\n",
      "2018-12-07T16:12:15.792311: step 847, loss 1.14983, acc 0.5\n",
      "2018-12-07T16:12:17.012764: step 848, loss 1.24624, acc 0.46875\n",
      "2018-12-07T16:12:18.158451: step 849, loss 1.36984, acc 0.484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07T16:12:19.250427: step 850, loss 1.22464, acc 0.5\n",
      "2018-12-07T16:12:20.506099: step 851, loss 1.23226, acc 0.59375\n",
      "2018-12-07T16:12:21.784134: step 852, loss 1.44689, acc 0.421875\n",
      "2018-12-07T16:12:22.968203: step 853, loss 1.28151, acc 0.546875\n",
      "2018-12-07T16:12:24.002473: step 854, loss 1.4928, acc 0.453125\n",
      "2018-12-07T16:12:25.019678: step 855, loss 1.37977, acc 0.5\n",
      "2018-12-07T16:12:26.090725: step 856, loss 1.15961, acc 0.515625\n",
      "2018-12-07T16:12:27.150563: step 857, loss 1.45966, acc 0.40625\n",
      "2018-12-07T16:12:28.402727: step 858, loss 1.28499, acc 0.5625\n",
      "2018-12-07T16:12:29.450056: step 859, loss 1.37353, acc 0.40625\n",
      "2018-12-07T16:12:30.455902: step 860, loss 1.35167, acc 0.546875\n",
      "2018-12-07T16:12:31.836356: step 861, loss 1.27925, acc 0.515625\n",
      "2018-12-07T16:12:32.868047: step 862, loss 1.35956, acc 0.4375\n",
      "2018-12-07T16:12:33.936379: step 863, loss 1.31228, acc 0.484375\n",
      "2018-12-07T16:12:35.044839: step 864, loss 1.26326, acc 0.4375\n",
      "2018-12-07T16:12:36.082857: step 865, loss 1.16439, acc 0.5625\n",
      "2018-12-07T16:12:37.223619: step 866, loss 1.16879, acc 0.5\n",
      "2018-12-07T16:12:38.302079: step 867, loss 1.08929, acc 0.546875\n",
      "2018-12-07T16:12:39.468299: step 868, loss 1.06587, acc 0.625\n",
      "2018-12-07T16:12:40.581578: step 869, loss 1.05731, acc 0.59375\n",
      "2018-12-07T16:12:41.780811: step 870, loss 1.2449, acc 0.390625\n",
      "2018-12-07T16:12:42.991912: step 871, loss 1.27695, acc 0.515625\n",
      "2018-12-07T16:12:44.028430: step 872, loss 1.2884, acc 0.40625\n",
      "2018-12-07T16:12:45.152467: step 873, loss 1.45694, acc 0.453125\n",
      "2018-12-07T16:12:46.418431: step 874, loss 1.32141, acc 0.5\n",
      "2018-12-07T16:12:47.511275: step 875, loss 1.13813, acc 0.53125\n",
      "2018-12-07T16:12:48.520035: step 876, loss 1.17345, acc 0.46875\n",
      "2018-12-07T16:12:49.752526: step 877, loss 1.08463, acc 0.46875\n",
      "2018-12-07T16:12:50.798612: step 878, loss 1.183, acc 0.53125\n",
      "2018-12-07T16:12:51.867416: step 879, loss 1.19819, acc 0.578125\n",
      "2018-12-07T16:12:52.921385: step 880, loss 1.39516, acc 0.453125\n",
      "2018-12-07T16:12:54.088613: step 881, loss 1.28783, acc 0.421875\n",
      "2018-12-07T16:12:55.384096: step 882, loss 1.24129, acc 0.53125\n",
      "2018-12-07T16:12:56.582059: step 883, loss 1.11968, acc 0.59375\n",
      "2018-12-07T16:12:57.709065: step 884, loss 1.22834, acc 0.53125\n",
      "2018-12-07T16:12:59.045312: step 885, loss 1.27485, acc 0.5\n",
      "2018-12-07T16:13:00.242957: step 886, loss 1.35619, acc 0.46875\n",
      "2018-12-07T16:13:01.296979: step 887, loss 1.16111, acc 0.5625\n",
      "2018-12-07T16:13:02.312568: step 888, loss 1.136, acc 0.546875\n",
      "2018-12-07T16:13:03.534190: step 889, loss 1.01771, acc 0.625\n",
      "2018-12-07T16:13:04.745671: step 890, loss 1.23657, acc 0.515625\n",
      "2018-12-07T16:13:05.788773: step 891, loss 1.35787, acc 0.453125\n",
      "2018-12-07T16:13:06.815654: step 892, loss 1.26986, acc 0.546875\n",
      "2018-12-07T16:13:08.114527: step 893, loss 1.05663, acc 0.546875\n",
      "2018-12-07T16:13:09.161150: step 894, loss 1.26739, acc 0.484375\n",
      "2018-12-07T16:13:10.136220: step 895, loss 1.10857, acc 0.546875\n",
      "2018-12-07T16:13:11.228844: step 896, loss 1.47129, acc 0.421875\n",
      "2018-12-07T16:13:12.301729: step 897, loss 1.25317, acc 0.453125\n",
      "2018-12-07T16:13:13.394372: step 898, loss 1.07399, acc 0.578125\n",
      "2018-12-07T16:13:14.596559: step 899, loss 1.15766, acc 0.484375\n",
      "2018-12-07T16:13:15.692169: step 900, loss 1.19748, acc 0.546875\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T16:13:17.019622: step 900, loss 1.09938, acc 0.612\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-900\n",
      "\n",
      "2018-12-07T16:13:18.977016: step 901, loss 1.13756, acc 0.53125\n",
      "2018-12-07T16:13:20.383207: step 902, loss 1.10975, acc 0.640625\n",
      "2018-12-07T16:13:21.675221: step 903, loss 1.16656, acc 0.5625\n",
      "2018-12-07T16:13:23.109913: step 904, loss 1.19645, acc 0.484375\n",
      "2018-12-07T16:13:24.225484: step 905, loss 1.23618, acc 0.484375\n",
      "2018-12-07T16:13:25.760016: step 906, loss 1.13893, acc 0.5\n",
      "2018-12-07T16:13:27.352211: step 907, loss 1.11913, acc 0.609375\n",
      "2018-12-07T16:13:28.427472: step 908, loss 1.22659, acc 0.5\n",
      "2018-12-07T16:13:29.485773: step 909, loss 1.24154, acc 0.53125\n",
      "2018-12-07T16:13:30.544082: step 910, loss 1.31065, acc 0.453125\n",
      "2018-12-07T16:13:31.782731: step 911, loss 1.15309, acc 0.53125\n",
      "2018-12-07T16:13:32.840211: step 912, loss 1.26362, acc 0.421875\n",
      "2018-12-07T16:13:34.157018: step 913, loss 1.33691, acc 0.484375\n",
      "2018-12-07T16:13:35.452029: step 914, loss 1.25921, acc 0.453125\n",
      "2018-12-07T16:13:36.521618: step 915, loss 1.34261, acc 0.484375\n",
      "2018-12-07T16:13:38.097190: step 916, loss 1.37148, acc 0.375\n",
      "2018-12-07T16:13:39.211461: step 917, loss 1.28858, acc 0.46875\n",
      "2018-12-07T16:13:40.317756: step 918, loss 1.31236, acc 0.46875\n",
      "2018-12-07T16:13:41.744721: step 919, loss 1.15484, acc 0.5\n",
      "2018-12-07T16:13:43.031849: step 920, loss 1.16039, acc 0.53125\n",
      "2018-12-07T16:13:44.257928: step 921, loss 1.15447, acc 0.515625\n",
      "2018-12-07T16:13:45.402979: step 922, loss 1.27687, acc 0.53125\n",
      "2018-12-07T16:13:46.629774: step 923, loss 1.25469, acc 0.484375\n",
      "2018-12-07T16:13:47.790911: step 924, loss 1.24292, acc 0.484375\n",
      "2018-12-07T16:13:48.964358: step 925, loss 1.34157, acc 0.390625\n",
      "2018-12-07T16:13:50.292244: step 926, loss 1.09707, acc 0.484375\n",
      "2018-12-07T16:13:51.584295: step 927, loss 1.39163, acc 0.375\n",
      "2018-12-07T16:13:52.628886: step 928, loss 1.26338, acc 0.53125\n",
      "2018-12-07T16:13:53.710723: step 929, loss 1.17553, acc 0.5625\n",
      "2018-12-07T16:13:54.836869: step 930, loss 1.19058, acc 0.625\n",
      "2018-12-07T16:13:55.953065: step 931, loss 1.33844, acc 0.421875\n",
      "2018-12-07T16:13:57.251213: step 932, loss 1.24574, acc 0.46875\n",
      "2018-12-07T16:13:58.737767: step 933, loss 1.32977, acc 0.453125\n",
      "2018-12-07T16:13:59.930762: step 934, loss 1.13621, acc 0.59375\n",
      "2018-12-07T16:14:01.051976: step 935, loss 1.14765, acc 0.53125\n",
      "2018-12-07T16:14:02.254102: step 936, loss 1.21711, acc 0.453125\n",
      "2018-12-07T16:14:03.577342: step 937, loss 1.03155, acc 0.65625\n",
      "2018-12-07T16:14:04.719573: step 938, loss 1.21531, acc 0.53125\n",
      "2018-12-07T16:14:06.206001: step 939, loss 1.20709, acc 0.578125\n",
      "2018-12-07T16:14:07.522550: step 940, loss 1.22421, acc 0.484375\n",
      "2018-12-07T16:14:08.783247: step 941, loss 1.27027, acc 0.5\n",
      "2018-12-07T16:14:09.830225: step 942, loss 1.14532, acc 0.53125\n",
      "2018-12-07T16:14:11.088126: step 943, loss 0.948628, acc 0.6875\n",
      "2018-12-07T16:14:12.440104: step 944, loss 1.20453, acc 0.5\n",
      "2018-12-07T16:14:13.997805: step 945, loss 1.21999, acc 0.515625\n",
      "2018-12-07T16:14:15.275498: step 946, loss 1.38952, acc 0.421875\n",
      "2018-12-07T16:14:16.719696: step 947, loss 1.36821, acc 0.453125\n",
      "2018-12-07T16:14:17.941228: step 948, loss 1.19959, acc 0.578125\n",
      "2018-12-07T16:14:19.186024: step 949, loss 1.24753, acc 0.453125\n",
      "2018-12-07T16:14:20.629831: step 950, loss 1.24841, acc 0.484375\n",
      "2018-12-07T16:14:21.701239: step 951, loss 1.33705, acc 0.484375\n",
      "2018-12-07T16:14:22.956888: step 952, loss 1.42567, acc 0.46875\n",
      "2018-12-07T16:14:24.053070: step 953, loss 1.27666, acc 0.578125\n",
      "2018-12-07T16:14:25.631931: step 954, loss 1.20541, acc 0.546875\n",
      "2018-12-07T16:14:27.025127: step 955, loss 1.27664, acc 0.453125\n",
      "2018-12-07T16:14:28.301587: step 956, loss 1.13337, acc 0.546875\n",
      "2018-12-07T16:14:29.475639: step 957, loss 1.25426, acc 0.5625\n",
      "2018-12-07T16:14:30.535554: step 958, loss 0.955472, acc 0.6875\n",
      "2018-12-07T16:14:31.783577: step 959, loss 1.2281, acc 0.5\n",
      "2018-12-07T16:14:33.208035: step 960, loss 1.22618, acc 0.5\n",
      "2018-12-07T16:14:34.709988: step 961, loss 1.1805, acc 0.5\n",
      "2018-12-07T16:14:36.365682: step 962, loss 1.17691, acc 0.515625\n",
      "2018-12-07T16:14:38.187159: step 963, loss 1.27353, acc 0.453125\n",
      "2018-12-07T16:14:39.335905: step 964, loss 1.1735, acc 0.5\n",
      "2018-12-07T16:14:40.612181: step 965, loss 1.28431, acc 0.515625\n",
      "2018-12-07T16:14:42.114984: step 966, loss 1.09509, acc 0.5625\n",
      "2018-12-07T16:14:43.354528: step 967, loss 1.15834, acc 0.5\n",
      "2018-12-07T16:14:45.023419: step 968, loss 1.28719, acc 0.40625\n",
      "2018-12-07T16:14:46.140169: step 969, loss 1.12307, acc 0.5625\n",
      "2018-12-07T16:14:47.558329: step 970, loss 1.1204, acc 0.53125\n",
      "2018-12-07T16:14:48.996603: step 971, loss 1.24024, acc 0.453125\n",
      "2018-12-07T16:14:50.175755: step 972, loss 1.27044, acc 0.46875\n",
      "2018-12-07T16:14:51.432082: step 973, loss 1.12907, acc 0.578125\n",
      "2018-12-07T16:14:52.910277: step 974, loss 1.18612, acc 0.5\n",
      "2018-12-07T16:14:54.054023: step 975, loss 1.43721, acc 0.40625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07T16:14:55.071347: step 976, loss 0.996893, acc 0.625\n",
      "2018-12-07T16:14:56.330694: step 977, loss 1.40003, acc 0.4375\n",
      "2018-12-07T16:14:57.675702: step 978, loss 1.04839, acc 0.484375\n",
      "2018-12-07T16:14:58.835547: step 979, loss 1.23203, acc 0.515625\n",
      "2018-12-07T16:15:00.135182: step 980, loss 1.31429, acc 0.46875\n",
      "2018-12-07T16:15:01.347622: step 981, loss 1.35987, acc 0.578125\n",
      "2018-12-07T16:15:02.567690: step 982, loss 1.1891, acc 0.453125\n",
      "2018-12-07T16:15:03.785314: step 983, loss 1.1634, acc 0.53125\n",
      "2018-12-07T16:15:05.364673: step 984, loss 1.18812, acc 0.515625\n",
      "2018-12-07T16:15:06.423207: step 985, loss 1.34452, acc 0.40625\n",
      "2018-12-07T16:15:07.593002: step 986, loss 1.29823, acc 0.46875\n",
      "2018-12-07T16:15:08.720630: step 987, loss 1.40134, acc 0.484375\n",
      "2018-12-07T16:15:09.850872: step 988, loss 1.30051, acc 0.40625\n",
      "2018-12-07T16:15:11.171723: step 989, loss 1.41545, acc 0.5\n",
      "2018-12-07T16:15:12.483284: step 990, loss 1.25022, acc 0.5\n",
      "2018-12-07T16:15:13.709678: step 991, loss 1.18359, acc 0.5\n",
      "2018-12-07T16:15:14.765968: step 992, loss 1.2224, acc 0.484375\n",
      "2018-12-07T16:15:16.240306: step 993, loss 1.2122, acc 0.484375\n",
      "2018-12-07T16:15:17.562845: step 994, loss 1.11677, acc 0.53125\n",
      "2018-12-07T16:15:18.771109: step 995, loss 1.06025, acc 0.5625\n",
      "2018-12-07T16:15:20.052211: step 996, loss 1.36996, acc 0.515625\n",
      "2018-12-07T16:15:21.148249: step 997, loss 0.974073, acc 0.625\n",
      "2018-12-07T16:15:22.215312: step 998, loss 1.238, acc 0.46875\n",
      "2018-12-07T16:15:23.758807: step 999, loss 1.32814, acc 0.4375\n",
      "2018-12-07T16:15:24.998870: step 1000, loss 1.18266, acc 0.421875\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T16:15:26.235561: step 1000, loss 1.0557, acc 0.616\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-1000\n",
      "\n",
      "2018-12-07T16:15:28.331722: step 1001, loss 1.2421, acc 0.46875\n",
      "2018-12-07T16:15:29.514807: step 1002, loss 1.20582, acc 0.515625\n",
      "2018-12-07T16:15:30.901023: step 1003, loss 1.49125, acc 0.421875\n",
      "2018-12-07T16:15:32.450997: step 1004, loss 1.15599, acc 0.5625\n",
      "2018-12-07T16:15:34.400181: step 1005, loss 1.16569, acc 0.578125\n",
      "2018-12-07T16:15:35.956992: step 1006, loss 1.14551, acc 0.5\n",
      "2018-12-07T16:15:37.155777: step 1007, loss 1.22469, acc 0.53125\n",
      "2018-12-07T16:15:38.366439: step 1008, loss 1.1389, acc 0.5\n",
      "2018-12-07T16:15:39.589964: step 1009, loss 1.17692, acc 0.546875\n",
      "2018-12-07T16:15:41.264020: step 1010, loss 1.11187, acc 0.5625\n",
      "2018-12-07T16:15:42.758954: step 1011, loss 1.28761, acc 0.4375\n",
      "2018-12-07T16:15:44.173026: step 1012, loss 1.30404, acc 0.4375\n",
      "2018-12-07T16:15:45.325767: step 1013, loss 1.19306, acc 0.515625\n",
      "2018-12-07T16:15:46.833839: step 1014, loss 1.01856, acc 0.625\n",
      "2018-12-07T16:15:48.123450: step 1015, loss 1.02599, acc 0.546875\n",
      "2018-12-07T16:15:49.381102: step 1016, loss 1.2071, acc 0.546875\n",
      "2018-12-07T16:15:50.658910: step 1017, loss 1.40037, acc 0.40625\n",
      "2018-12-07T16:15:52.482609: step 1018, loss 1.11461, acc 0.5625\n",
      "2018-12-07T16:15:53.679109: step 1019, loss 1.27416, acc 0.484375\n",
      "2018-12-07T16:15:55.067506: step 1020, loss 1.22149, acc 0.453125\n",
      "2018-12-07T16:15:56.512212: step 1021, loss 1.22981, acc 0.5\n",
      "2018-12-07T16:15:57.823210: step 1022, loss 0.979532, acc 0.640625\n",
      "2018-12-07T16:15:59.180255: step 1023, loss 1.38015, acc 0.40625\n",
      "2018-12-07T16:16:00.750832: step 1024, loss 1.22277, acc 0.546875\n",
      "2018-12-07T16:16:02.511017: step 1025, loss 1.03493, acc 0.65625\n",
      "2018-12-07T16:16:03.792112: step 1026, loss 1.21278, acc 0.546875\n",
      "2018-12-07T16:16:05.093907: step 1027, loss 1.17062, acc 0.46875\n",
      "2018-12-07T16:16:06.584795: step 1028, loss 1.32691, acc 0.453125\n",
      "2018-12-07T16:16:08.044530: step 1029, loss 1.40116, acc 0.4375\n",
      "2018-12-07T16:16:09.344123: step 1030, loss 1.166, acc 0.46875\n",
      "2018-12-07T16:16:10.727434: step 1031, loss 1.25755, acc 0.390625\n",
      "2018-12-07T16:16:11.994809: step 1032, loss 1.28439, acc 0.4375\n",
      "2018-12-07T16:16:13.456122: step 1033, loss 1.38877, acc 0.46875\n",
      "2018-12-07T16:16:14.782341: step 1034, loss 1.03494, acc 0.515625\n",
      "2018-12-07T16:16:16.256393: step 1035, loss 1.18722, acc 0.53125\n",
      "2018-12-07T16:16:17.815994: step 1036, loss 1.2359, acc 0.53125\n",
      "2018-12-07T16:16:19.871633: step 1037, loss 1.2918, acc 0.4375\n",
      "2018-12-07T16:16:21.199072: step 1038, loss 1.04686, acc 0.59375\n",
      "2018-12-07T16:16:22.618252: step 1039, loss 1.13262, acc 0.578125\n",
      "2018-12-07T16:16:23.930041: step 1040, loss 1.20502, acc 0.59375\n",
      "2018-12-07T16:16:25.419837: step 1041, loss 1.21739, acc 0.484375\n",
      "2018-12-07T16:16:26.738832: step 1042, loss 1.25642, acc 0.53125\n",
      "2018-12-07T16:16:28.610669: step 1043, loss 1.35539, acc 0.421875\n",
      "2018-12-07T16:16:29.936405: step 1044, loss 1.0952, acc 0.640625\n",
      "2018-12-07T16:16:31.444594: step 1045, loss 1.30483, acc 0.5\n",
      "2018-12-07T16:16:32.819615: step 1046, loss 1.16763, acc 0.578125\n",
      "2018-12-07T16:16:34.117594: step 1047, loss 1.23529, acc 0.515625\n",
      "2018-12-07T16:16:35.497992: step 1048, loss 1.14813, acc 0.546875\n",
      "2018-12-07T16:16:36.709256: step 1049, loss 1.32957, acc 0.546875\n",
      "2018-12-07T16:16:38.228236: step 1050, loss 1.37431, acc 0.34375\n",
      "2018-12-07T16:16:39.707504: step 1051, loss 1.12192, acc 0.515625\n",
      "2018-12-07T16:16:40.888748: step 1052, loss 1.08187, acc 0.53125\n",
      "2018-12-07T16:16:42.302500: step 1053, loss 1.26989, acc 0.5\n",
      "2018-12-07T16:16:43.743612: step 1054, loss 1.22448, acc 0.5625\n",
      "2018-12-07T16:16:45.190525: step 1055, loss 1.31305, acc 0.484375\n",
      "2018-12-07T16:16:46.626497: step 1056, loss 1.16816, acc 0.53125\n",
      "2018-12-07T16:16:47.984510: step 1057, loss 1.25228, acc 0.5\n",
      "2018-12-07T16:16:49.449091: step 1058, loss 1.1925, acc 0.53125\n",
      "2018-12-07T16:16:50.668074: step 1059, loss 1.03823, acc 0.515625\n",
      "2018-12-07T16:16:51.967623: step 1060, loss 1.07512, acc 0.59375\n",
      "2018-12-07T16:16:53.485871: step 1061, loss 1.19702, acc 0.484375\n",
      "2018-12-07T16:16:54.913464: step 1062, loss 1.22054, acc 0.484375\n",
      "2018-12-07T16:16:56.470627: step 1063, loss 1.31228, acc 0.484375\n",
      "2018-12-07T16:16:58.004490: step 1064, loss 1.26913, acc 0.484375\n",
      "2018-12-07T16:16:59.216415: step 1065, loss 1.12278, acc 0.53125\n",
      "2018-12-07T16:17:00.602039: step 1066, loss 1.21491, acc 0.515625\n",
      "2018-12-07T16:17:01.864547: step 1067, loss 1.05274, acc 0.5625\n",
      "2018-12-07T16:17:03.097037: step 1068, loss 1.13175, acc 0.5625\n",
      "2018-12-07T16:17:04.876398: step 1069, loss 1.2611, acc 0.484375\n",
      "2018-12-07T16:17:06.418408: step 1070, loss 1.18995, acc 0.5\n",
      "2018-12-07T16:17:08.170657: step 1071, loss 1.09146, acc 0.5625\n",
      "2018-12-07T16:17:09.493828: step 1072, loss 1.20735, acc 0.484375\n",
      "2018-12-07T16:17:10.842951: step 1073, loss 1.16707, acc 0.515625\n",
      "2018-12-07T16:17:12.195065: step 1074, loss 1.17201, acc 0.546875\n",
      "2018-12-07T16:17:13.779069: step 1075, loss 1.19699, acc 0.53125\n",
      "2018-12-07T16:17:15.345505: step 1076, loss 1.13248, acc 0.46875\n",
      "2018-12-07T16:17:16.730090: step 1077, loss 1.08858, acc 0.578125\n",
      "2018-12-07T16:17:18.105693: step 1078, loss 1.2632, acc 0.421875\n",
      "2018-12-07T16:17:19.464396: step 1079, loss 1.09269, acc 0.671875\n",
      "2018-12-07T16:17:20.695899: step 1080, loss 1.22057, acc 0.453125\n",
      "2018-12-07T16:17:22.201793: step 1081, loss 1.18306, acc 0.515625\n",
      "2018-12-07T16:17:23.842818: step 1082, loss 1.10431, acc 0.609375\n",
      "2018-12-07T16:17:25.333317: step 1083, loss 1.20031, acc 0.5\n",
      "2018-12-07T16:17:26.758449: step 1084, loss 1.10088, acc 0.609375\n",
      "2018-12-07T16:17:28.341539: step 1085, loss 1.40067, acc 0.40625\n",
      "2018-12-07T16:17:29.998283: step 1086, loss 1.0569, acc 0.546875\n",
      "2018-12-07T16:17:31.214574: step 1087, loss 1.08078, acc 0.53125\n",
      "2018-12-07T16:17:32.905703: step 1088, loss 1.29037, acc 0.484375\n",
      "2018-12-07T16:17:34.290901: step 1089, loss 1.13019, acc 0.46875\n",
      "2018-12-07T16:17:36.102450: step 1090, loss 1.25862, acc 0.484375\n",
      "2018-12-07T16:17:37.538359: step 1091, loss 1.30612, acc 0.453125\n",
      "2018-12-07T16:17:39.239358: step 1092, loss 1.2977, acc 0.515625\n",
      "2018-12-07T16:17:40.908327: step 1093, loss 1.35389, acc 0.4375\n",
      "2018-12-07T16:17:42.308655: step 1094, loss 1.15108, acc 0.546875\n",
      "2018-12-07T16:17:44.006147: step 1095, loss 1.24131, acc 0.515625\n",
      "2018-12-07T16:17:45.949027: step 1096, loss 1.1887, acc 0.484375\n",
      "2018-12-07T16:17:47.455294: step 1097, loss 0.984515, acc 0.640625\n",
      "2018-12-07T16:17:49.273611: step 1098, loss 1.15147, acc 0.578125\n",
      "2018-12-07T16:17:51.041788: step 1099, loss 1.31808, acc 0.546875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07T16:17:52.978892: step 1100, loss 1.32027, acc 0.4375\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T16:17:54.212594: step 1100, loss 1.06712, acc 0.616\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-1100\n",
      "\n",
      "2018-12-07T16:17:56.525562: step 1101, loss 1.36586, acc 0.484375\n",
      "2018-12-07T16:17:58.302281: step 1102, loss 1.22045, acc 0.53125\n",
      "2018-12-07T16:17:59.835168: step 1103, loss 1.22396, acc 0.53125\n",
      "2018-12-07T16:18:01.895808: step 1104, loss 1.30455, acc 0.421875\n",
      "2018-12-07T16:18:03.552282: step 1105, loss 1.06162, acc 0.578125\n",
      "2018-12-07T16:18:05.308361: step 1106, loss 1.07204, acc 0.59375\n",
      "2018-12-07T16:18:07.121072: step 1107, loss 1.06426, acc 0.5625\n",
      "2018-12-07T16:18:09.201197: step 1108, loss 0.904827, acc 0.671875\n",
      "2018-12-07T16:18:10.717327: step 1109, loss 1.14113, acc 0.609375\n",
      "2018-12-07T16:18:12.269189: step 1110, loss 1.12511, acc 0.5\n",
      "2018-12-07T16:18:14.109480: step 1111, loss 1.3198, acc 0.46875\n",
      "2018-12-07T16:18:15.501519: step 1112, loss 1.2723, acc 0.5\n",
      "2018-12-07T16:18:17.850204: step 1113, loss 1.16746, acc 0.515625\n",
      "2018-12-07T16:18:19.594873: step 1114, loss 1.25265, acc 0.46875\n",
      "2018-12-07T16:18:21.528198: step 1115, loss 1.12153, acc 0.578125\n",
      "2018-12-07T16:18:23.344320: step 1116, loss 1.27056, acc 0.46875\n",
      "2018-12-07T16:18:24.953044: step 1117, loss 1.30662, acc 0.5\n",
      "2018-12-07T16:18:26.632891: step 1118, loss 1.09323, acc 0.59375\n",
      "2018-12-07T16:18:28.083471: step 1119, loss 1.07926, acc 0.609375\n",
      "2018-12-07T16:18:29.554445: step 1120, loss 1.32611, acc 0.390625\n",
      "2018-12-07T16:18:31.271267: step 1121, loss 1.05373, acc 0.609375\n",
      "2018-12-07T16:18:32.777445: step 1122, loss 1.03969, acc 0.578125\n",
      "2018-12-07T16:18:34.632905: step 1123, loss 1.18013, acc 0.515625\n",
      "2018-12-07T16:18:36.384732: step 1124, loss 1.32396, acc 0.484375\n",
      "2018-12-07T16:18:38.108688: step 1125, loss 1.10417, acc 0.53125\n",
      "2018-12-07T16:18:39.798830: step 1126, loss 1.28312, acc 0.5625\n",
      "2018-12-07T16:18:41.311413: step 1127, loss 1.29711, acc 0.484375\n",
      "2018-12-07T16:18:42.699827: step 1128, loss 1.35871, acc 0.375\n",
      "2018-12-07T16:18:44.417293: step 1129, loss 1.12494, acc 0.546875\n",
      "2018-12-07T16:18:46.043344: step 1130, loss 1.21552, acc 0.515625\n",
      "2018-12-07T16:18:48.534624: step 1131, loss 1.2193, acc 0.46875\n",
      "2018-12-07T16:18:50.001810: step 1132, loss 1.18077, acc 0.578125\n",
      "2018-12-07T16:18:51.904528: step 1133, loss 1.27161, acc 0.46875\n",
      "2018-12-07T16:18:53.417333: step 1134, loss 1.18526, acc 0.4375\n",
      "2018-12-07T16:18:55.141603: step 1135, loss 0.977285, acc 0.65625\n",
      "2018-12-07T16:18:56.553286: step 1136, loss 1.14033, acc 0.5\n",
      "2018-12-07T16:18:58.296054: step 1137, loss 1.32707, acc 0.421875\n",
      "2018-12-07T16:18:59.987298: step 1138, loss 1.05569, acc 0.59375\n",
      "2018-12-07T16:19:01.751007: step 1139, loss 1.0778, acc 0.5625\n",
      "2018-12-07T16:19:03.803568: step 1140, loss 1.1461, acc 0.578125\n",
      "2018-12-07T16:19:05.452223: step 1141, loss 1.27491, acc 0.40625\n",
      "2018-12-07T16:19:07.431159: step 1142, loss 1.28396, acc 0.5\n",
      "2018-12-07T16:19:09.092377: step 1143, loss 1.15399, acc 0.59375\n",
      "2018-12-07T16:19:10.509428: step 1144, loss 1.0688, acc 0.53125\n",
      "2018-12-07T16:19:11.989512: step 1145, loss 1.38282, acc 0.453125\n",
      "2018-12-07T16:19:13.595816: step 1146, loss 1.21762, acc 0.484375\n",
      "2018-12-07T16:19:14.979709: step 1147, loss 1.17038, acc 0.484375\n",
      "2018-12-07T16:19:16.519817: step 1148, loss 1.28049, acc 0.515625\n",
      "2018-12-07T16:19:18.376643: step 1149, loss 1.14964, acc 0.5\n",
      "2018-12-07T16:19:20.354723: step 1150, loss 1.04403, acc 0.5625\n",
      "2018-12-07T16:19:21.888077: step 1151, loss 1.34329, acc 0.453125\n",
      "2018-12-07T16:19:23.437728: step 1152, loss 1.20243, acc 0.484375\n",
      "2018-12-07T16:19:25.034530: step 1153, loss 1.19601, acc 0.5\n",
      "2018-12-07T16:19:26.724285: step 1154, loss 1.12565, acc 0.546875\n",
      "2018-12-07T16:19:28.360574: step 1155, loss 1.14989, acc 0.578125\n",
      "2018-12-07T16:19:30.228852: step 1156, loss 0.996653, acc 0.609375\n",
      "2018-12-07T16:19:31.959296: step 1157, loss 1.14367, acc 0.5\n",
      "2018-12-07T16:19:33.557529: step 1158, loss 1.20762, acc 0.515625\n",
      "2018-12-07T16:19:35.555255: step 1159, loss 1.37189, acc 0.375\n",
      "2018-12-07T16:19:37.192706: step 1160, loss 1.25279, acc 0.546875\n",
      "2018-12-07T16:19:38.342567: step 1161, loss 1.05709, acc 0.586957\n",
      "2018-12-07T16:19:39.379964: step 1162, loss 1.22473, acc 0.53125\n",
      "2018-12-07T16:19:40.348781: step 1163, loss 1.14798, acc 0.5\n",
      "2018-12-07T16:19:41.353160: step 1164, loss 1.26098, acc 0.515625\n",
      "2018-12-07T16:19:42.350637: step 1165, loss 1.21174, acc 0.4375\n",
      "2018-12-07T16:19:43.370413: step 1166, loss 1.00878, acc 0.5625\n",
      "2018-12-07T16:19:44.340800: step 1167, loss 1.1343, acc 0.515625\n",
      "2018-12-07T16:19:45.367506: step 1168, loss 1.1942, acc 0.578125\n",
      "2018-12-07T16:19:46.317996: step 1169, loss 1.07749, acc 0.53125\n",
      "2018-12-07T16:19:47.358550: step 1170, loss 1.17003, acc 0.546875\n",
      "2018-12-07T16:19:48.368101: step 1171, loss 0.990362, acc 0.59375\n",
      "2018-12-07T16:19:49.510635: step 1172, loss 1.09941, acc 0.59375\n",
      "2018-12-07T16:19:50.479984: step 1173, loss 1.12479, acc 0.5625\n",
      "2018-12-07T16:19:51.515392: step 1174, loss 1.15695, acc 0.5625\n",
      "2018-12-07T16:19:52.511054: step 1175, loss 1.17744, acc 0.5\n",
      "2018-12-07T16:19:53.563589: step 1176, loss 1.18577, acc 0.546875\n",
      "2018-12-07T16:19:54.539377: step 1177, loss 1.09143, acc 0.59375\n",
      "2018-12-07T16:19:55.567648: step 1178, loss 1.27375, acc 0.5\n",
      "2018-12-07T16:19:56.535366: step 1179, loss 1.06324, acc 0.578125\n",
      "2018-12-07T16:19:57.569500: step 1180, loss 0.972846, acc 0.65625\n",
      "2018-12-07T16:19:58.737067: step 1181, loss 1.16836, acc 0.5\n",
      "2018-12-07T16:19:59.754100: step 1182, loss 1.00893, acc 0.546875\n",
      "2018-12-07T16:20:00.730695: step 1183, loss 1.4014, acc 0.390625\n",
      "2018-12-07T16:20:01.723795: step 1184, loss 1.00771, acc 0.640625\n",
      "2018-12-07T16:20:02.677377: step 1185, loss 1.26523, acc 0.515625\n",
      "2018-12-07T16:20:03.829497: step 1186, loss 1.20265, acc 0.609375\n",
      "2018-12-07T16:20:04.826528: step 1187, loss 1.14324, acc 0.609375\n",
      "2018-12-07T16:20:05.834078: step 1188, loss 1.03086, acc 0.625\n",
      "2018-12-07T16:20:06.824328: step 1189, loss 1.34779, acc 0.46875\n",
      "2018-12-07T16:20:07.896683: step 1190, loss 1.1063, acc 0.609375\n",
      "2018-12-07T16:20:09.030462: step 1191, loss 1.14562, acc 0.578125\n",
      "2018-12-07T16:20:10.016080: step 1192, loss 1.09337, acc 0.59375\n",
      "2018-12-07T16:20:11.109307: step 1193, loss 1.28314, acc 0.453125\n",
      "2018-12-07T16:20:12.082502: step 1194, loss 1.01425, acc 0.640625\n",
      "2018-12-07T16:20:13.081009: step 1195, loss 1.19361, acc 0.453125\n",
      "2018-12-07T16:20:14.048550: step 1196, loss 1.04893, acc 0.625\n",
      "2018-12-07T16:20:15.058468: step 1197, loss 1.08393, acc 0.59375\n",
      "2018-12-07T16:20:16.285734: step 1198, loss 0.982226, acc 0.609375\n",
      "2018-12-07T16:20:17.311598: step 1199, loss 1.11855, acc 0.53125\n",
      "2018-12-07T16:20:18.456717: step 1200, loss 1.25432, acc 0.515625\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T16:20:19.705376: step 1200, loss 1.01493, acc 0.592\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-1200\n",
      "\n",
      "2018-12-07T16:20:21.344291: step 1201, loss 1.07428, acc 0.5625\n",
      "2018-12-07T16:20:22.403537: step 1202, loss 1.06007, acc 0.59375\n",
      "2018-12-07T16:20:23.451572: step 1203, loss 1.05257, acc 0.546875\n",
      "2018-12-07T16:20:24.654819: step 1204, loss 1.02158, acc 0.609375\n",
      "2018-12-07T16:20:25.688028: step 1205, loss 1.03115, acc 0.65625\n",
      "2018-12-07T16:20:26.828812: step 1206, loss 1.0423, acc 0.515625\n",
      "2018-12-07T16:20:28.092017: step 1207, loss 1.1658, acc 0.484375\n",
      "2018-12-07T16:20:29.319446: step 1208, loss 1.11843, acc 0.5625\n",
      "2018-12-07T16:20:30.749385: step 1209, loss 1.10038, acc 0.59375\n",
      "2018-12-07T16:20:31.786738: step 1210, loss 1.10533, acc 0.515625\n",
      "2018-12-07T16:20:32.839120: step 1211, loss 1.05713, acc 0.53125\n",
      "2018-12-07T16:20:33.818161: step 1212, loss 1.13888, acc 0.515625\n",
      "2018-12-07T16:20:34.871987: step 1213, loss 1.10921, acc 0.59375\n",
      "2018-12-07T16:20:35.889229: step 1214, loss 1.06649, acc 0.578125\n",
      "2018-12-07T16:20:36.909805: step 1215, loss 0.970837, acc 0.5625\n",
      "2018-12-07T16:20:38.133926: step 1216, loss 1.37837, acc 0.421875\n",
      "2018-12-07T16:20:39.295679: step 1217, loss 1.16484, acc 0.515625\n",
      "2018-12-07T16:20:40.345934: step 1218, loss 1.05472, acc 0.546875\n",
      "2018-12-07T16:20:41.496568: step 1219, loss 1.17888, acc 0.34375\n",
      "2018-12-07T16:20:42.676056: step 1220, loss 1.10097, acc 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07T16:20:43.683786: step 1221, loss 1.08152, acc 0.578125\n",
      "2018-12-07T16:20:44.691133: step 1222, loss 1.1806, acc 0.546875\n",
      "2018-12-07T16:20:45.766153: step 1223, loss 1.09771, acc 0.53125\n",
      "2018-12-07T16:20:46.798972: step 1224, loss 1.18261, acc 0.53125\n",
      "2018-12-07T16:20:47.818247: step 1225, loss 1.14119, acc 0.578125\n",
      "2018-12-07T16:20:48.836748: step 1226, loss 1.29757, acc 0.484375\n",
      "2018-12-07T16:20:49.874230: step 1227, loss 0.928054, acc 0.640625\n",
      "2018-12-07T16:20:50.873999: step 1228, loss 1.10877, acc 0.515625\n",
      "2018-12-07T16:20:51.882693: step 1229, loss 1.02679, acc 0.578125\n",
      "2018-12-07T16:20:52.995121: step 1230, loss 1.26459, acc 0.59375\n",
      "2018-12-07T16:20:53.994574: step 1231, loss 1.22759, acc 0.46875\n",
      "2018-12-07T16:20:55.013210: step 1232, loss 1.13917, acc 0.5625\n",
      "2018-12-07T16:20:56.013980: step 1233, loss 1.07388, acc 0.515625\n",
      "2018-12-07T16:20:57.098681: step 1234, loss 1.22454, acc 0.453125\n",
      "2018-12-07T16:20:58.252707: step 1235, loss 1.17974, acc 0.4375\n",
      "2018-12-07T16:20:59.295875: step 1236, loss 1.18468, acc 0.578125\n",
      "2018-12-07T16:21:00.316480: step 1237, loss 1.23055, acc 0.5\n",
      "2018-12-07T16:21:01.488108: step 1238, loss 1.09652, acc 0.625\n",
      "2018-12-07T16:21:02.814719: step 1239, loss 1.37027, acc 0.484375\n",
      "2018-12-07T16:21:04.013772: step 1240, loss 1.09788, acc 0.5\n",
      "2018-12-07T16:21:05.036876: step 1241, loss 0.955579, acc 0.734375\n",
      "2018-12-07T16:21:06.234225: step 1242, loss 1.28399, acc 0.484375\n",
      "2018-12-07T16:21:07.560054: step 1243, loss 0.959317, acc 0.625\n",
      "2018-12-07T16:21:08.717093: step 1244, loss 1.08234, acc 0.640625\n",
      "2018-12-07T16:21:09.967919: step 1245, loss 1.07442, acc 0.578125\n",
      "2018-12-07T16:21:11.314939: step 1246, loss 1.10759, acc 0.609375\n",
      "2018-12-07T16:21:12.417255: step 1247, loss 1.17704, acc 0.5625\n",
      "2018-12-07T16:21:13.609374: step 1248, loss 1.18443, acc 0.515625\n",
      "2018-12-07T16:21:14.660767: step 1249, loss 1.11571, acc 0.59375\n",
      "2018-12-07T16:21:15.885470: step 1250, loss 1.07354, acc 0.59375\n",
      "2018-12-07T16:21:17.116753: step 1251, loss 1.11197, acc 0.515625\n",
      "2018-12-07T16:21:18.286338: step 1252, loss 1.19239, acc 0.4375\n",
      "2018-12-07T16:21:19.419495: step 1253, loss 0.979348, acc 0.640625\n",
      "2018-12-07T16:21:20.640085: step 1254, loss 1.02728, acc 0.546875\n",
      "2018-12-07T16:21:22.230876: step 1255, loss 1.07971, acc 0.59375\n",
      "2018-12-07T16:21:23.280489: step 1256, loss 1.14288, acc 0.546875\n",
      "2018-12-07T16:21:24.494368: step 1257, loss 1.05958, acc 0.625\n",
      "2018-12-07T16:21:25.535854: step 1258, loss 1.08965, acc 0.59375\n",
      "2018-12-07T16:21:26.600082: step 1259, loss 1.10031, acc 0.578125\n",
      "2018-12-07T16:21:27.709702: step 1260, loss 1.2008, acc 0.484375\n",
      "2018-12-07T16:21:28.720510: step 1261, loss 1.15083, acc 0.59375\n",
      "2018-12-07T16:21:29.776662: step 1262, loss 1.06487, acc 0.640625\n",
      "2018-12-07T16:21:30.983864: step 1263, loss 1.08261, acc 0.5625\n",
      "2018-12-07T16:21:31.998289: step 1264, loss 0.982505, acc 0.640625\n",
      "2018-12-07T16:21:33.160407: step 1265, loss 0.969408, acc 0.609375\n",
      "2018-12-07T16:21:34.191753: step 1266, loss 0.955122, acc 0.671875\n",
      "2018-12-07T16:21:35.285312: step 1267, loss 1.10964, acc 0.515625\n",
      "2018-12-07T16:21:36.426878: step 1268, loss 1.07137, acc 0.609375\n",
      "2018-12-07T16:21:37.548282: step 1269, loss 1.12729, acc 0.515625\n",
      "2018-12-07T16:21:38.746132: step 1270, loss 1.13603, acc 0.5625\n",
      "2018-12-07T16:21:39.808112: step 1271, loss 1.17272, acc 0.578125\n",
      "2018-12-07T16:21:40.829753: step 1272, loss 1.26097, acc 0.5625\n",
      "2018-12-07T16:21:41.980211: step 1273, loss 0.985536, acc 0.625\n",
      "2018-12-07T16:21:43.004182: step 1274, loss 1.19292, acc 0.578125\n",
      "2018-12-07T16:21:44.199059: step 1275, loss 0.996354, acc 0.671875\n",
      "2018-12-07T16:21:45.224039: step 1276, loss 1.07249, acc 0.59375\n",
      "2018-12-07T16:21:46.251451: step 1277, loss 0.975792, acc 0.578125\n",
      "2018-12-07T16:21:47.314678: step 1278, loss 1.24263, acc 0.5625\n",
      "2018-12-07T16:21:49.086167: step 1279, loss 1.22605, acc 0.5\n",
      "2018-12-07T16:21:50.240664: step 1280, loss 1.11812, acc 0.484375\n",
      "2018-12-07T16:21:51.355580: step 1281, loss 1.12588, acc 0.546875\n",
      "2018-12-07T16:21:52.525700: step 1282, loss 0.963142, acc 0.5625\n",
      "2018-12-07T16:21:53.569355: step 1283, loss 1.0579, acc 0.5625\n",
      "2018-12-07T16:21:54.735660: step 1284, loss 1.13443, acc 0.53125\n",
      "2018-12-07T16:21:55.883955: step 1285, loss 1.1229, acc 0.546875\n",
      "2018-12-07T16:21:56.902169: step 1286, loss 1.13812, acc 0.578125\n",
      "2018-12-07T16:21:58.109684: step 1287, loss 1.17225, acc 0.5625\n",
      "2018-12-07T16:21:59.167540: step 1288, loss 1.03178, acc 0.5625\n",
      "2018-12-07T16:22:00.154784: step 1289, loss 1.04225, acc 0.609375\n",
      "2018-12-07T16:22:01.202115: step 1290, loss 1.01643, acc 0.546875\n",
      "2018-12-07T16:22:02.341650: step 1291, loss 0.938165, acc 0.640625\n",
      "2018-12-07T16:22:03.643860: step 1292, loss 1.23007, acc 0.515625\n",
      "2018-12-07T16:22:04.755389: step 1293, loss 1.07357, acc 0.53125\n",
      "2018-12-07T16:22:06.193346: step 1294, loss 0.901814, acc 0.671875\n",
      "2018-12-07T16:22:07.276455: step 1295, loss 1.1666, acc 0.546875\n",
      "2018-12-07T16:22:08.431703: step 1296, loss 1.03209, acc 0.640625\n",
      "2018-12-07T16:22:09.525597: step 1297, loss 0.972005, acc 0.578125\n",
      "2018-12-07T16:22:10.567617: step 1298, loss 1.00968, acc 0.609375\n",
      "2018-12-07T16:22:11.577508: step 1299, loss 1.05891, acc 0.578125\n",
      "2018-12-07T16:22:12.630691: step 1300, loss 1.05948, acc 0.625\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T16:22:13.870376: step 1300, loss 1.0046, acc 0.656\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-1300\n",
      "\n",
      "2018-12-07T16:22:15.446733: step 1301, loss 1.06517, acc 0.59375\n",
      "2018-12-07T16:22:16.610608: step 1302, loss 1.25108, acc 0.53125\n",
      "2018-12-07T16:22:17.919845: step 1303, loss 1.13516, acc 0.578125\n",
      "2018-12-07T16:22:19.273583: step 1304, loss 1.05494, acc 0.5625\n",
      "2018-12-07T16:22:20.304374: step 1305, loss 1.08287, acc 0.5625\n",
      "2018-12-07T16:22:21.524659: step 1306, loss 1.34405, acc 0.421875\n",
      "2018-12-07T16:22:22.573579: step 1307, loss 1.03292, acc 0.59375\n",
      "2018-12-07T16:22:23.633914: step 1308, loss 1.18798, acc 0.5\n",
      "2018-12-07T16:22:24.755435: step 1309, loss 1.01497, acc 0.515625\n",
      "2018-12-07T16:22:26.424672: step 1310, loss 1.08891, acc 0.546875\n",
      "2018-12-07T16:22:28.058651: step 1311, loss 1.10568, acc 0.5\n",
      "2018-12-07T16:22:29.316349: step 1312, loss 1.11008, acc 0.53125\n",
      "2018-12-07T16:22:30.362526: step 1313, loss 1.09986, acc 0.515625\n",
      "2018-12-07T16:22:31.411851: step 1314, loss 0.980955, acc 0.609375\n",
      "2018-12-07T16:22:32.455653: step 1315, loss 1.11295, acc 0.53125\n",
      "2018-12-07T16:22:33.750597: step 1316, loss 0.83947, acc 0.6875\n",
      "2018-12-07T16:22:34.978818: step 1317, loss 1.18413, acc 0.484375\n",
      "2018-12-07T16:22:36.028202: step 1318, loss 1.1165, acc 0.484375\n",
      "2018-12-07T16:22:37.392616: step 1319, loss 1.10388, acc 0.578125\n",
      "2018-12-07T16:22:38.442097: step 1320, loss 1.09453, acc 0.484375\n",
      "2018-12-07T16:22:39.536384: step 1321, loss 1.0419, acc 0.59375\n",
      "2018-12-07T16:22:40.628727: step 1322, loss 1.17132, acc 0.46875\n",
      "2018-12-07T16:22:41.868636: step 1323, loss 1.18546, acc 0.5625\n",
      "2018-12-07T16:22:43.074423: step 1324, loss 1.18825, acc 0.453125\n",
      "2018-12-07T16:22:44.358703: step 1325, loss 1.15424, acc 0.5\n",
      "2018-12-07T16:22:45.419693: step 1326, loss 1.112, acc 0.5625\n",
      "2018-12-07T16:22:46.610934: step 1327, loss 1.07362, acc 0.609375\n",
      "2018-12-07T16:22:47.924427: step 1328, loss 0.959657, acc 0.59375\n",
      "2018-12-07T16:22:49.302598: step 1329, loss 0.992328, acc 0.625\n",
      "2018-12-07T16:22:50.371741: step 1330, loss 1.02902, acc 0.625\n",
      "2018-12-07T16:22:51.499641: step 1331, loss 0.780267, acc 0.75\n",
      "2018-12-07T16:22:52.837958: step 1332, loss 1.09649, acc 0.515625\n",
      "2018-12-07T16:22:54.075960: step 1333, loss 1.24529, acc 0.515625\n",
      "2018-12-07T16:22:55.177255: step 1334, loss 1.03059, acc 0.546875\n",
      "2018-12-07T16:22:56.429682: step 1335, loss 1.30098, acc 0.5\n",
      "2018-12-07T16:22:57.646579: step 1336, loss 1.02293, acc 0.578125\n",
      "2018-12-07T16:22:58.916108: step 1337, loss 1.01193, acc 0.609375\n",
      "2018-12-07T16:23:00.025408: step 1338, loss 1.25067, acc 0.453125\n",
      "2018-12-07T16:23:01.475979: step 1339, loss 1.04344, acc 0.65625\n",
      "2018-12-07T16:23:02.798263: step 1340, loss 1.13032, acc 0.484375\n",
      "2018-12-07T16:23:03.932546: step 1341, loss 1.09353, acc 0.609375\n",
      "2018-12-07T16:23:05.030899: step 1342, loss 1.20125, acc 0.453125\n",
      "2018-12-07T16:23:06.104970: step 1343, loss 1.03778, acc 0.578125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07T16:23:07.192939: step 1344, loss 1.18527, acc 0.484375\n",
      "2018-12-07T16:23:08.446243: step 1345, loss 1.10692, acc 0.53125\n",
      "2018-12-07T16:23:09.772236: step 1346, loss 1.20991, acc 0.5\n",
      "2018-12-07T16:23:10.830256: step 1347, loss 1.01369, acc 0.609375\n",
      "2018-12-07T16:23:12.000307: step 1348, loss 1.12038, acc 0.578125\n",
      "2018-12-07T16:23:13.215545: step 1349, loss 1.17929, acc 0.515625\n",
      "2018-12-07T16:23:14.382582: step 1350, loss 0.968386, acc 0.625\n",
      "2018-12-07T16:23:15.576126: step 1351, loss 1.07766, acc 0.46875\n",
      "2018-12-07T16:23:16.652938: step 1352, loss 0.919046, acc 0.734375\n",
      "2018-12-07T16:23:17.916285: step 1353, loss 1.09398, acc 0.671875\n",
      "2018-12-07T16:23:19.366107: step 1354, loss 1.19575, acc 0.515625\n",
      "2018-12-07T16:23:20.726679: step 1355, loss 1.04918, acc 0.625\n",
      "2018-12-07T16:23:22.194878: step 1356, loss 1.11329, acc 0.546875\n",
      "2018-12-07T16:23:23.266338: step 1357, loss 1.18869, acc 0.53125\n",
      "2018-12-07T16:23:24.843822: step 1358, loss 0.847276, acc 0.703125\n",
      "2018-12-07T16:23:26.179666: step 1359, loss 1.22578, acc 0.53125\n",
      "2018-12-07T16:23:27.375210: step 1360, loss 1.49228, acc 0.421875\n",
      "2018-12-07T16:23:28.574480: step 1361, loss 1.04351, acc 0.53125\n",
      "2018-12-07T16:23:29.802527: step 1362, loss 1.20313, acc 0.546875\n",
      "2018-12-07T16:23:30.985436: step 1363, loss 1.04784, acc 0.578125\n",
      "2018-12-07T16:23:32.146647: step 1364, loss 0.936943, acc 0.671875\n",
      "2018-12-07T16:23:33.393841: step 1365, loss 1.23329, acc 0.5\n",
      "2018-12-07T16:23:34.585110: step 1366, loss 1.0629, acc 0.6875\n",
      "2018-12-07T16:23:35.786099: step 1367, loss 1.34497, acc 0.34375\n",
      "2018-12-07T16:23:36.874257: step 1368, loss 1.41133, acc 0.5\n",
      "2018-12-07T16:23:38.310847: step 1369, loss 1.02172, acc 0.578125\n",
      "2018-12-07T16:23:39.527161: step 1370, loss 0.927367, acc 0.625\n",
      "2018-12-07T16:23:40.751009: step 1371, loss 1.12157, acc 0.515625\n",
      "2018-12-07T16:23:42.035403: step 1372, loss 1.06363, acc 0.640625\n",
      "2018-12-07T16:23:43.298471: step 1373, loss 1.1391, acc 0.5\n",
      "2018-12-07T16:23:44.452895: step 1374, loss 1.1903, acc 0.5625\n",
      "2018-12-07T16:23:45.529351: step 1375, loss 1.06363, acc 0.546875\n",
      "2018-12-07T16:23:46.730604: step 1376, loss 1.15892, acc 0.53125\n",
      "2018-12-07T16:23:47.912879: step 1377, loss 1.18409, acc 0.5\n",
      "2018-12-07T16:23:49.143010: step 1378, loss 0.972276, acc 0.765625\n",
      "2018-12-07T16:23:50.379333: step 1379, loss 1.21004, acc 0.546875\n",
      "2018-12-07T16:23:51.484779: step 1380, loss 1.00218, acc 0.59375\n",
      "2018-12-07T16:23:52.617872: step 1381, loss 1.04061, acc 0.546875\n",
      "2018-12-07T16:23:54.162635: step 1382, loss 1.07883, acc 0.578125\n",
      "2018-12-07T16:23:55.600391: step 1383, loss 1.05944, acc 0.53125\n",
      "2018-12-07T16:23:56.834361: step 1384, loss 1.05269, acc 0.546875\n",
      "2018-12-07T16:23:58.110183: step 1385, loss 0.939256, acc 0.671875\n",
      "2018-12-07T16:23:59.236858: step 1386, loss 1.14626, acc 0.546875\n",
      "2018-12-07T16:24:00.272877: step 1387, loss 1.06327, acc 0.625\n",
      "2018-12-07T16:24:01.518828: step 1388, loss 0.961695, acc 0.671875\n",
      "2018-12-07T16:24:02.791558: step 1389, loss 1.11958, acc 0.53125\n",
      "2018-12-07T16:24:03.973635: step 1390, loss 1.03863, acc 0.53125\n",
      "2018-12-07T16:24:05.427214: step 1391, loss 1.09937, acc 0.59375\n",
      "2018-12-07T16:24:06.582183: step 1392, loss 1.24944, acc 0.40625\n",
      "2018-12-07T16:24:07.719980: step 1393, loss 0.927426, acc 0.703125\n",
      "2018-12-07T16:24:08.778315: step 1394, loss 0.940282, acc 0.71875\n",
      "2018-12-07T16:24:09.890889: step 1395, loss 0.944814, acc 0.65625\n",
      "2018-12-07T16:24:11.432318: step 1396, loss 1.1217, acc 0.5\n",
      "2018-12-07T16:24:12.571779: step 1397, loss 1.09955, acc 0.515625\n",
      "2018-12-07T16:24:13.635485: step 1398, loss 1.20956, acc 0.515625\n",
      "2018-12-07T16:24:15.001112: step 1399, loss 1.0068, acc 0.65625\n",
      "2018-12-07T16:24:16.195240: step 1400, loss 1.14301, acc 0.5\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T16:24:17.482797: step 1400, loss 0.980414, acc 0.656\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-1400\n",
      "\n",
      "2018-12-07T16:24:19.442552: step 1401, loss 1.17431, acc 0.546875\n",
      "2018-12-07T16:24:20.809843: step 1402, loss 1.10963, acc 0.5625\n",
      "2018-12-07T16:24:22.205891: step 1403, loss 1.00665, acc 0.578125\n",
      "2018-12-07T16:24:23.373519: step 1404, loss 1.00867, acc 0.578125\n",
      "2018-12-07T16:24:24.714265: step 1405, loss 1.05634, acc 0.546875\n",
      "2018-12-07T16:24:25.853174: step 1406, loss 1.31382, acc 0.4375\n",
      "2018-12-07T16:24:27.001317: step 1407, loss 1.07507, acc 0.5625\n",
      "2018-12-07T16:24:28.252418: step 1408, loss 1.06031, acc 0.5625\n",
      "2018-12-07T16:24:29.874982: step 1409, loss 1.23988, acc 0.453125\n",
      "2018-12-07T16:24:31.447178: step 1410, loss 1.03014, acc 0.609375\n",
      "2018-12-07T16:24:32.943637: step 1411, loss 1.34931, acc 0.46875\n",
      "2018-12-07T16:24:34.192337: step 1412, loss 0.998398, acc 0.609375\n",
      "2018-12-07T16:24:35.407831: step 1413, loss 1.01636, acc 0.59375\n",
      "2018-12-07T16:24:36.685839: step 1414, loss 1.28334, acc 0.453125\n",
      "2018-12-07T16:24:38.169917: step 1415, loss 0.986737, acc 0.671875\n",
      "2018-12-07T16:24:39.648137: step 1416, loss 1.24501, acc 0.4375\n",
      "2018-12-07T16:24:40.891179: step 1417, loss 1.15903, acc 0.546875\n",
      "2018-12-07T16:24:42.099960: step 1418, loss 1.05557, acc 0.5\n",
      "2018-12-07T16:24:43.269754: step 1419, loss 1.23517, acc 0.46875\n",
      "2018-12-07T16:24:44.527450: step 1420, loss 1.21922, acc 0.5\n",
      "2018-12-07T16:24:46.180749: step 1421, loss 1.2393, acc 0.53125\n",
      "2018-12-07T16:24:47.724292: step 1422, loss 1.11316, acc 0.46875\n",
      "2018-12-07T16:24:49.236440: step 1423, loss 1.11998, acc 0.515625\n",
      "2018-12-07T16:24:50.593793: step 1424, loss 0.944156, acc 0.59375\n",
      "2018-12-07T16:24:51.794419: step 1425, loss 0.976915, acc 0.59375\n",
      "2018-12-07T16:24:53.243862: step 1426, loss 1.14159, acc 0.59375\n",
      "2018-12-07T16:24:54.474712: step 1427, loss 1.04401, acc 0.546875\n",
      "2018-12-07T16:24:55.716867: step 1428, loss 1.08382, acc 0.640625\n",
      "2018-12-07T16:24:57.050113: step 1429, loss 1.06293, acc 0.546875\n",
      "2018-12-07T16:24:58.450825: step 1430, loss 1.08882, acc 0.59375\n",
      "2018-12-07T16:24:59.796616: step 1431, loss 0.870664, acc 0.640625\n",
      "2018-12-07T16:25:01.122301: step 1432, loss 1.05496, acc 0.546875\n",
      "2018-12-07T16:25:02.325831: step 1433, loss 1.3147, acc 0.453125\n",
      "2018-12-07T16:25:03.535887: step 1434, loss 0.92678, acc 0.671875\n",
      "2018-12-07T16:25:05.146167: step 1435, loss 1.0545, acc 0.578125\n",
      "2018-12-07T16:25:06.427147: step 1436, loss 0.993417, acc 0.578125\n",
      "2018-12-07T16:25:07.761296: step 1437, loss 1.17688, acc 0.53125\n",
      "2018-12-07T16:25:09.492545: step 1438, loss 0.907994, acc 0.640625\n",
      "2018-12-07T16:25:10.837278: step 1439, loss 1.12682, acc 0.5625\n",
      "2018-12-07T16:25:12.381031: step 1440, loss 1.10882, acc 0.515625\n",
      "2018-12-07T16:25:13.563990: step 1441, loss 1.08008, acc 0.609375\n",
      "2018-12-07T16:25:14.889864: step 1442, loss 1.26425, acc 0.46875\n",
      "2018-12-07T16:25:16.163865: step 1443, loss 1.19772, acc 0.578125\n",
      "2018-12-07T16:25:17.545822: step 1444, loss 0.901969, acc 0.71875\n",
      "2018-12-07T16:25:18.930437: step 1445, loss 1.17723, acc 0.546875\n",
      "2018-12-07T16:25:20.306098: step 1446, loss 1.1544, acc 0.53125\n",
      "2018-12-07T16:25:21.625954: step 1447, loss 1.20388, acc 0.5\n",
      "2018-12-07T16:25:22.839583: step 1448, loss 1.17639, acc 0.546875\n",
      "2018-12-07T16:25:24.015990: step 1449, loss 1.08515, acc 0.578125\n",
      "2018-12-07T16:25:25.408407: step 1450, loss 1.06308, acc 0.546875\n",
      "2018-12-07T16:25:26.653531: step 1451, loss 0.986953, acc 0.625\n",
      "2018-12-07T16:25:28.146236: step 1452, loss 1.10627, acc 0.53125\n",
      "2018-12-07T16:25:29.636016: step 1453, loss 1.20012, acc 0.578125\n",
      "2018-12-07T16:25:30.941439: step 1454, loss 1.0943, acc 0.59375\n",
      "2018-12-07T16:25:32.169559: step 1455, loss 1.3181, acc 0.515625\n",
      "2018-12-07T16:25:33.502121: step 1456, loss 1.0783, acc 0.515625\n",
      "2018-12-07T16:25:35.022585: step 1457, loss 0.943188, acc 0.640625\n",
      "2018-12-07T16:25:36.614306: step 1458, loss 1.14137, acc 0.515625\n",
      "2018-12-07T16:25:37.886332: step 1459, loss 1.0651, acc 0.609375\n",
      "2018-12-07T16:25:39.260152: step 1460, loss 1.08078, acc 0.640625\n",
      "2018-12-07T16:25:40.689137: step 1461, loss 1.0493, acc 0.59375\n",
      "2018-12-07T16:25:42.334909: step 1462, loss 1.09308, acc 0.53125\n",
      "2018-12-07T16:25:43.627821: step 1463, loss 1.21711, acc 0.46875\n",
      "2018-12-07T16:25:45.240820: step 1464, loss 0.943656, acc 0.671875\n",
      "2018-12-07T16:25:46.689379: step 1465, loss 1.32401, acc 0.421875\n",
      "2018-12-07T16:25:48.440739: step 1466, loss 1.1272, acc 0.546875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07T16:25:49.812174: step 1467, loss 1.11714, acc 0.5625\n",
      "2018-12-07T16:25:51.770791: step 1468, loss 1.03977, acc 0.578125\n",
      "2018-12-07T16:25:53.380372: step 1469, loss 1.18996, acc 0.484375\n",
      "2018-12-07T16:25:54.850782: step 1470, loss 0.977526, acc 0.65625\n",
      "2018-12-07T16:25:56.221053: step 1471, loss 0.903677, acc 0.6875\n",
      "2018-12-07T16:25:58.044340: step 1472, loss 0.971385, acc 0.5625\n",
      "2018-12-07T16:25:59.686982: step 1473, loss 1.04368, acc 0.53125\n",
      "2018-12-07T16:26:01.031538: step 1474, loss 1.1015, acc 0.609375\n",
      "2018-12-07T16:26:02.367564: step 1475, loss 1.31186, acc 0.515625\n",
      "2018-12-07T16:26:03.797735: step 1476, loss 0.96546, acc 0.609375\n",
      "2018-12-07T16:26:05.280985: step 1477, loss 1.09209, acc 0.578125\n",
      "2018-12-07T16:26:06.999862: step 1478, loss 0.957996, acc 0.625\n",
      "2018-12-07T16:26:08.709839: step 1479, loss 0.999574, acc 0.59375\n",
      "2018-12-07T16:26:10.205856: step 1480, loss 1.05782, acc 0.546875\n",
      "2018-12-07T16:26:11.739270: step 1481, loss 1.17404, acc 0.5\n",
      "2018-12-07T16:26:13.299462: step 1482, loss 1.06721, acc 0.59375\n",
      "2018-12-07T16:26:14.779080: step 1483, loss 1.06335, acc 0.5625\n",
      "2018-12-07T16:26:16.372003: step 1484, loss 1.0794, acc 0.609375\n",
      "2018-12-07T16:26:17.919778: step 1485, loss 0.859936, acc 0.6875\n",
      "2018-12-07T16:26:19.477334: step 1486, loss 1.14459, acc 0.515625\n",
      "2018-12-07T16:26:20.770337: step 1487, loss 1.02222, acc 0.578125\n",
      "2018-12-07T16:26:22.643105: step 1488, loss 1.18398, acc 0.578125\n",
      "2018-12-07T16:26:24.275003: step 1489, loss 1.09745, acc 0.53125\n",
      "2018-12-07T16:26:25.770229: step 1490, loss 1.07199, acc 0.609375\n",
      "2018-12-07T16:26:27.319766: step 1491, loss 1.0415, acc 0.578125\n",
      "2018-12-07T16:26:29.204622: step 1492, loss 1.03504, acc 0.578125\n",
      "2018-12-07T16:26:31.235607: step 1493, loss 1.06487, acc 0.5625\n",
      "2018-12-07T16:26:33.091603: step 1494, loss 1.03963, acc 0.609375\n",
      "2018-12-07T16:26:34.769835: step 1495, loss 1.11821, acc 0.4375\n",
      "2018-12-07T16:26:36.923801: step 1496, loss 0.952413, acc 0.59375\n",
      "2018-12-07T16:26:38.469048: step 1497, loss 1.09249, acc 0.546875\n",
      "2018-12-07T16:26:40.639820: step 1498, loss 0.971806, acc 0.625\n",
      "2018-12-07T16:26:41.978438: step 1499, loss 1.12768, acc 0.53125\n",
      "2018-12-07T16:26:43.485810: step 1500, loss 1.03186, acc 0.546875\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T16:26:44.715522: step 1500, loss 0.940792, acc 0.692\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-1500\n",
      "\n",
      "2018-12-07T16:26:46.868053: step 1501, loss 0.994493, acc 0.59375\n",
      "2018-12-07T16:26:48.563296: step 1502, loss 1.01249, acc 0.578125\n",
      "2018-12-07T16:26:50.346797: step 1503, loss 0.977106, acc 0.625\n",
      "2018-12-07T16:26:52.305079: step 1504, loss 1.13339, acc 0.515625\n",
      "2018-12-07T16:26:54.239813: step 1505, loss 1.0628, acc 0.546875\n",
      "2018-12-07T16:26:56.093040: step 1506, loss 0.934615, acc 0.640625\n",
      "2018-12-07T16:26:57.592551: step 1507, loss 1.05727, acc 0.578125\n",
      "2018-12-07T16:26:59.363200: step 1508, loss 1.00563, acc 0.609375\n",
      "2018-12-07T16:27:01.424161: step 1509, loss 1.29701, acc 0.453125\n",
      "2018-12-07T16:27:03.319921: step 1510, loss 1.11722, acc 0.609375\n",
      "2018-12-07T16:27:04.805582: step 1511, loss 1.02473, acc 0.609375\n",
      "2018-12-07T16:27:06.631099: step 1512, loss 0.904836, acc 0.59375\n",
      "2018-12-07T16:27:08.244785: step 1513, loss 1.11243, acc 0.53125\n",
      "2018-12-07T16:27:10.269129: step 1514, loss 1.08843, acc 0.515625\n",
      "2018-12-07T16:27:12.233938: step 1515, loss 1.05185, acc 0.578125\n",
      "2018-12-07T16:27:13.716907: step 1516, loss 1.08507, acc 0.578125\n",
      "2018-12-07T16:27:15.157253: step 1517, loss 1.26004, acc 0.515625\n",
      "2018-12-07T16:27:16.799114: step 1518, loss 1.25735, acc 0.453125\n",
      "2018-12-07T16:27:19.165075: step 1519, loss 1.02598, acc 0.578125\n",
      "2018-12-07T16:27:20.678415: step 1520, loss 1.06675, acc 0.53125\n",
      "2018-12-07T16:27:22.522383: step 1521, loss 1.0439, acc 0.53125\n",
      "2018-12-07T16:27:24.451566: step 1522, loss 1.1147, acc 0.484375\n",
      "2018-12-07T16:27:25.942109: step 1523, loss 1.05805, acc 0.578125\n",
      "2018-12-07T16:27:28.224553: step 1524, loss 1.02169, acc 0.59375\n",
      "2018-12-07T16:27:30.430351: step 1525, loss 1.05852, acc 0.5625\n",
      "2018-12-07T16:27:32.330205: step 1526, loss 1.00643, acc 0.65625\n",
      "2018-12-07T16:27:33.834408: step 1527, loss 1.11407, acc 0.53125\n",
      "2018-12-07T16:27:35.788714: step 1528, loss 1.0735, acc 0.59375\n",
      "2018-12-07T16:27:38.008876: step 1529, loss 1.07994, acc 0.5625\n",
      "2018-12-07T16:27:39.928412: step 1530, loss 1.01737, acc 0.59375\n",
      "2018-12-07T16:27:42.027289: step 1531, loss 0.995292, acc 0.59375\n",
      "2018-12-07T16:27:43.869375: step 1532, loss 0.913434, acc 0.6875\n",
      "2018-12-07T16:27:45.974961: step 1533, loss 1.01397, acc 0.625\n",
      "2018-12-07T16:27:47.974192: step 1534, loss 1.02163, acc 0.625\n",
      "2018-12-07T16:27:49.641467: step 1535, loss 1.09686, acc 0.5625\n",
      "2018-12-07T16:27:51.205840: step 1536, loss 1.11248, acc 0.546875\n",
      "2018-12-07T16:27:52.875774: step 1537, loss 0.949605, acc 0.625\n",
      "2018-12-07T16:27:54.576301: step 1538, loss 1.01381, acc 0.578125\n",
      "2018-12-07T16:27:56.172175: step 1539, loss 1.0974, acc 0.5\n",
      "2018-12-07T16:27:57.837307: step 1540, loss 0.918649, acc 0.640625\n",
      "2018-12-07T16:27:59.654652: step 1541, loss 1.17579, acc 0.515625\n",
      "2018-12-07T16:28:01.736958: step 1542, loss 1.02801, acc 0.640625\n",
      "2018-12-07T16:28:03.376837: step 1543, loss 1.07678, acc 0.546875\n",
      "2018-12-07T16:28:06.125488: step 1544, loss 0.964243, acc 0.671875\n",
      "2018-12-07T16:28:08.372480: step 1545, loss 1.15091, acc 0.5\n",
      "2018-12-07T16:28:11.548223: step 1546, loss 1.10142, acc 0.5\n",
      "2018-12-07T16:28:13.768423: step 1547, loss 1.18114, acc 0.484375\n",
      "2018-12-07T16:28:15.183448: step 1548, loss 1.13611, acc 0.543478\n",
      "2018-12-07T16:28:16.305748: step 1549, loss 0.87196, acc 0.640625\n",
      "2018-12-07T16:28:17.588739: step 1550, loss 1.02806, acc 0.5625\n",
      "2018-12-07T16:28:18.636326: step 1551, loss 1.05281, acc 0.578125\n",
      "2018-12-07T16:28:19.898245: step 1552, loss 0.979109, acc 0.546875\n",
      "2018-12-07T16:28:20.894510: step 1553, loss 1.00702, acc 0.59375\n",
      "2018-12-07T16:28:21.936376: step 1554, loss 0.951833, acc 0.671875\n",
      "2018-12-07T16:28:22.996409: step 1555, loss 1.06091, acc 0.578125\n",
      "2018-12-07T16:28:24.029551: step 1556, loss 0.683694, acc 0.796875\n",
      "2018-12-07T16:28:25.046349: step 1557, loss 0.994736, acc 0.671875\n",
      "2018-12-07T16:28:26.038275: step 1558, loss 1.04979, acc 0.6875\n",
      "2018-12-07T16:28:27.049726: step 1559, loss 0.864356, acc 0.765625\n",
      "2018-12-07T16:28:28.365702: step 1560, loss 0.963762, acc 0.6875\n",
      "2018-12-07T16:28:29.445855: step 1561, loss 1.0051, acc 0.671875\n",
      "2018-12-07T16:28:30.457620: step 1562, loss 0.998748, acc 0.609375\n",
      "2018-12-07T16:28:31.523467: step 1563, loss 1.02338, acc 0.609375\n",
      "2018-12-07T16:28:32.498655: step 1564, loss 1.06522, acc 0.640625\n",
      "2018-12-07T16:28:33.510242: step 1565, loss 1.11663, acc 0.59375\n",
      "2018-12-07T16:28:34.490557: step 1566, loss 1.08184, acc 0.609375\n",
      "2018-12-07T16:28:35.898081: step 1567, loss 1.02659, acc 0.640625\n",
      "2018-12-07T16:28:37.051134: step 1568, loss 0.737826, acc 0.75\n",
      "2018-12-07T16:28:38.254264: step 1569, loss 1.11341, acc 0.609375\n",
      "2018-12-07T16:28:39.279459: step 1570, loss 0.951665, acc 0.609375\n",
      "2018-12-07T16:28:40.364722: step 1571, loss 0.95, acc 0.65625\n",
      "2018-12-07T16:28:41.368027: step 1572, loss 1.20797, acc 0.5\n",
      "2018-12-07T16:28:42.478824: step 1573, loss 1.01601, acc 0.578125\n",
      "2018-12-07T16:28:43.932241: step 1574, loss 1.102, acc 0.59375\n",
      "2018-12-07T16:28:45.105811: step 1575, loss 1.10606, acc 0.515625\n",
      "2018-12-07T16:28:46.113864: step 1576, loss 0.988606, acc 0.640625\n",
      "2018-12-07T16:28:47.297620: step 1577, loss 0.996606, acc 0.53125\n",
      "2018-12-07T16:28:48.387421: step 1578, loss 1.05774, acc 0.5625\n",
      "2018-12-07T16:28:49.409366: step 1579, loss 1.0499, acc 0.59375\n",
      "2018-12-07T16:28:50.392350: step 1580, loss 1.05379, acc 0.59375\n",
      "2018-12-07T16:28:51.410416: step 1581, loss 0.866703, acc 0.6875\n",
      "2018-12-07T16:28:52.441585: step 1582, loss 0.925632, acc 0.671875\n",
      "2018-12-07T16:28:53.622195: step 1583, loss 0.86873, acc 0.6875\n",
      "2018-12-07T16:28:54.686868: step 1584, loss 0.994908, acc 0.609375\n",
      "2018-12-07T16:28:56.126114: step 1585, loss 1.07209, acc 0.546875\n",
      "2018-12-07T16:28:57.484617: step 1586, loss 1.39144, acc 0.484375\n",
      "2018-12-07T16:28:58.532550: step 1587, loss 0.907666, acc 0.609375\n",
      "2018-12-07T16:28:59.554046: step 1588, loss 0.954829, acc 0.65625\n",
      "2018-12-07T16:29:00.717893: step 1589, loss 0.957794, acc 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07T16:29:01.757848: step 1590, loss 0.90471, acc 0.6875\n",
      "2018-12-07T16:29:03.043466: step 1591, loss 0.965782, acc 0.578125\n",
      "2018-12-07T16:29:04.184654: step 1592, loss 1.01902, acc 0.625\n",
      "2018-12-07T16:29:05.399055: step 1593, loss 0.897263, acc 0.71875\n",
      "2018-12-07T16:29:06.411971: step 1594, loss 0.810061, acc 0.640625\n",
      "2018-12-07T16:29:07.495975: step 1595, loss 0.991395, acc 0.578125\n",
      "2018-12-07T16:29:08.505636: step 1596, loss 0.928647, acc 0.609375\n",
      "2018-12-07T16:29:09.542348: step 1597, loss 0.971983, acc 0.625\n",
      "2018-12-07T16:29:10.561092: step 1598, loss 1.04953, acc 0.59375\n",
      "2018-12-07T16:29:11.625319: step 1599, loss 1.0257, acc 0.625\n",
      "2018-12-07T16:29:12.612420: step 1600, loss 0.840898, acc 0.734375\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T16:29:13.855096: step 1600, loss 0.916204, acc 0.696\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-1600\n",
      "\n",
      "2018-12-07T16:29:15.719885: step 1601, loss 0.966485, acc 0.65625\n",
      "2018-12-07T16:29:16.751532: step 1602, loss 1.15806, acc 0.515625\n",
      "2018-12-07T16:29:17.868608: step 1603, loss 0.91608, acc 0.578125\n",
      "2018-12-07T16:29:18.898110: step 1604, loss 0.965254, acc 0.640625\n",
      "2018-12-07T16:29:19.992869: step 1605, loss 1.0177, acc 0.578125\n",
      "2018-12-07T16:29:21.060486: step 1606, loss 1.03658, acc 0.5\n",
      "2018-12-07T16:29:22.141371: step 1607, loss 1.11531, acc 0.53125\n",
      "2018-12-07T16:29:23.189825: step 1608, loss 0.931396, acc 0.65625\n",
      "2018-12-07T16:29:24.476546: step 1609, loss 0.772041, acc 0.734375\n",
      "2018-12-07T16:29:25.887790: step 1610, loss 0.86949, acc 0.640625\n",
      "2018-12-07T16:29:27.046503: step 1611, loss 1.07975, acc 0.625\n",
      "2018-12-07T16:29:28.339487: step 1612, loss 1.18262, acc 0.53125\n",
      "2018-12-07T16:29:29.396960: step 1613, loss 1.18985, acc 0.53125\n",
      "2018-12-07T16:29:30.580631: step 1614, loss 0.913644, acc 0.625\n",
      "2018-12-07T16:29:31.760567: step 1615, loss 1.17428, acc 0.5\n",
      "2018-12-07T16:29:32.804525: step 1616, loss 0.933766, acc 0.640625\n",
      "2018-12-07T16:29:33.919841: step 1617, loss 0.946542, acc 0.71875\n",
      "2018-12-07T16:29:35.345264: step 1618, loss 0.965786, acc 0.65625\n",
      "2018-12-07T16:29:36.395512: step 1619, loss 1.07824, acc 0.609375\n",
      "2018-12-07T16:29:37.506641: step 1620, loss 0.868291, acc 0.625\n",
      "2018-12-07T16:29:38.641354: step 1621, loss 0.945442, acc 0.671875\n",
      "2018-12-07T16:29:40.245604: step 1622, loss 0.914097, acc 0.6875\n",
      "2018-12-07T16:29:41.302400: step 1623, loss 1.13035, acc 0.59375\n",
      "2018-12-07T16:29:42.322434: step 1624, loss 1.12517, acc 0.546875\n",
      "2018-12-07T16:29:43.651956: step 1625, loss 1.27981, acc 0.5625\n",
      "2018-12-07T16:29:44.686242: step 1626, loss 1.19047, acc 0.453125\n",
      "2018-12-07T16:29:45.776119: step 1627, loss 1.00122, acc 0.640625\n",
      "2018-12-07T16:29:46.958154: step 1628, loss 0.951895, acc 0.65625\n",
      "2018-12-07T16:29:48.477935: step 1629, loss 1.07301, acc 0.5625\n",
      "2018-12-07T16:29:49.583851: step 1630, loss 0.922874, acc 0.671875\n",
      "2018-12-07T16:29:50.624645: step 1631, loss 0.989585, acc 0.609375\n",
      "2018-12-07T16:29:51.703028: step 1632, loss 0.947811, acc 0.671875\n",
      "2018-12-07T16:29:52.778601: step 1633, loss 1.09423, acc 0.546875\n",
      "2018-12-07T16:29:53.834880: step 1634, loss 1.0128, acc 0.609375\n",
      "2018-12-07T16:29:55.154599: step 1635, loss 1.06016, acc 0.609375\n",
      "2018-12-07T16:29:56.201449: step 1636, loss 1.13047, acc 0.578125\n",
      "2018-12-07T16:29:57.252316: step 1637, loss 0.977834, acc 0.578125\n",
      "2018-12-07T16:29:58.512841: step 1638, loss 0.970316, acc 0.625\n",
      "2018-12-07T16:29:59.660316: step 1639, loss 0.986706, acc 0.546875\n",
      "2018-12-07T16:30:00.729075: step 1640, loss 0.857119, acc 0.65625\n",
      "2018-12-07T16:30:01.972038: step 1641, loss 1.09467, acc 0.546875\n",
      "2018-12-07T16:30:03.101543: step 1642, loss 0.81579, acc 0.703125\n",
      "2018-12-07T16:30:04.180977: step 1643, loss 0.996817, acc 0.59375\n",
      "2018-12-07T16:30:05.495253: step 1644, loss 1.00014, acc 0.640625\n",
      "2018-12-07T16:30:06.544211: step 1645, loss 1.01504, acc 0.609375\n",
      "2018-12-07T16:30:07.791756: step 1646, loss 1.07522, acc 0.65625\n",
      "2018-12-07T16:30:08.989792: step 1647, loss 1.02114, acc 0.578125\n",
      "2018-12-07T16:30:10.044172: step 1648, loss 1.12489, acc 0.484375\n",
      "2018-12-07T16:30:11.207214: step 1649, loss 0.938538, acc 0.59375\n",
      "2018-12-07T16:30:12.369995: step 1650, loss 1.06274, acc 0.53125\n",
      "2018-12-07T16:30:13.611851: step 1651, loss 1.05326, acc 0.640625\n",
      "2018-12-07T16:30:14.600512: step 1652, loss 1.12902, acc 0.40625\n",
      "2018-12-07T16:30:15.837016: step 1653, loss 1.05947, acc 0.546875\n",
      "2018-12-07T16:30:17.091945: step 1654, loss 1.04112, acc 0.53125\n",
      "2018-12-07T16:30:18.486936: step 1655, loss 1.20365, acc 0.5\n",
      "2018-12-07T16:30:19.576526: step 1656, loss 1.14815, acc 0.53125\n",
      "2018-12-07T16:30:20.587736: step 1657, loss 1.1021, acc 0.546875\n",
      "2018-12-07T16:30:21.811276: step 1658, loss 1.00361, acc 0.609375\n",
      "2018-12-07T16:30:22.886579: step 1659, loss 1.32636, acc 0.484375\n",
      "2018-12-07T16:30:23.996926: step 1660, loss 0.973192, acc 0.609375\n",
      "2018-12-07T16:30:25.212055: step 1661, loss 0.961536, acc 0.609375\n",
      "2018-12-07T16:30:26.210015: step 1662, loss 1.03587, acc 0.625\n",
      "2018-12-07T16:30:27.277229: step 1663, loss 1.14403, acc 0.609375\n",
      "2018-12-07T16:30:28.348670: step 1664, loss 0.865519, acc 0.71875\n",
      "2018-12-07T16:30:29.735425: step 1665, loss 1.0104, acc 0.671875\n",
      "2018-12-07T16:30:30.934943: step 1666, loss 0.875518, acc 0.65625\n",
      "2018-12-07T16:30:32.090228: step 1667, loss 1.11698, acc 0.5625\n",
      "2018-12-07T16:30:33.159012: step 1668, loss 1.01795, acc 0.578125\n",
      "2018-12-07T16:30:34.233126: step 1669, loss 1.08795, acc 0.578125\n",
      "2018-12-07T16:30:35.240918: step 1670, loss 1.09681, acc 0.5\n",
      "2018-12-07T16:30:36.307207: step 1671, loss 1.01116, acc 0.59375\n",
      "2018-12-07T16:30:37.444495: step 1672, loss 0.965626, acc 0.5625\n",
      "2018-12-07T16:30:38.514479: step 1673, loss 1.1115, acc 0.515625\n",
      "2018-12-07T16:30:40.031520: step 1674, loss 1.15451, acc 0.515625\n",
      "2018-12-07T16:30:41.302219: step 1675, loss 0.949646, acc 0.640625\n",
      "2018-12-07T16:30:42.475747: step 1676, loss 1.10735, acc 0.546875\n",
      "2018-12-07T16:30:43.631825: step 1677, loss 0.939849, acc 0.65625\n",
      "2018-12-07T16:30:44.760117: step 1678, loss 1.0143, acc 0.59375\n",
      "2018-12-07T16:30:46.049750: step 1679, loss 1.02192, acc 0.671875\n",
      "2018-12-07T16:30:47.178468: step 1680, loss 1.04404, acc 0.5625\n",
      "2018-12-07T16:30:48.650498: step 1681, loss 1.0494, acc 0.546875\n",
      "2018-12-07T16:30:49.702221: step 1682, loss 1.04598, acc 0.625\n",
      "2018-12-07T16:30:50.907366: step 1683, loss 1.01936, acc 0.546875\n",
      "2018-12-07T16:30:51.907024: step 1684, loss 0.946887, acc 0.609375\n",
      "2018-12-07T16:30:53.164217: step 1685, loss 0.891036, acc 0.65625\n",
      "2018-12-07T16:30:54.464607: step 1686, loss 1.13681, acc 0.609375\n",
      "2018-12-07T16:30:55.693560: step 1687, loss 1.10364, acc 0.515625\n",
      "2018-12-07T16:30:56.833178: step 1688, loss 0.9607, acc 0.609375\n",
      "2018-12-07T16:30:58.003210: step 1689, loss 1.11372, acc 0.609375\n",
      "2018-12-07T16:30:59.329147: step 1690, loss 1.00681, acc 0.609375\n",
      "2018-12-07T16:31:00.503937: step 1691, loss 0.93687, acc 0.65625\n",
      "2018-12-07T16:31:01.716525: step 1692, loss 1.00887, acc 0.640625\n",
      "2018-12-07T16:31:02.767103: step 1693, loss 0.974762, acc 0.609375\n",
      "2018-12-07T16:31:03.840262: step 1694, loss 1.1376, acc 0.578125\n",
      "2018-12-07T16:31:05.368762: step 1695, loss 1.03814, acc 0.515625\n",
      "2018-12-07T16:31:06.435790: step 1696, loss 0.95602, acc 0.65625\n",
      "2018-12-07T16:31:07.551901: step 1697, loss 0.909645, acc 0.578125\n",
      "2018-12-07T16:31:08.697368: step 1698, loss 1.04321, acc 0.546875\n",
      "2018-12-07T16:31:10.066196: step 1699, loss 1.02051, acc 0.609375\n",
      "2018-12-07T16:31:11.162035: step 1700, loss 0.936949, acc 0.671875\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T16:31:12.378782: step 1700, loss 0.911479, acc 0.68\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-1700\n",
      "\n",
      "2018-12-07T16:31:14.163553: step 1701, loss 0.928649, acc 0.671875\n",
      "2018-12-07T16:31:15.483805: step 1702, loss 0.982263, acc 0.578125\n",
      "2018-12-07T16:31:16.732040: step 1703, loss 0.959504, acc 0.609375\n",
      "2018-12-07T16:31:18.039867: step 1704, loss 1.10225, acc 0.515625\n",
      "2018-12-07T16:31:19.117259: step 1705, loss 1.13074, acc 0.53125\n",
      "2018-12-07T16:31:20.340536: step 1706, loss 0.936604, acc 0.609375\n",
      "2018-12-07T16:31:21.577729: step 1707, loss 0.896362, acc 0.609375\n",
      "2018-12-07T16:31:22.809966: step 1708, loss 1.15034, acc 0.453125\n",
      "2018-12-07T16:31:23.926611: step 1709, loss 0.984476, acc 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07T16:31:25.017204: step 1710, loss 1.01567, acc 0.640625\n",
      "2018-12-07T16:31:26.096404: step 1711, loss 1.27535, acc 0.53125\n",
      "2018-12-07T16:31:27.280958: step 1712, loss 1.17944, acc 0.578125\n",
      "2018-12-07T16:31:28.740680: step 1713, loss 1.08952, acc 0.625\n",
      "2018-12-07T16:31:29.997053: step 1714, loss 0.971231, acc 0.65625\n",
      "2018-12-07T16:31:31.369218: step 1715, loss 0.98534, acc 0.609375\n",
      "2018-12-07T16:31:32.718182: step 1716, loss 0.98214, acc 0.59375\n",
      "2018-12-07T16:31:33.815300: step 1717, loss 1.09316, acc 0.46875\n",
      "2018-12-07T16:31:35.321582: step 1718, loss 0.966645, acc 0.609375\n",
      "2018-12-07T16:31:36.945468: step 1719, loss 0.812956, acc 0.671875\n",
      "2018-12-07T16:31:38.415238: step 1720, loss 1.10859, acc 0.53125\n",
      "2018-12-07T16:31:39.745597: step 1721, loss 0.959875, acc 0.609375\n",
      "2018-12-07T16:31:41.583477: step 1722, loss 0.955334, acc 0.671875\n",
      "2018-12-07T16:31:42.936478: step 1723, loss 1.07504, acc 0.5625\n",
      "2018-12-07T16:31:44.045712: step 1724, loss 1.07897, acc 0.53125\n",
      "2018-12-07T16:31:45.169862: step 1725, loss 1.07787, acc 0.59375\n",
      "2018-12-07T16:31:46.255917: step 1726, loss 1.04574, acc 0.53125\n",
      "2018-12-07T16:31:47.422512: step 1727, loss 0.996938, acc 0.546875\n",
      "2018-12-07T16:31:48.949159: step 1728, loss 1.0784, acc 0.609375\n",
      "2018-12-07T16:31:50.318833: step 1729, loss 0.994269, acc 0.5625\n",
      "2018-12-07T16:31:51.407217: step 1730, loss 1.11916, acc 0.53125\n",
      "2018-12-07T16:31:52.807002: step 1731, loss 0.898843, acc 0.640625\n",
      "2018-12-07T16:31:53.954013: step 1732, loss 0.873671, acc 0.6875\n",
      "2018-12-07T16:31:55.215172: step 1733, loss 0.949437, acc 0.6875\n",
      "2018-12-07T16:31:56.466596: step 1734, loss 1.24986, acc 0.46875\n",
      "2018-12-07T16:31:57.626808: step 1735, loss 0.779163, acc 0.734375\n",
      "2018-12-07T16:31:58.845607: step 1736, loss 1.05647, acc 0.515625\n",
      "2018-12-07T16:32:00.538827: step 1737, loss 0.986071, acc 0.5625\n",
      "2018-12-07T16:32:01.789597: step 1738, loss 1.05906, acc 0.59375\n",
      "2018-12-07T16:32:03.095275: step 1739, loss 0.9411, acc 0.609375\n",
      "2018-12-07T16:32:04.299840: step 1740, loss 1.04393, acc 0.59375\n",
      "2018-12-07T16:32:05.490794: step 1741, loss 1.13812, acc 0.59375\n",
      "2018-12-07T16:32:07.027798: step 1742, loss 0.946174, acc 0.625\n",
      "2018-12-07T16:32:08.359989: step 1743, loss 0.998366, acc 0.578125\n",
      "2018-12-07T16:32:09.997992: step 1744, loss 0.996844, acc 0.546875\n",
      "2018-12-07T16:32:11.530604: step 1745, loss 0.952056, acc 0.65625\n",
      "2018-12-07T16:32:12.654886: step 1746, loss 0.817833, acc 0.671875\n",
      "2018-12-07T16:32:13.793517: step 1747, loss 1.05716, acc 0.578125\n",
      "2018-12-07T16:32:14.913374: step 1748, loss 1.05836, acc 0.515625\n",
      "2018-12-07T16:32:15.978877: step 1749, loss 1.10875, acc 0.59375\n",
      "2018-12-07T16:32:17.127253: step 1750, loss 0.971744, acc 0.640625\n",
      "2018-12-07T16:32:18.529137: step 1751, loss 0.949601, acc 0.59375\n",
      "2018-12-07T16:32:19.752690: step 1752, loss 0.849746, acc 0.609375\n",
      "2018-12-07T16:32:21.322519: step 1753, loss 1.08925, acc 0.484375\n",
      "2018-12-07T16:32:22.959058: step 1754, loss 0.919917, acc 0.6875\n",
      "2018-12-07T16:32:24.244301: step 1755, loss 1.09344, acc 0.53125\n",
      "2018-12-07T16:32:25.613240: step 1756, loss 0.93382, acc 0.65625\n",
      "2018-12-07T16:32:26.994176: step 1757, loss 1.12392, acc 0.53125\n",
      "2018-12-07T16:32:28.279007: step 1758, loss 0.902104, acc 0.65625\n",
      "2018-12-07T16:32:29.588087: step 1759, loss 0.827429, acc 0.65625\n",
      "2018-12-07T16:32:31.334425: step 1760, loss 0.93362, acc 0.640625\n",
      "2018-12-07T16:32:32.587646: step 1761, loss 1.09467, acc 0.453125\n",
      "2018-12-07T16:32:33.922874: step 1762, loss 0.87854, acc 0.640625\n",
      "2018-12-07T16:32:35.163182: step 1763, loss 1.13806, acc 0.578125\n",
      "2018-12-07T16:32:36.583406: step 1764, loss 0.992246, acc 0.625\n",
      "2018-12-07T16:32:37.725685: step 1765, loss 1.14154, acc 0.453125\n",
      "2018-12-07T16:32:39.083636: step 1766, loss 1.04882, acc 0.65625\n",
      "2018-12-07T16:32:40.446959: step 1767, loss 1.1256, acc 0.53125\n",
      "2018-12-07T16:32:41.674643: step 1768, loss 0.925489, acc 0.6875\n",
      "2018-12-07T16:32:42.916824: step 1769, loss 1.11538, acc 0.578125\n",
      "2018-12-07T16:32:44.167956: step 1770, loss 1.04613, acc 0.65625\n",
      "2018-12-07T16:32:45.447559: step 1771, loss 1.1013, acc 0.578125\n",
      "2018-12-07T16:32:46.555251: step 1772, loss 1.05961, acc 0.53125\n",
      "2018-12-07T16:32:48.255537: step 1773, loss 1.10272, acc 0.59375\n",
      "2018-12-07T16:32:49.636947: step 1774, loss 1.00875, acc 0.5625\n",
      "2018-12-07T16:32:50.758289: step 1775, loss 1.2197, acc 0.4375\n",
      "2018-12-07T16:32:52.106561: step 1776, loss 1.10549, acc 0.59375\n",
      "2018-12-07T16:32:53.213640: step 1777, loss 1.1087, acc 0.59375\n",
      "2018-12-07T16:32:54.419290: step 1778, loss 1.2758, acc 0.53125\n",
      "2018-12-07T16:32:55.666868: step 1779, loss 0.889997, acc 0.6875\n",
      "2018-12-07T16:32:56.744003: step 1780, loss 1.00363, acc 0.5625\n",
      "2018-12-07T16:32:58.032913: step 1781, loss 0.997783, acc 0.609375\n",
      "2018-12-07T16:32:59.406748: step 1782, loss 0.972125, acc 0.578125\n",
      "2018-12-07T16:33:00.507691: step 1783, loss 1.0967, acc 0.5625\n",
      "2018-12-07T16:33:01.656016: step 1784, loss 0.886545, acc 0.578125\n",
      "2018-12-07T16:33:02.743344: step 1785, loss 1.36418, acc 0.515625\n",
      "2018-12-07T16:33:04.063273: step 1786, loss 0.848608, acc 0.671875\n",
      "2018-12-07T16:33:05.504877: step 1787, loss 0.921312, acc 0.640625\n",
      "2018-12-07T16:33:06.733951: step 1788, loss 1.00407, acc 0.59375\n",
      "2018-12-07T16:33:08.266239: step 1789, loss 1.10443, acc 0.5\n",
      "2018-12-07T16:33:09.639342: step 1790, loss 1.05668, acc 0.515625\n",
      "2018-12-07T16:33:10.748582: step 1791, loss 1.13545, acc 0.5\n",
      "2018-12-07T16:33:11.855748: step 1792, loss 1.06439, acc 0.59375\n",
      "2018-12-07T16:33:13.272678: step 1793, loss 1.02783, acc 0.53125\n",
      "2018-12-07T16:33:14.397792: step 1794, loss 1.12494, acc 0.515625\n",
      "2018-12-07T16:33:15.635967: step 1795, loss 0.948107, acc 0.625\n",
      "2018-12-07T16:33:16.750967: step 1796, loss 1.05311, acc 0.53125\n",
      "2018-12-07T16:33:17.983139: step 1797, loss 0.979894, acc 0.5625\n",
      "2018-12-07T16:33:19.116360: step 1798, loss 0.880273, acc 0.703125\n",
      "2018-12-07T16:33:20.208619: step 1799, loss 1.15745, acc 0.5625\n",
      "2018-12-07T16:33:21.521281: step 1800, loss 0.908509, acc 0.59375\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T16:33:22.769944: step 1800, loss 0.940209, acc 0.664\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-1800\n",
      "\n",
      "2018-12-07T16:33:24.962629: step 1801, loss 1.15571, acc 0.578125\n",
      "2018-12-07T16:33:26.723274: step 1802, loss 1.06406, acc 0.578125\n",
      "2018-12-07T16:33:28.172267: step 1803, loss 1.00983, acc 0.609375\n",
      "2018-12-07T16:33:29.470257: step 1804, loss 1.03212, acc 0.578125\n",
      "2018-12-07T16:33:30.675457: step 1805, loss 1.06015, acc 0.671875\n",
      "2018-12-07T16:33:31.854320: step 1806, loss 0.840127, acc 0.671875\n",
      "2018-12-07T16:33:33.310868: step 1807, loss 0.970311, acc 0.640625\n",
      "2018-12-07T16:33:34.503428: step 1808, loss 0.955404, acc 0.578125\n",
      "2018-12-07T16:33:36.129298: step 1809, loss 0.8159, acc 0.71875\n",
      "2018-12-07T16:33:37.599740: step 1810, loss 0.927945, acc 0.640625\n",
      "2018-12-07T16:33:38.877281: step 1811, loss 0.892563, acc 0.59375\n",
      "2018-12-07T16:33:40.136190: step 1812, loss 0.836976, acc 0.640625\n",
      "2018-12-07T16:33:41.717947: step 1813, loss 1.05258, acc 0.65625\n",
      "2018-12-07T16:33:42.935240: step 1814, loss 1.08117, acc 0.515625\n",
      "2018-12-07T16:33:44.175661: step 1815, loss 0.881489, acc 0.65625\n",
      "2018-12-07T16:33:45.506048: step 1816, loss 1.04288, acc 0.59375\n",
      "2018-12-07T16:33:47.260910: step 1817, loss 0.844388, acc 0.734375\n",
      "2018-12-07T16:33:48.457707: step 1818, loss 0.820091, acc 0.6875\n",
      "2018-12-07T16:33:49.926154: step 1819, loss 0.922342, acc 0.671875\n",
      "2018-12-07T16:33:51.474657: step 1820, loss 0.91399, acc 0.640625\n",
      "2018-12-07T16:33:53.397999: step 1821, loss 0.955094, acc 0.65625\n",
      "2018-12-07T16:33:54.836396: step 1822, loss 1.1463, acc 0.578125\n",
      "2018-12-07T16:33:56.476713: step 1823, loss 0.883877, acc 0.640625\n",
      "2018-12-07T16:33:57.701922: step 1824, loss 0.848786, acc 0.671875\n",
      "2018-12-07T16:33:59.465842: step 1825, loss 0.898707, acc 0.609375\n",
      "2018-12-07T16:34:01.030109: step 1826, loss 0.96707, acc 0.640625\n",
      "2018-12-07T16:34:02.465481: step 1827, loss 0.802615, acc 0.6875\n",
      "2018-12-07T16:34:04.229827: step 1828, loss 1.01041, acc 0.59375\n",
      "2018-12-07T16:34:05.700091: step 1829, loss 1.07516, acc 0.5625\n",
      "2018-12-07T16:34:07.052539: step 1830, loss 0.972823, acc 0.53125\n",
      "2018-12-07T16:34:08.464646: step 1831, loss 1.14522, acc 0.578125\n",
      "2018-12-07T16:34:10.184384: step 1832, loss 0.91225, acc 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07T16:34:12.181376: step 1833, loss 0.91958, acc 0.6875\n",
      "2018-12-07T16:34:13.722747: step 1834, loss 0.955403, acc 0.59375\n",
      "2018-12-07T16:34:15.253923: step 1835, loss 1.03336, acc 0.59375\n",
      "2018-12-07T16:34:16.407616: step 1836, loss 1.13805, acc 0.4375\n",
      "2018-12-07T16:34:17.760222: step 1837, loss 1.01483, acc 0.640625\n",
      "2018-12-07T16:34:19.303569: step 1838, loss 1.05952, acc 0.5625\n",
      "2018-12-07T16:34:20.649493: step 1839, loss 1.03187, acc 0.609375\n",
      "2018-12-07T16:34:21.994165: step 1840, loss 1.08131, acc 0.59375\n",
      "2018-12-07T16:34:23.328080: step 1841, loss 1.04754, acc 0.546875\n",
      "2018-12-07T16:34:24.860259: step 1842, loss 1.08534, acc 0.5625\n",
      "2018-12-07T16:34:26.197250: step 1843, loss 0.916713, acc 0.640625\n",
      "2018-12-07T16:34:27.737524: step 1844, loss 0.967263, acc 0.59375\n",
      "2018-12-07T16:34:29.362074: step 1845, loss 0.863386, acc 0.640625\n",
      "2018-12-07T16:34:31.001912: step 1846, loss 0.913142, acc 0.65625\n",
      "2018-12-07T16:34:32.480696: step 1847, loss 0.942654, acc 0.625\n",
      "2018-12-07T16:34:33.733895: step 1848, loss 0.986439, acc 0.671875\n",
      "2018-12-07T16:34:34.959077: step 1849, loss 0.895074, acc 0.609375\n",
      "2018-12-07T16:34:36.291137: step 1850, loss 0.927107, acc 0.609375\n",
      "2018-12-07T16:34:37.683600: step 1851, loss 1.03177, acc 0.59375\n",
      "2018-12-07T16:34:38.989458: step 1852, loss 1.04452, acc 0.53125\n",
      "2018-12-07T16:34:40.346718: step 1853, loss 1.07971, acc 0.625\n",
      "2018-12-07T16:34:41.860237: step 1854, loss 0.877646, acc 0.703125\n",
      "2018-12-07T16:34:43.237146: step 1855, loss 1.24011, acc 0.484375\n",
      "2018-12-07T16:34:44.783116: step 1856, loss 1.03006, acc 0.640625\n",
      "2018-12-07T16:34:46.259304: step 1857, loss 0.863963, acc 0.640625\n",
      "2018-12-07T16:34:48.214660: step 1858, loss 0.856997, acc 0.65625\n",
      "2018-12-07T16:34:49.875207: step 1859, loss 0.969886, acc 0.578125\n",
      "2018-12-07T16:34:51.145867: step 1860, loss 0.879567, acc 0.65625\n",
      "2018-12-07T16:34:52.922747: step 1861, loss 0.905098, acc 0.609375\n",
      "2018-12-07T16:34:54.430596: step 1862, loss 1.21113, acc 0.53125\n",
      "2018-12-07T16:34:56.201777: step 1863, loss 1.06569, acc 0.5625\n",
      "2018-12-07T16:34:57.696656: step 1864, loss 1.02024, acc 0.59375\n",
      "2018-12-07T16:34:59.937548: step 1865, loss 0.959815, acc 0.625\n",
      "2018-12-07T16:35:01.604740: step 1866, loss 1.02101, acc 0.625\n",
      "2018-12-07T16:35:03.179093: step 1867, loss 1.12177, acc 0.515625\n",
      "2018-12-07T16:35:04.717953: step 1868, loss 1.16772, acc 0.5\n",
      "2018-12-07T16:35:06.026174: step 1869, loss 1.04062, acc 0.640625\n",
      "2018-12-07T16:35:07.680886: step 1870, loss 1.04935, acc 0.609375\n",
      "2018-12-07T16:35:09.310349: step 1871, loss 0.957911, acc 0.578125\n",
      "2018-12-07T16:35:10.932336: step 1872, loss 0.850115, acc 0.609375\n",
      "2018-12-07T16:35:12.509183: step 1873, loss 0.904817, acc 0.625\n",
      "2018-12-07T16:35:13.989249: step 1874, loss 1.12728, acc 0.546875\n",
      "2018-12-07T16:35:15.450289: step 1875, loss 0.922388, acc 0.625\n",
      "2018-12-07T16:35:17.176869: step 1876, loss 0.885172, acc 0.625\n",
      "2018-12-07T16:35:19.679651: step 1877, loss 0.863867, acc 0.703125\n",
      "2018-12-07T16:35:21.467187: step 1878, loss 0.968561, acc 0.578125\n",
      "2018-12-07T16:35:23.190781: step 1879, loss 1.1184, acc 0.484375\n",
      "2018-12-07T16:35:24.575998: step 1880, loss 0.980554, acc 0.640625\n",
      "2018-12-07T16:35:26.336289: step 1881, loss 0.934997, acc 0.65625\n",
      "2018-12-07T16:35:28.240580: step 1882, loss 0.850554, acc 0.65625\n",
      "2018-12-07T16:35:30.133098: step 1883, loss 0.912873, acc 0.640625\n",
      "2018-12-07T16:35:31.727041: step 1884, loss 0.908027, acc 0.671875\n",
      "2018-12-07T16:35:33.310659: step 1885, loss 1.14532, acc 0.578125\n",
      "2018-12-07T16:35:35.133784: step 1886, loss 1.00529, acc 0.59375\n",
      "2018-12-07T16:35:36.945316: step 1887, loss 0.835056, acc 0.6875\n",
      "2018-12-07T16:35:38.990358: step 1888, loss 1.118, acc 0.59375\n",
      "2018-12-07T16:35:40.550102: step 1889, loss 1.13593, acc 0.546875\n",
      "2018-12-07T16:35:42.258868: step 1890, loss 1.07359, acc 0.59375\n",
      "2018-12-07T16:35:43.909180: step 1891, loss 1.26392, acc 0.4375\n",
      "2018-12-07T16:35:45.548143: step 1892, loss 1.02365, acc 0.5625\n",
      "2018-12-07T16:35:47.430463: step 1893, loss 0.891022, acc 0.65625\n",
      "2018-12-07T16:35:49.208490: step 1894, loss 1.10194, acc 0.59375\n",
      "2018-12-07T16:35:50.935474: step 1895, loss 1.00207, acc 0.5625\n",
      "2018-12-07T16:35:53.068288: step 1896, loss 1.00808, acc 0.59375\n",
      "2018-12-07T16:35:54.585535: step 1897, loss 1.0324, acc 0.59375\n",
      "2018-12-07T16:35:56.446838: step 1898, loss 1.08519, acc 0.4375\n",
      "2018-12-07T16:35:58.143710: step 1899, loss 1.27715, acc 0.5\n",
      "2018-12-07T16:36:00.507347: step 1900, loss 1.07714, acc 0.578125\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T16:36:01.756008: step 1900, loss 0.864932, acc 0.756\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-1900\n",
      "\n",
      "2018-12-07T16:36:03.958005: step 1901, loss 1.11484, acc 0.546875\n",
      "2018-12-07T16:36:05.775506: step 1902, loss 0.992231, acc 0.5625\n",
      "2018-12-07T16:36:07.489573: step 1903, loss 0.964135, acc 0.59375\n",
      "2018-12-07T16:36:09.139448: step 1904, loss 1.02135, acc 0.609375\n",
      "2018-12-07T16:36:10.692873: step 1905, loss 1.00923, acc 0.671875\n",
      "2018-12-07T16:36:12.077400: step 1906, loss 1.08434, acc 0.5\n",
      "2018-12-07T16:36:14.093939: step 1907, loss 1.01712, acc 0.671875\n",
      "2018-12-07T16:36:15.858342: step 1908, loss 0.940699, acc 0.640625\n",
      "2018-12-07T16:36:17.529810: step 1909, loss 0.950247, acc 0.640625\n",
      "2018-12-07T16:36:19.369733: step 1910, loss 0.852737, acc 0.625\n",
      "2018-12-07T16:36:21.471311: step 1911, loss 0.978413, acc 0.59375\n",
      "2018-12-07T16:36:23.551359: step 1912, loss 1.04786, acc 0.640625\n",
      "2018-12-07T16:36:25.240353: step 1913, loss 0.975342, acc 0.5625\n",
      "2018-12-07T16:36:27.191980: step 1914, loss 0.774083, acc 0.703125\n",
      "2018-12-07T16:36:28.891439: step 1915, loss 1.14169, acc 0.5\n",
      "2018-12-07T16:36:30.716323: step 1916, loss 0.993139, acc 0.609375\n",
      "2018-12-07T16:36:32.296590: step 1917, loss 0.875447, acc 0.671875\n",
      "2018-12-07T16:36:33.775691: step 1918, loss 0.867945, acc 0.640625\n",
      "2018-12-07T16:36:35.322990: step 1919, loss 1.08244, acc 0.5\n",
      "2018-12-07T16:36:37.378486: step 1920, loss 1.09866, acc 0.578125\n",
      "2018-12-07T16:36:39.271652: step 1921, loss 0.878051, acc 0.640625\n",
      "2018-12-07T16:36:41.036606: step 1922, loss 0.807459, acc 0.671875\n",
      "2018-12-07T16:36:42.556247: step 1923, loss 1.03878, acc 0.609375\n",
      "2018-12-07T16:36:44.404380: step 1924, loss 0.900914, acc 0.671875\n",
      "2018-12-07T16:36:45.789557: step 1925, loss 0.973008, acc 0.671875\n",
      "2018-12-07T16:36:47.465659: step 1926, loss 1.00061, acc 0.578125\n",
      "2018-12-07T16:36:49.010718: step 1927, loss 1.15365, acc 0.515625\n",
      "2018-12-07T16:36:50.798971: step 1928, loss 1.22837, acc 0.59375\n",
      "2018-12-07T16:36:52.256138: step 1929, loss 0.896844, acc 0.640625\n",
      "2018-12-07T16:36:53.721856: step 1930, loss 0.904613, acc 0.609375\n",
      "2018-12-07T16:36:55.267925: step 1931, loss 1.01434, acc 0.625\n",
      "2018-12-07T16:36:57.073228: step 1932, loss 1.01679, acc 0.609375\n",
      "2018-12-07T16:36:58.637728: step 1933, loss 1.0089, acc 0.65625\n",
      "2018-12-07T16:37:00.728260: step 1934, loss 1.1442, acc 0.59375\n",
      "2018-12-07T16:37:02.203223: step 1935, loss 0.777657, acc 0.717391\n",
      "2018-12-07T16:37:03.246759: step 1936, loss 1.07137, acc 0.5625\n",
      "2018-12-07T16:37:04.427687: step 1937, loss 0.913464, acc 0.625\n",
      "2018-12-07T16:37:05.462418: step 1938, loss 1.11622, acc 0.609375\n",
      "2018-12-07T16:37:06.583790: step 1939, loss 1.04873, acc 0.515625\n",
      "2018-12-07T16:37:07.662525: step 1940, loss 0.813186, acc 0.703125\n",
      "2018-12-07T16:37:08.641856: step 1941, loss 0.937682, acc 0.640625\n",
      "2018-12-07T16:37:09.758325: step 1942, loss 0.962798, acc 0.59375\n",
      "2018-12-07T16:37:10.747214: step 1943, loss 0.873449, acc 0.671875\n",
      "2018-12-07T16:37:11.881627: step 1944, loss 0.897376, acc 0.609375\n",
      "2018-12-07T16:37:12.887773: step 1945, loss 1.02986, acc 0.578125\n",
      "2018-12-07T16:37:13.892356: step 1946, loss 0.747499, acc 0.734375\n",
      "2018-12-07T16:37:15.034306: step 1947, loss 0.971317, acc 0.59375\n",
      "2018-12-07T16:37:16.029409: step 1948, loss 1.04066, acc 0.5625\n",
      "2018-12-07T16:37:17.336959: step 1949, loss 0.8994, acc 0.65625\n",
      "2018-12-07T16:37:18.304997: step 1950, loss 0.972682, acc 0.5625\n",
      "2018-12-07T16:37:19.421276: step 1951, loss 0.976859, acc 0.59375\n",
      "2018-12-07T16:37:20.406789: step 1952, loss 0.899323, acc 0.640625\n",
      "2018-12-07T16:37:21.430396: step 1953, loss 0.864662, acc 0.6875\n",
      "2018-12-07T16:37:22.575004: step 1954, loss 1.06445, acc 0.59375\n",
      "2018-12-07T16:37:23.594571: step 1955, loss 0.952595, acc 0.640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07T16:37:24.579756: step 1956, loss 1.10186, acc 0.53125\n",
      "2018-12-07T16:37:25.607539: step 1957, loss 0.718771, acc 0.71875\n",
      "2018-12-07T16:37:26.914356: step 1958, loss 1.01241, acc 0.625\n",
      "2018-12-07T16:37:28.017994: step 1959, loss 0.794795, acc 0.71875\n",
      "2018-12-07T16:37:29.187664: step 1960, loss 0.991967, acc 0.5625\n",
      "2018-12-07T16:37:30.153481: step 1961, loss 0.880894, acc 0.625\n",
      "2018-12-07T16:37:31.365918: step 1962, loss 0.823893, acc 0.65625\n",
      "2018-12-07T16:37:32.607213: step 1963, loss 0.925058, acc 0.625\n",
      "2018-12-07T16:37:33.814872: step 1964, loss 0.881787, acc 0.65625\n",
      "2018-12-07T16:37:34.825135: step 1965, loss 1.17332, acc 0.546875\n",
      "2018-12-07T16:37:35.853958: step 1966, loss 0.986668, acc 0.609375\n",
      "2018-12-07T16:37:36.929047: step 1967, loss 0.967045, acc 0.609375\n",
      "2018-12-07T16:37:38.049592: step 1968, loss 0.906105, acc 0.640625\n",
      "2018-12-07T16:37:39.088343: step 1969, loss 1.06214, acc 0.515625\n",
      "2018-12-07T16:37:40.098408: step 1970, loss 1.00511, acc 0.515625\n",
      "2018-12-07T16:37:41.130348: step 1971, loss 1.00114, acc 0.625\n",
      "2018-12-07T16:37:42.270297: step 1972, loss 0.924364, acc 0.65625\n",
      "2018-12-07T16:37:43.309271: step 1973, loss 0.948362, acc 0.65625\n",
      "2018-12-07T16:37:44.283451: step 1974, loss 0.922792, acc 0.609375\n",
      "2018-12-07T16:37:45.482506: step 1975, loss 0.847848, acc 0.625\n",
      "2018-12-07T16:37:46.624493: step 1976, loss 1.01754, acc 0.625\n",
      "2018-12-07T16:37:47.686201: step 1977, loss 0.93796, acc 0.6875\n",
      "2018-12-07T16:37:48.671970: step 1978, loss 1.0602, acc 0.65625\n",
      "2018-12-07T16:37:49.657608: step 1979, loss 0.903542, acc 0.640625\n",
      "2018-12-07T16:37:50.789926: step 1980, loss 0.900206, acc 0.71875\n",
      "2018-12-07T16:37:51.814082: step 1981, loss 0.835513, acc 0.71875\n",
      "2018-12-07T16:37:52.823743: step 1982, loss 0.89027, acc 0.578125\n",
      "2018-12-07T16:37:53.900861: step 1983, loss 0.887642, acc 0.640625\n",
      "2018-12-07T16:37:54.908702: step 1984, loss 0.942482, acc 0.65625\n",
      "2018-12-07T16:37:55.914201: step 1985, loss 1.01207, acc 0.59375\n",
      "2018-12-07T16:37:56.926102: step 1986, loss 0.919728, acc 0.65625\n",
      "2018-12-07T16:37:58.031504: step 1987, loss 0.83009, acc 0.703125\n",
      "2018-12-07T16:37:59.124306: step 1988, loss 0.839844, acc 0.65625\n",
      "2018-12-07T16:38:00.157612: step 1989, loss 1.1016, acc 0.546875\n",
      "2018-12-07T16:38:01.181370: step 1990, loss 0.781101, acc 0.703125\n",
      "2018-12-07T16:38:02.203192: step 1991, loss 0.905454, acc 0.6875\n",
      "2018-12-07T16:38:03.386478: step 1992, loss 0.834965, acc 0.703125\n",
      "2018-12-07T16:38:04.509484: step 1993, loss 0.860607, acc 0.6875\n",
      "2018-12-07T16:38:05.518539: step 1994, loss 1.03385, acc 0.578125\n",
      "2018-12-07T16:38:06.685426: step 1995, loss 0.870408, acc 0.71875\n",
      "2018-12-07T16:38:07.761355: step 1996, loss 1.08308, acc 0.53125\n",
      "2018-12-07T16:38:08.887822: step 1997, loss 0.838716, acc 0.625\n",
      "2018-12-07T16:38:09.911740: step 1998, loss 0.924888, acc 0.59375\n",
      "2018-12-07T16:38:11.032653: step 1999, loss 1.20929, acc 0.59375\n",
      "2018-12-07T16:38:12.003906: step 2000, loss 0.869172, acc 0.703125\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T16:38:13.233617: step 2000, loss 0.848045, acc 0.7\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-2000\n",
      "\n",
      "2018-12-07T16:38:14.936240: step 2001, loss 0.885199, acc 0.65625\n",
      "2018-12-07T16:38:16.002068: step 2002, loss 1.04936, acc 0.578125\n",
      "2018-12-07T16:38:17.432380: step 2003, loss 0.772547, acc 0.734375\n",
      "2018-12-07T16:38:18.495369: step 2004, loss 1.08642, acc 0.5625\n",
      "2018-12-07T16:38:19.565269: step 2005, loss 0.958344, acc 0.609375\n",
      "2018-12-07T16:38:20.607809: step 2006, loss 0.857644, acc 0.640625\n",
      "2018-12-07T16:38:21.625800: step 2007, loss 1.01138, acc 0.59375\n",
      "2018-12-07T16:38:22.841413: step 2008, loss 0.993077, acc 0.578125\n",
      "2018-12-07T16:38:23.921403: step 2009, loss 0.929339, acc 0.65625\n",
      "2018-12-07T16:38:25.201830: step 2010, loss 0.876527, acc 0.625\n",
      "2018-12-07T16:38:26.245800: step 2011, loss 0.936573, acc 0.640625\n",
      "2018-12-07T16:38:27.354160: step 2012, loss 0.855603, acc 0.734375\n",
      "2018-12-07T16:38:28.435158: step 2013, loss 0.980391, acc 0.640625\n",
      "2018-12-07T16:38:29.536624: step 2014, loss 0.892798, acc 0.671875\n",
      "2018-12-07T16:38:30.883660: step 2015, loss 1.04286, acc 0.578125\n",
      "2018-12-07T16:38:31.969540: step 2016, loss 1.03423, acc 0.5625\n",
      "2018-12-07T16:38:33.144240: step 2017, loss 0.957846, acc 0.609375\n",
      "2018-12-07T16:38:34.369787: step 2018, loss 0.938046, acc 0.609375\n",
      "2018-12-07T16:38:35.543613: step 2019, loss 0.85864, acc 0.734375\n",
      "2018-12-07T16:38:36.572532: step 2020, loss 0.804719, acc 0.78125\n",
      "2018-12-07T16:38:37.681065: step 2021, loss 0.855778, acc 0.65625\n",
      "2018-12-07T16:38:38.733293: step 2022, loss 0.983326, acc 0.625\n",
      "2018-12-07T16:38:39.928854: step 2023, loss 0.814529, acc 0.6875\n",
      "2018-12-07T16:38:41.127830: step 2024, loss 0.929319, acc 0.625\n",
      "2018-12-07T16:38:42.149255: step 2025, loss 0.91128, acc 0.65625\n",
      "2018-12-07T16:38:43.230079: step 2026, loss 0.849766, acc 0.6875\n",
      "2018-12-07T16:38:44.449896: step 2027, loss 1.01974, acc 0.53125\n",
      "2018-12-07T16:38:45.984472: step 2028, loss 0.942547, acc 0.625\n",
      "2018-12-07T16:38:46.999491: step 2029, loss 0.983922, acc 0.625\n",
      "2018-12-07T16:38:48.273340: step 2030, loss 0.822892, acc 0.640625\n",
      "2018-12-07T16:38:49.603523: step 2031, loss 1.09138, acc 0.59375\n",
      "2018-12-07T16:38:50.672013: step 2032, loss 0.7958, acc 0.65625\n",
      "2018-12-07T16:38:51.911487: step 2033, loss 0.892849, acc 0.65625\n",
      "2018-12-07T16:38:53.192221: step 2034, loss 0.938638, acc 0.609375\n",
      "2018-12-07T16:38:54.250771: step 2035, loss 0.856969, acc 0.609375\n",
      "2018-12-07T16:38:55.458999: step 2036, loss 0.932264, acc 0.625\n",
      "2018-12-07T16:38:56.647090: step 2037, loss 0.780185, acc 0.71875\n",
      "2018-12-07T16:38:57.816824: step 2038, loss 1.06025, acc 0.609375\n",
      "2018-12-07T16:38:59.168071: step 2039, loss 0.973478, acc 0.59375\n",
      "2018-12-07T16:39:00.519856: step 2040, loss 0.842094, acc 0.6875\n",
      "2018-12-07T16:39:01.845178: step 2041, loss 0.77415, acc 0.765625\n",
      "2018-12-07T16:39:02.887124: step 2042, loss 0.794431, acc 0.75\n",
      "2018-12-07T16:39:04.071185: step 2043, loss 0.826549, acc 0.671875\n",
      "2018-12-07T16:39:05.165889: step 2044, loss 0.950006, acc 0.703125\n",
      "2018-12-07T16:39:06.204739: step 2045, loss 0.813075, acc 0.703125\n",
      "2018-12-07T16:39:07.263649: step 2046, loss 0.892183, acc 0.640625\n",
      "2018-12-07T16:39:08.303351: step 2047, loss 0.799827, acc 0.734375\n",
      "2018-12-07T16:39:09.526100: step 2048, loss 0.958937, acc 0.625\n",
      "2018-12-07T16:39:10.524100: step 2049, loss 0.821751, acc 0.671875\n",
      "2018-12-07T16:39:11.585650: step 2050, loss 0.848791, acc 0.65625\n",
      "2018-12-07T16:39:12.635529: step 2051, loss 0.886206, acc 0.6875\n",
      "2018-12-07T16:39:13.880840: step 2052, loss 1.04946, acc 0.609375\n",
      "2018-12-07T16:39:15.306266: step 2053, loss 0.78564, acc 0.71875\n",
      "2018-12-07T16:39:16.358053: step 2054, loss 0.895938, acc 0.65625\n",
      "2018-12-07T16:39:17.478901: step 2055, loss 0.997153, acc 0.59375\n",
      "2018-12-07T16:39:18.601804: step 2056, loss 1.06079, acc 0.640625\n",
      "2018-12-07T16:39:19.985600: step 2057, loss 0.776859, acc 0.671875\n",
      "2018-12-07T16:39:21.258672: step 2058, loss 0.704763, acc 0.75\n",
      "2018-12-07T16:39:22.482143: step 2059, loss 0.9788, acc 0.53125\n",
      "2018-12-07T16:39:23.713457: step 2060, loss 0.73592, acc 0.671875\n",
      "2018-12-07T16:39:24.818353: step 2061, loss 0.833402, acc 0.671875\n",
      "2018-12-07T16:39:25.975330: step 2062, loss 0.834833, acc 0.671875\n",
      "2018-12-07T16:39:27.389329: step 2063, loss 0.990656, acc 0.578125\n",
      "2018-12-07T16:39:28.440577: step 2064, loss 0.79552, acc 0.6875\n",
      "2018-12-07T16:39:29.476481: step 2065, loss 0.700225, acc 0.765625\n",
      "2018-12-07T16:39:30.786224: step 2066, loss 0.975471, acc 0.546875\n",
      "2018-12-07T16:39:31.896344: step 2067, loss 1.00255, acc 0.609375\n",
      "2018-12-07T16:39:33.175395: step 2068, loss 0.825681, acc 0.671875\n",
      "2018-12-07T16:39:34.177986: step 2069, loss 0.77539, acc 0.703125\n",
      "2018-12-07T16:39:35.276627: step 2070, loss 0.808013, acc 0.671875\n",
      "2018-12-07T16:39:36.297986: step 2071, loss 1.05428, acc 0.5625\n",
      "2018-12-07T16:39:37.571423: step 2072, loss 0.82166, acc 0.65625\n",
      "2018-12-07T16:39:38.768037: step 2073, loss 0.87266, acc 0.671875\n",
      "2018-12-07T16:39:40.073009: step 2074, loss 0.810983, acc 0.6875\n",
      "2018-12-07T16:39:41.305169: step 2075, loss 1.05058, acc 0.578125\n",
      "2018-12-07T16:39:42.621373: step 2076, loss 1.10583, acc 0.546875\n",
      "2018-12-07T16:39:43.739432: step 2077, loss 1.04576, acc 0.59375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07T16:39:44.751136: step 2078, loss 0.834362, acc 0.671875\n",
      "2018-12-07T16:39:45.979933: step 2079, loss 1.11569, acc 0.53125\n",
      "2018-12-07T16:39:47.167552: step 2080, loss 0.783361, acc 0.71875\n",
      "2018-12-07T16:39:48.402815: step 2081, loss 0.891817, acc 0.6875\n",
      "2018-12-07T16:39:49.693815: step 2082, loss 0.788643, acc 0.65625\n",
      "2018-12-07T16:39:50.749752: step 2083, loss 0.796833, acc 0.671875\n",
      "2018-12-07T16:39:52.097653: step 2084, loss 0.92527, acc 0.578125\n",
      "2018-12-07T16:39:53.377521: step 2085, loss 0.937951, acc 0.671875\n",
      "2018-12-07T16:39:54.514128: step 2086, loss 1.04623, acc 0.5625\n",
      "2018-12-07T16:39:55.524396: step 2087, loss 1.11497, acc 0.578125\n",
      "2018-12-07T16:39:56.543439: step 2088, loss 0.912029, acc 0.640625\n",
      "2018-12-07T16:39:57.686761: step 2089, loss 0.672596, acc 0.71875\n",
      "2018-12-07T16:39:58.967362: step 2090, loss 0.930163, acc 0.609375\n",
      "2018-12-07T16:39:59.991494: step 2091, loss 0.883961, acc 0.71875\n",
      "2018-12-07T16:40:01.057526: step 2092, loss 0.997951, acc 0.609375\n",
      "2018-12-07T16:40:02.117427: step 2093, loss 0.970388, acc 0.671875\n",
      "2018-12-07T16:40:03.145875: step 2094, loss 0.80486, acc 0.671875\n",
      "2018-12-07T16:40:04.369767: step 2095, loss 0.886061, acc 0.6875\n",
      "2018-12-07T16:40:05.419674: step 2096, loss 0.874632, acc 0.71875\n",
      "2018-12-07T16:40:06.618555: step 2097, loss 0.870749, acc 0.6875\n",
      "2018-12-07T16:40:07.712523: step 2098, loss 0.97138, acc 0.640625\n",
      "2018-12-07T16:40:08.870964: step 2099, loss 1.12649, acc 0.515625\n",
      "2018-12-07T16:40:09.961953: step 2100, loss 0.775753, acc 0.703125\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T16:40:11.192662: step 2100, loss 0.828392, acc 0.728\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-2100\n",
      "\n",
      "2018-12-07T16:40:12.765751: step 2101, loss 0.998439, acc 0.5625\n",
      "2018-12-07T16:40:14.029636: step 2102, loss 0.836901, acc 0.625\n",
      "2018-12-07T16:40:15.260003: step 2103, loss 0.90693, acc 0.703125\n",
      "2018-12-07T16:40:16.746250: step 2104, loss 0.765924, acc 0.78125\n",
      "2018-12-07T16:40:18.039734: step 2105, loss 0.954788, acc 0.625\n",
      "2018-12-07T16:40:19.778335: step 2106, loss 0.836926, acc 0.703125\n",
      "2018-12-07T16:40:21.058243: step 2107, loss 0.891965, acc 0.609375\n",
      "2018-12-07T16:40:22.189464: step 2108, loss 0.842208, acc 0.640625\n",
      "2018-12-07T16:40:23.405936: step 2109, loss 0.955222, acc 0.65625\n",
      "2018-12-07T16:40:24.645933: step 2110, loss 0.946035, acc 0.59375\n",
      "2018-12-07T16:40:25.899140: step 2111, loss 0.9603, acc 0.609375\n",
      "2018-12-07T16:40:27.738674: step 2112, loss 1.13677, acc 0.5\n",
      "2018-12-07T16:40:29.131087: step 2113, loss 1.21713, acc 0.515625\n",
      "2018-12-07T16:40:30.379589: step 2114, loss 0.780128, acc 0.65625\n",
      "2018-12-07T16:40:31.454833: step 2115, loss 0.814641, acc 0.734375\n",
      "2018-12-07T16:40:32.874814: step 2116, loss 0.866967, acc 0.65625\n",
      "2018-12-07T16:40:34.325481: step 2117, loss 0.808932, acc 0.640625\n",
      "2018-12-07T16:40:35.581151: step 2118, loss 0.97007, acc 0.625\n",
      "2018-12-07T16:40:37.174355: step 2119, loss 0.99825, acc 0.59375\n",
      "2018-12-07T16:40:38.318096: step 2120, loss 1.08126, acc 0.578125\n",
      "2018-12-07T16:40:39.665326: step 2121, loss 0.88219, acc 0.671875\n",
      "2018-12-07T16:40:40.979828: step 2122, loss 0.875504, acc 0.671875\n",
      "2018-12-07T16:40:42.496751: step 2123, loss 0.846855, acc 0.640625\n",
      "2018-12-07T16:40:43.618524: step 2124, loss 0.848882, acc 0.6875\n",
      "2018-12-07T16:40:44.837107: step 2125, loss 1.13369, acc 0.53125\n",
      "2018-12-07T16:40:46.236666: step 2126, loss 0.793486, acc 0.71875\n",
      "2018-12-07T16:40:47.310284: step 2127, loss 0.793426, acc 0.71875\n",
      "2018-12-07T16:40:48.483026: step 2128, loss 0.78338, acc 0.703125\n",
      "2018-12-07T16:40:49.821522: step 2129, loss 0.891938, acc 0.671875\n",
      "2018-12-07T16:40:50.988326: step 2130, loss 0.893958, acc 0.671875\n",
      "2018-12-07T16:40:52.050812: step 2131, loss 0.906289, acc 0.625\n",
      "2018-12-07T16:40:53.296867: step 2132, loss 0.974078, acc 0.625\n",
      "2018-12-07T16:40:54.555815: step 2133, loss 0.996036, acc 0.640625\n",
      "2018-12-07T16:40:55.745925: step 2134, loss 0.776065, acc 0.671875\n",
      "2018-12-07T16:40:57.274249: step 2135, loss 0.899508, acc 0.625\n",
      "2018-12-07T16:40:58.518951: step 2136, loss 0.852791, acc 0.703125\n",
      "2018-12-07T16:40:59.623904: step 2137, loss 0.802745, acc 0.703125\n",
      "2018-12-07T16:41:00.736657: step 2138, loss 1.13103, acc 0.5625\n",
      "2018-12-07T16:41:02.045621: step 2139, loss 0.783534, acc 0.75\n",
      "2018-12-07T16:41:03.262254: step 2140, loss 0.842199, acc 0.625\n",
      "2018-12-07T16:41:04.485552: step 2141, loss 0.76973, acc 0.6875\n",
      "2018-12-07T16:41:05.633953: step 2142, loss 0.999321, acc 0.53125\n",
      "2018-12-07T16:41:06.692390: step 2143, loss 1.04593, acc 0.609375\n",
      "2018-12-07T16:41:07.881759: step 2144, loss 0.866739, acc 0.703125\n",
      "2018-12-07T16:41:09.022388: step 2145, loss 0.885644, acc 0.703125\n",
      "2018-12-07T16:41:10.547936: step 2146, loss 0.921475, acc 0.671875\n",
      "2018-12-07T16:41:11.709467: step 2147, loss 0.900101, acc 0.671875\n",
      "2018-12-07T16:41:12.992594: step 2148, loss 0.795644, acc 0.703125\n",
      "2018-12-07T16:41:14.397784: step 2149, loss 0.920607, acc 0.609375\n",
      "2018-12-07T16:41:15.576279: step 2150, loss 0.934715, acc 0.609375\n",
      "2018-12-07T16:41:16.718750: step 2151, loss 0.888269, acc 0.609375\n",
      "2018-12-07T16:41:18.038265: step 2152, loss 0.870574, acc 0.6875\n",
      "2018-12-07T16:41:19.664857: step 2153, loss 0.864497, acc 0.59375\n",
      "2018-12-07T16:41:20.805549: step 2154, loss 0.915179, acc 0.640625\n",
      "2018-12-07T16:41:22.220674: step 2155, loss 0.915405, acc 0.6875\n",
      "2018-12-07T16:41:23.365295: step 2156, loss 0.883105, acc 0.65625\n",
      "2018-12-07T16:41:24.706978: step 2157, loss 0.808708, acc 0.703125\n",
      "2018-12-07T16:41:25.856670: step 2158, loss 0.85665, acc 0.71875\n",
      "2018-12-07T16:41:27.208440: step 2159, loss 0.841416, acc 0.65625\n",
      "2018-12-07T16:41:28.394307: step 2160, loss 0.893356, acc 0.671875\n",
      "2018-12-07T16:41:29.592507: step 2161, loss 0.84089, acc 0.671875\n",
      "2018-12-07T16:41:30.999335: step 2162, loss 0.810392, acc 0.71875\n",
      "2018-12-07T16:41:32.086846: step 2163, loss 0.722058, acc 0.75\n",
      "2018-12-07T16:41:33.346067: step 2164, loss 1.02127, acc 0.59375\n",
      "2018-12-07T16:41:34.661528: step 2165, loss 0.821793, acc 0.640625\n",
      "2018-12-07T16:41:35.767234: step 2166, loss 0.871966, acc 0.6875\n",
      "2018-12-07T16:41:37.062188: step 2167, loss 0.91076, acc 0.59375\n",
      "2018-12-07T16:41:38.386904: step 2168, loss 0.807975, acc 0.71875\n",
      "2018-12-07T16:41:39.688937: step 2169, loss 0.817729, acc 0.71875\n",
      "2018-12-07T16:41:40.881854: step 2170, loss 0.925816, acc 0.609375\n",
      "2018-12-07T16:41:42.288380: step 2171, loss 0.82648, acc 0.703125\n",
      "2018-12-07T16:41:43.512901: step 2172, loss 1.10809, acc 0.59375\n",
      "2018-12-07T16:41:44.801078: step 2173, loss 0.864618, acc 0.65625\n",
      "2018-12-07T16:41:46.085335: step 2174, loss 0.866231, acc 0.65625\n",
      "2018-12-07T16:41:47.249430: step 2175, loss 0.969404, acc 0.625\n",
      "2018-12-07T16:41:48.593575: step 2176, loss 0.829495, acc 0.703125\n",
      "2018-12-07T16:41:49.864393: step 2177, loss 0.789003, acc 0.703125\n",
      "2018-12-07T16:41:51.268880: step 2178, loss 0.816691, acc 0.65625\n",
      "2018-12-07T16:41:52.368425: step 2179, loss 0.831941, acc 0.6875\n",
      "2018-12-07T16:41:53.659941: step 2180, loss 0.964188, acc 0.640625\n",
      "2018-12-07T16:41:54.960127: step 2181, loss 0.842436, acc 0.65625\n",
      "2018-12-07T16:41:56.314636: step 2182, loss 0.964047, acc 0.625\n",
      "2018-12-07T16:41:57.842859: step 2183, loss 0.971427, acc 0.640625\n",
      "2018-12-07T16:41:59.133373: step 2184, loss 0.724569, acc 0.71875\n",
      "2018-12-07T16:42:00.360628: step 2185, loss 0.927819, acc 0.640625\n",
      "2018-12-07T16:42:01.845403: step 2186, loss 0.984018, acc 0.640625\n",
      "2018-12-07T16:42:02.994836: step 2187, loss 0.859457, acc 0.671875\n",
      "2018-12-07T16:42:04.201256: step 2188, loss 0.905498, acc 0.640625\n",
      "2018-12-07T16:42:05.603373: step 2189, loss 0.857939, acc 0.75\n",
      "2018-12-07T16:42:06.816045: step 2190, loss 1.03849, acc 0.5625\n",
      "2018-12-07T16:42:08.216733: step 2191, loss 0.962611, acc 0.578125\n",
      "2018-12-07T16:42:09.703600: step 2192, loss 0.967306, acc 0.6875\n",
      "2018-12-07T16:42:11.151612: step 2193, loss 0.945531, acc 0.71875\n",
      "2018-12-07T16:42:12.507632: step 2194, loss 0.965886, acc 0.640625\n",
      "2018-12-07T16:42:13.790948: step 2195, loss 1.00757, acc 0.625\n",
      "2018-12-07T16:42:14.819563: step 2196, loss 1.01181, acc 0.609375\n",
      "2018-12-07T16:42:16.100090: step 2197, loss 0.975248, acc 0.671875\n",
      "2018-12-07T16:42:17.190781: step 2198, loss 0.845614, acc 0.546875\n",
      "2018-12-07T16:42:18.630107: step 2199, loss 0.834064, acc 0.671875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07T16:42:19.845203: step 2200, loss 0.842986, acc 0.71875\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T16:42:21.064942: step 2200, loss 0.823384, acc 0.728\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-2200\n",
      "\n",
      "2018-12-07T16:42:22.948200: step 2201, loss 0.948984, acc 0.609375\n",
      "2018-12-07T16:42:24.428330: step 2202, loss 0.892304, acc 0.734375\n",
      "2018-12-07T16:42:25.835131: step 2203, loss 0.749371, acc 0.703125\n",
      "2018-12-07T16:42:27.086014: step 2204, loss 0.923545, acc 0.6875\n",
      "2018-12-07T16:42:28.669626: step 2205, loss 0.865403, acc 0.625\n",
      "2018-12-07T16:42:30.444304: step 2206, loss 0.970171, acc 0.609375\n",
      "2018-12-07T16:42:31.967188: step 2207, loss 0.863623, acc 0.59375\n",
      "2018-12-07T16:42:33.326482: step 2208, loss 0.868686, acc 0.625\n",
      "2018-12-07T16:42:34.531528: step 2209, loss 0.905231, acc 0.671875\n",
      "2018-12-07T16:42:35.776632: step 2210, loss 0.815803, acc 0.703125\n",
      "2018-12-07T16:42:37.153214: step 2211, loss 0.941865, acc 0.59375\n",
      "2018-12-07T16:42:38.506992: step 2212, loss 1.0267, acc 0.609375\n",
      "2018-12-07T16:42:40.062095: step 2213, loss 0.922739, acc 0.640625\n",
      "2018-12-07T16:42:41.478975: step 2214, loss 0.799016, acc 0.703125\n",
      "2018-12-07T16:42:42.662451: step 2215, loss 0.847993, acc 0.6875\n",
      "2018-12-07T16:42:43.854453: step 2216, loss 0.853731, acc 0.671875\n",
      "2018-12-07T16:42:45.766232: step 2217, loss 0.844816, acc 0.65625\n",
      "2018-12-07T16:42:46.989511: step 2218, loss 0.931942, acc 0.65625\n",
      "2018-12-07T16:42:48.633392: step 2219, loss 0.835706, acc 0.6875\n",
      "2018-12-07T16:42:49.782308: step 2220, loss 0.897631, acc 0.59375\n",
      "2018-12-07T16:42:50.967546: step 2221, loss 0.926608, acc 0.640625\n",
      "2018-12-07T16:42:52.618802: step 2222, loss 0.739396, acc 0.75\n",
      "2018-12-07T16:42:53.967514: step 2223, loss 0.881875, acc 0.640625\n",
      "2018-12-07T16:42:55.460226: step 2224, loss 1.00348, acc 0.5625\n",
      "2018-12-07T16:42:57.006545: step 2225, loss 0.890758, acc 0.6875\n",
      "2018-12-07T16:42:58.738134: step 2226, loss 0.804792, acc 0.703125\n",
      "2018-12-07T16:43:00.231771: step 2227, loss 0.918096, acc 0.625\n",
      "2018-12-07T16:43:01.403614: step 2228, loss 0.986203, acc 0.625\n",
      "2018-12-07T16:43:02.858261: step 2229, loss 1.05856, acc 0.546875\n",
      "2018-12-07T16:43:04.291308: step 2230, loss 0.964517, acc 0.671875\n",
      "2018-12-07T16:43:05.486943: step 2231, loss 0.718037, acc 0.71875\n",
      "2018-12-07T16:43:06.958083: step 2232, loss 0.84407, acc 0.671875\n",
      "2018-12-07T16:43:08.336059: step 2233, loss 0.817154, acc 0.6875\n",
      "2018-12-07T16:43:10.147448: step 2234, loss 0.962896, acc 0.65625\n",
      "2018-12-07T16:43:11.496325: step 2235, loss 0.818485, acc 0.71875\n",
      "2018-12-07T16:43:12.896002: step 2236, loss 1.01113, acc 0.625\n",
      "2018-12-07T16:43:14.216172: step 2237, loss 0.859032, acc 0.671875\n",
      "2018-12-07T16:43:15.883558: step 2238, loss 0.785564, acc 0.71875\n",
      "2018-12-07T16:43:17.507516: step 2239, loss 0.876735, acc 0.65625\n",
      "2018-12-07T16:43:19.220122: step 2240, loss 0.971523, acc 0.59375\n",
      "2018-12-07T16:43:20.543997: step 2241, loss 0.787696, acc 0.703125\n",
      "2018-12-07T16:43:21.830505: step 2242, loss 0.840003, acc 0.703125\n",
      "2018-12-07T16:43:23.225488: step 2243, loss 0.883162, acc 0.734375\n",
      "2018-12-07T16:43:24.431860: step 2244, loss 0.905095, acc 0.65625\n",
      "2018-12-07T16:43:26.176090: step 2245, loss 0.835472, acc 0.734375\n",
      "2018-12-07T16:43:27.568662: step 2246, loss 0.993714, acc 0.59375\n",
      "2018-12-07T16:43:29.070528: step 2247, loss 1.08962, acc 0.515625\n",
      "2018-12-07T16:43:30.361052: step 2248, loss 0.926573, acc 0.65625\n",
      "2018-12-07T16:43:31.848797: step 2249, loss 1.07078, acc 0.5\n",
      "2018-12-07T16:43:33.445026: step 2250, loss 0.895889, acc 0.65625\n",
      "2018-12-07T16:43:34.607366: step 2251, loss 1.06133, acc 0.546875\n",
      "2018-12-07T16:43:35.975228: step 2252, loss 0.769434, acc 0.71875\n",
      "2018-12-07T16:43:37.508130: step 2253, loss 1.11234, acc 0.53125\n",
      "2018-12-07T16:43:39.101096: step 2254, loss 0.874001, acc 0.671875\n",
      "2018-12-07T16:43:40.465281: step 2255, loss 0.856259, acc 0.640625\n",
      "2018-12-07T16:43:42.009852: step 2256, loss 1.02368, acc 0.5625\n",
      "2018-12-07T16:43:43.468869: step 2257, loss 0.897288, acc 0.671875\n",
      "2018-12-07T16:43:44.823839: step 2258, loss 0.962708, acc 0.546875\n",
      "2018-12-07T16:43:46.245838: step 2259, loss 0.669613, acc 0.765625\n",
      "2018-12-07T16:43:47.607118: step 2260, loss 1.07534, acc 0.609375\n",
      "2018-12-07T16:43:48.904351: step 2261, loss 0.859967, acc 0.640625\n",
      "2018-12-07T16:43:50.543224: step 2262, loss 0.998411, acc 0.609375\n",
      "2018-12-07T16:43:52.073178: step 2263, loss 0.825935, acc 0.75\n",
      "2018-12-07T16:43:53.855412: step 2264, loss 0.799251, acc 0.671875\n",
      "2018-12-07T16:43:55.652819: step 2265, loss 0.85192, acc 0.640625\n",
      "2018-12-07T16:43:57.283253: step 2266, loss 0.877314, acc 0.65625\n",
      "2018-12-07T16:43:58.638688: step 2267, loss 1.00287, acc 0.609375\n",
      "2018-12-07T16:44:00.054525: step 2268, loss 0.946771, acc 0.578125\n",
      "2018-12-07T16:44:01.832282: step 2269, loss 0.971096, acc 0.59375\n",
      "2018-12-07T16:44:03.468434: step 2270, loss 0.853736, acc 0.640625\n",
      "2018-12-07T16:44:05.288562: step 2271, loss 0.951972, acc 0.609375\n",
      "2018-12-07T16:44:06.713015: step 2272, loss 1.02336, acc 0.546875\n",
      "2018-12-07T16:44:08.372338: step 2273, loss 0.914279, acc 0.59375\n",
      "2018-12-07T16:44:10.157747: step 2274, loss 0.843303, acc 0.703125\n",
      "2018-12-07T16:44:11.983036: step 2275, loss 0.832884, acc 0.625\n",
      "2018-12-07T16:44:14.305710: step 2276, loss 1.02526, acc 0.640625\n",
      "2018-12-07T16:44:15.971454: step 2277, loss 1.00574, acc 0.609375\n",
      "2018-12-07T16:44:17.661490: step 2278, loss 0.939551, acc 0.625\n",
      "2018-12-07T16:44:19.162292: step 2279, loss 0.772859, acc 0.765625\n",
      "2018-12-07T16:44:20.800684: step 2280, loss 0.900915, acc 0.578125\n",
      "2018-12-07T16:44:22.402254: step 2281, loss 0.719252, acc 0.734375\n",
      "2018-12-07T16:44:23.862588: step 2282, loss 0.832406, acc 0.671875\n",
      "2018-12-07T16:44:25.581179: step 2283, loss 0.992209, acc 0.578125\n",
      "2018-12-07T16:44:27.466358: step 2284, loss 0.986296, acc 0.53125\n",
      "2018-12-07T16:44:29.048038: step 2285, loss 0.893793, acc 0.65625\n",
      "2018-12-07T16:44:30.970729: step 2286, loss 1.04078, acc 0.59375\n",
      "2018-12-07T16:44:32.338157: step 2287, loss 0.951522, acc 0.578125\n",
      "2018-12-07T16:44:33.966522: step 2288, loss 0.874741, acc 0.625\n",
      "2018-12-07T16:44:36.163856: step 2289, loss 0.798077, acc 0.6875\n",
      "2018-12-07T16:44:38.064618: step 2290, loss 0.837456, acc 0.671875\n",
      "2018-12-07T16:44:40.027623: step 2291, loss 0.923109, acc 0.609375\n",
      "2018-12-07T16:44:41.610488: step 2292, loss 0.709798, acc 0.78125\n",
      "2018-12-07T16:44:43.299305: step 2293, loss 0.877043, acc 0.703125\n",
      "2018-12-07T16:44:45.178808: step 2294, loss 0.823272, acc 0.640625\n",
      "2018-12-07T16:44:46.782450: step 2295, loss 0.866531, acc 0.6875\n",
      "2018-12-07T16:44:48.794395: step 2296, loss 0.807929, acc 0.734375\n",
      "2018-12-07T16:44:50.597248: step 2297, loss 0.850621, acc 0.625\n",
      "2018-12-07T16:44:52.124425: step 2298, loss 0.816262, acc 0.640625\n",
      "2018-12-07T16:44:53.823488: step 2299, loss 0.858649, acc 0.671875\n",
      "2018-12-07T16:44:55.726772: step 2300, loss 0.938065, acc 0.640625\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T16:44:56.968450: step 2300, loss 0.776184, acc 0.74\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-2300\n",
      "\n",
      "2018-12-07T16:44:59.321062: step 2301, loss 0.904527, acc 0.609375\n",
      "2018-12-07T16:45:01.270396: step 2302, loss 0.874783, acc 0.6875\n",
      "2018-12-07T16:45:03.056261: step 2303, loss 0.920398, acc 0.609375\n",
      "2018-12-07T16:45:04.607666: step 2304, loss 0.984571, acc 0.625\n",
      "2018-12-07T16:45:06.068174: step 2305, loss 1.06326, acc 0.578125\n",
      "2018-12-07T16:45:07.600591: step 2306, loss 0.739017, acc 0.734375\n",
      "2018-12-07T16:45:09.812759: step 2307, loss 1.07417, acc 0.609375\n",
      "2018-12-07T16:45:11.493908: step 2308, loss 0.85379, acc 0.671875\n",
      "2018-12-07T16:45:13.036237: step 2309, loss 1.07485, acc 0.609375\n",
      "2018-12-07T16:45:15.175285: step 2310, loss 0.889384, acc 0.671875\n",
      "2018-12-07T16:45:17.299178: step 2311, loss 0.974401, acc 0.578125\n",
      "2018-12-07T16:45:19.095402: step 2312, loss 0.861092, acc 0.6875\n",
      "2018-12-07T16:45:20.734143: step 2313, loss 0.84435, acc 0.640625\n",
      "2018-12-07T16:45:22.105281: step 2314, loss 0.902731, acc 0.625\n",
      "2018-12-07T16:45:24.259131: step 2315, loss 0.826489, acc 0.734375\n",
      "2018-12-07T16:45:25.750014: step 2316, loss 0.879448, acc 0.625\n",
      "2018-12-07T16:45:27.617995: step 2317, loss 0.953973, acc 0.671875\n",
      "2018-12-07T16:45:29.399942: step 2318, loss 0.996021, acc 0.65625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07T16:45:31.593738: step 2319, loss 0.868963, acc 0.671875\n",
      "2018-12-07T16:45:33.221524: step 2320, loss 1.02518, acc 0.578125\n",
      "2018-12-07T16:45:34.887437: step 2321, loss 1.16929, acc 0.46875\n",
      "2018-12-07T16:45:36.206508: step 2322, loss 0.944344, acc 0.717391\n",
      "2018-12-07T16:45:37.423788: step 2323, loss 0.879112, acc 0.640625\n",
      "2018-12-07T16:45:38.433723: step 2324, loss 0.848262, acc 0.65625\n",
      "2018-12-07T16:45:39.423760: step 2325, loss 0.817679, acc 0.6875\n",
      "2018-12-07T16:45:40.434205: step 2326, loss 0.856422, acc 0.703125\n",
      "2018-12-07T16:45:41.691833: step 2327, loss 0.77404, acc 0.734375\n",
      "2018-12-07T16:45:42.661833: step 2328, loss 0.797726, acc 0.765625\n",
      "2018-12-07T16:45:43.640122: step 2329, loss 0.900214, acc 0.65625\n",
      "2018-12-07T16:45:44.663261: step 2330, loss 0.775446, acc 0.734375\n",
      "2018-12-07T16:45:45.663900: step 2331, loss 0.898311, acc 0.765625\n",
      "2018-12-07T16:45:46.832479: step 2332, loss 0.881008, acc 0.671875\n",
      "2018-12-07T16:45:47.839600: step 2333, loss 0.884863, acc 0.65625\n",
      "2018-12-07T16:45:48.808537: step 2334, loss 0.739129, acc 0.6875\n",
      "2018-12-07T16:45:49.805199: step 2335, loss 0.883913, acc 0.65625\n",
      "2018-12-07T16:45:50.793817: step 2336, loss 0.794356, acc 0.671875\n",
      "2018-12-07T16:45:51.981779: step 2337, loss 0.827187, acc 0.671875\n",
      "2018-12-07T16:45:53.008955: step 2338, loss 0.905705, acc 0.65625\n",
      "2018-12-07T16:45:53.990820: step 2339, loss 0.918437, acc 0.65625\n",
      "2018-12-07T16:45:55.128828: step 2340, loss 0.911268, acc 0.65625\n",
      "2018-12-07T16:45:56.096363: step 2341, loss 0.691075, acc 0.78125\n",
      "2018-12-07T16:45:57.106887: step 2342, loss 0.836704, acc 0.703125\n",
      "2018-12-07T16:45:58.170692: step 2343, loss 0.814334, acc 0.6875\n",
      "2018-12-07T16:45:59.173070: step 2344, loss 0.881723, acc 0.625\n",
      "2018-12-07T16:46:00.345821: step 2345, loss 0.869873, acc 0.6875\n",
      "2018-12-07T16:46:01.333506: step 2346, loss 0.588368, acc 0.796875\n",
      "2018-12-07T16:46:02.322852: step 2347, loss 0.769842, acc 0.671875\n",
      "2018-12-07T16:46:03.353506: step 2348, loss 0.938321, acc 0.640625\n",
      "2018-12-07T16:46:04.345306: step 2349, loss 0.826818, acc 0.625\n",
      "2018-12-07T16:46:05.386481: step 2350, loss 0.870639, acc 0.671875\n",
      "2018-12-07T16:46:06.367759: step 2351, loss 0.913051, acc 0.65625\n",
      "2018-12-07T16:46:07.365895: step 2352, loss 0.847508, acc 0.6875\n",
      "2018-12-07T16:46:08.383869: step 2353, loss 0.828617, acc 0.65625\n",
      "2018-12-07T16:46:09.533116: step 2354, loss 0.794126, acc 0.6875\n",
      "2018-12-07T16:46:10.518487: step 2355, loss 0.859238, acc 0.640625\n",
      "2018-12-07T16:46:11.552023: step 2356, loss 0.834411, acc 0.6875\n",
      "2018-12-07T16:46:12.568707: step 2357, loss 0.875832, acc 0.640625\n",
      "2018-12-07T16:46:13.634254: step 2358, loss 1.04411, acc 0.671875\n",
      "2018-12-07T16:46:14.648646: step 2359, loss 0.916299, acc 0.625\n",
      "2018-12-07T16:46:15.789641: step 2360, loss 0.74555, acc 0.75\n",
      "2018-12-07T16:46:17.047389: step 2361, loss 0.953619, acc 0.609375\n",
      "2018-12-07T16:46:18.132038: step 2362, loss 0.81272, acc 0.640625\n",
      "2018-12-07T16:46:19.146223: step 2363, loss 0.863762, acc 0.703125\n",
      "2018-12-07T16:46:20.102651: step 2364, loss 0.777974, acc 0.65625\n",
      "2018-12-07T16:46:21.145261: step 2365, loss 0.797929, acc 0.6875\n",
      "2018-12-07T16:46:22.176793: step 2366, loss 0.788684, acc 0.71875\n",
      "2018-12-07T16:46:23.304482: step 2367, loss 0.774887, acc 0.71875\n",
      "2018-12-07T16:46:24.385369: step 2368, loss 0.682633, acc 0.734375\n",
      "2018-12-07T16:46:25.526091: step 2369, loss 0.993553, acc 0.578125\n",
      "2018-12-07T16:46:26.521423: step 2370, loss 0.753487, acc 0.6875\n",
      "2018-12-07T16:46:27.617400: step 2371, loss 0.944981, acc 0.65625\n",
      "2018-12-07T16:46:28.635849: step 2372, loss 0.755754, acc 0.671875\n",
      "2018-12-07T16:46:29.631696: step 2373, loss 0.926155, acc 0.703125\n",
      "2018-12-07T16:46:30.624048: step 2374, loss 0.782893, acc 0.6875\n",
      "2018-12-07T16:46:31.628891: step 2375, loss 0.761945, acc 0.71875\n",
      "2018-12-07T16:46:32.608429: step 2376, loss 0.761258, acc 0.765625\n",
      "2018-12-07T16:46:33.627851: step 2377, loss 0.80163, acc 0.703125\n",
      "2018-12-07T16:46:34.598345: step 2378, loss 0.82484, acc 0.640625\n",
      "2018-12-07T16:46:35.931584: step 2379, loss 0.824734, acc 0.71875\n",
      "2018-12-07T16:46:37.104000: step 2380, loss 0.854212, acc 0.703125\n",
      "2018-12-07T16:46:38.180208: step 2381, loss 0.839513, acc 0.703125\n",
      "2018-12-07T16:46:39.195699: step 2382, loss 0.868613, acc 0.71875\n",
      "2018-12-07T16:46:40.188385: step 2383, loss 0.76935, acc 0.65625\n",
      "2018-12-07T16:46:41.198410: step 2384, loss 0.884694, acc 0.625\n",
      "2018-12-07T16:46:42.346911: step 2385, loss 0.766546, acc 0.765625\n",
      "2018-12-07T16:46:43.973704: step 2386, loss 0.749116, acc 0.671875\n",
      "2018-12-07T16:46:44.989078: step 2387, loss 1.01102, acc 0.671875\n",
      "2018-12-07T16:46:45.977101: step 2388, loss 0.858702, acc 0.640625\n",
      "2018-12-07T16:46:46.996408: step 2389, loss 0.806813, acc 0.734375\n",
      "2018-12-07T16:46:48.092888: step 2390, loss 0.823674, acc 0.625\n",
      "2018-12-07T16:46:49.128565: step 2391, loss 0.758206, acc 0.703125\n",
      "2018-12-07T16:46:50.240910: step 2392, loss 0.906429, acc 0.625\n",
      "2018-12-07T16:46:51.442561: step 2393, loss 0.798376, acc 0.71875\n",
      "2018-12-07T16:46:52.601519: step 2394, loss 0.826204, acc 0.6875\n",
      "2018-12-07T16:46:53.731990: step 2395, loss 0.802447, acc 0.625\n",
      "2018-12-07T16:46:55.187343: step 2396, loss 0.818941, acc 0.71875\n",
      "2018-12-07T16:46:56.552944: step 2397, loss 0.763234, acc 0.71875\n",
      "2018-12-07T16:46:57.828508: step 2398, loss 0.710732, acc 0.6875\n",
      "2018-12-07T16:46:58.996068: step 2399, loss 0.712856, acc 0.734375\n",
      "2018-12-07T16:46:59.990191: step 2400, loss 0.745687, acc 0.671875\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T16:47:01.233863: step 2400, loss 0.755795, acc 0.728\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-2400\n",
      "\n",
      "2018-12-07T16:47:03.398835: step 2401, loss 0.611431, acc 0.828125\n",
      "2018-12-07T16:47:04.472446: step 2402, loss 0.795699, acc 0.671875\n",
      "2018-12-07T16:47:05.815551: step 2403, loss 0.601912, acc 0.84375\n",
      "2018-12-07T16:47:06.803830: step 2404, loss 0.793718, acc 0.71875\n",
      "2018-12-07T16:47:07.978775: step 2405, loss 0.746523, acc 0.703125\n",
      "2018-12-07T16:47:09.315320: step 2406, loss 0.891478, acc 0.59375\n",
      "2018-12-07T16:47:10.493849: step 2407, loss 0.902268, acc 0.609375\n",
      "2018-12-07T16:47:11.518843: step 2408, loss 1.00369, acc 0.59375\n",
      "2018-12-07T16:47:12.507407: step 2409, loss 0.873517, acc 0.625\n",
      "2018-12-07T16:47:13.678837: step 2410, loss 0.82357, acc 0.625\n",
      "2018-12-07T16:47:14.739088: step 2411, loss 0.731415, acc 0.78125\n",
      "2018-12-07T16:47:15.960014: step 2412, loss 0.828773, acc 0.625\n",
      "2018-12-07T16:47:17.143098: step 2413, loss 0.642808, acc 0.734375\n",
      "2018-12-07T16:47:18.257013: step 2414, loss 0.794501, acc 0.671875\n",
      "2018-12-07T16:47:19.394171: step 2415, loss 0.583281, acc 0.78125\n",
      "2018-12-07T16:47:20.515141: step 2416, loss 0.873282, acc 0.625\n",
      "2018-12-07T16:47:21.583452: step 2417, loss 0.86713, acc 0.640625\n",
      "2018-12-07T16:47:22.626909: step 2418, loss 0.709428, acc 0.75\n",
      "2018-12-07T16:47:23.943484: step 2419, loss 0.851471, acc 0.65625\n",
      "2018-12-07T16:47:25.123716: step 2420, loss 0.677448, acc 0.65625\n",
      "2018-12-07T16:47:26.566467: step 2421, loss 0.93177, acc 0.609375\n",
      "2018-12-07T16:47:27.884436: step 2422, loss 0.75285, acc 0.6875\n",
      "2018-12-07T16:47:29.367562: step 2423, loss 0.863468, acc 0.703125\n",
      "2018-12-07T16:47:30.541072: step 2424, loss 0.857919, acc 0.6875\n",
      "2018-12-07T16:47:31.572715: step 2425, loss 0.869785, acc 0.703125\n",
      "2018-12-07T16:47:32.708349: step 2426, loss 0.739543, acc 0.75\n",
      "2018-12-07T16:47:33.846012: step 2427, loss 0.648321, acc 0.796875\n",
      "2018-12-07T16:47:34.833716: step 2428, loss 0.923992, acc 0.640625\n",
      "2018-12-07T16:47:36.225428: step 2429, loss 0.784434, acc 0.703125\n",
      "2018-12-07T16:47:37.529220: step 2430, loss 0.725668, acc 0.78125\n",
      "2018-12-07T16:47:38.660064: step 2431, loss 1.00871, acc 0.640625\n",
      "2018-12-07T16:47:39.901685: step 2432, loss 0.906691, acc 0.625\n",
      "2018-12-07T16:47:40.948243: step 2433, loss 0.719897, acc 0.8125\n",
      "2018-12-07T16:47:42.014468: step 2434, loss 0.77338, acc 0.734375\n",
      "2018-12-07T16:47:43.097933: step 2435, loss 0.805253, acc 0.6875\n",
      "2018-12-07T16:47:44.238676: step 2436, loss 0.740821, acc 0.703125\n",
      "2018-12-07T16:47:45.478202: step 2437, loss 0.861036, acc 0.703125\n",
      "2018-12-07T16:47:46.801286: step 2438, loss 0.729026, acc 0.734375\n",
      "2018-12-07T16:47:47.934344: step 2439, loss 0.922849, acc 0.609375\n",
      "2018-12-07T16:47:48.986500: step 2440, loss 0.828819, acc 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07T16:47:50.012515: step 2441, loss 0.800995, acc 0.734375\n",
      "2018-12-07T16:47:51.548807: step 2442, loss 0.977332, acc 0.609375\n",
      "2018-12-07T16:47:52.592759: step 2443, loss 0.91235, acc 0.6875\n",
      "2018-12-07T16:47:53.651429: step 2444, loss 0.571655, acc 0.8125\n",
      "2018-12-07T16:47:54.675188: step 2445, loss 0.764394, acc 0.703125\n",
      "2018-12-07T16:47:55.715718: step 2446, loss 0.903905, acc 0.625\n",
      "2018-12-07T16:47:56.938152: step 2447, loss 0.838031, acc 0.703125\n",
      "2018-12-07T16:47:58.390845: step 2448, loss 0.804291, acc 0.6875\n",
      "2018-12-07T16:47:59.455743: step 2449, loss 0.88453, acc 0.578125\n",
      "2018-12-07T16:48:00.507439: step 2450, loss 0.822734, acc 0.71875\n",
      "2018-12-07T16:48:01.832460: step 2451, loss 0.873813, acc 0.59375\n",
      "2018-12-07T16:48:02.828961: step 2452, loss 0.824335, acc 0.671875\n",
      "2018-12-07T16:48:03.904259: step 2453, loss 1.11839, acc 0.546875\n",
      "2018-12-07T16:48:05.001773: step 2454, loss 0.667631, acc 0.78125\n",
      "2018-12-07T16:48:06.157217: step 2455, loss 0.874359, acc 0.59375\n",
      "2018-12-07T16:48:07.231545: step 2456, loss 0.814975, acc 0.640625\n",
      "2018-12-07T16:48:08.665180: step 2457, loss 0.923746, acc 0.640625\n",
      "2018-12-07T16:48:09.734027: step 2458, loss 0.844895, acc 0.65625\n",
      "2018-12-07T16:48:10.785111: step 2459, loss 0.756471, acc 0.71875\n",
      "2018-12-07T16:48:11.889502: step 2460, loss 0.91276, acc 0.59375\n",
      "2018-12-07T16:48:13.060624: step 2461, loss 0.915354, acc 0.65625\n",
      "2018-12-07T16:48:14.190158: step 2462, loss 0.943452, acc 0.6875\n",
      "2018-12-07T16:48:15.286039: step 2463, loss 1.06055, acc 0.515625\n",
      "2018-12-07T16:48:16.350707: step 2464, loss 0.672918, acc 0.8125\n",
      "2018-12-07T16:48:17.537884: step 2465, loss 0.735652, acc 0.75\n",
      "2018-12-07T16:48:18.712500: step 2466, loss 0.948018, acc 0.671875\n",
      "2018-12-07T16:48:19.739754: step 2467, loss 0.93181, acc 0.625\n",
      "2018-12-07T16:48:20.950493: step 2468, loss 0.760063, acc 0.640625\n",
      "2018-12-07T16:48:21.995651: step 2469, loss 0.878486, acc 0.640625\n",
      "2018-12-07T16:48:23.186605: step 2470, loss 0.922493, acc 0.625\n",
      "2018-12-07T16:48:24.195721: step 2471, loss 0.953094, acc 0.609375\n",
      "2018-12-07T16:48:25.386222: step 2472, loss 1.09867, acc 0.5625\n",
      "2018-12-07T16:48:26.397022: step 2473, loss 0.891524, acc 0.703125\n",
      "2018-12-07T16:48:27.655267: step 2474, loss 0.747836, acc 0.625\n",
      "2018-12-07T16:48:28.717295: step 2475, loss 0.861282, acc 0.65625\n",
      "2018-12-07T16:48:30.138337: step 2476, loss 0.77754, acc 0.734375\n",
      "2018-12-07T16:48:31.236759: step 2477, loss 0.968949, acc 0.625\n",
      "2018-12-07T16:48:32.392882: step 2478, loss 0.861942, acc 0.625\n",
      "2018-12-07T16:48:33.629781: step 2479, loss 0.9309, acc 0.59375\n",
      "2018-12-07T16:48:34.646481: step 2480, loss 0.945529, acc 0.625\n",
      "2018-12-07T16:48:35.689760: step 2481, loss 0.836532, acc 0.703125\n",
      "2018-12-07T16:48:37.168787: step 2482, loss 0.85423, acc 0.65625\n",
      "2018-12-07T16:48:38.238380: step 2483, loss 0.762596, acc 0.6875\n",
      "2018-12-07T16:48:39.338010: step 2484, loss 1.02009, acc 0.671875\n",
      "2018-12-07T16:48:40.470113: step 2485, loss 0.785892, acc 0.65625\n",
      "2018-12-07T16:48:41.631416: step 2486, loss 0.797581, acc 0.75\n",
      "2018-12-07T16:48:42.775532: step 2487, loss 0.839866, acc 0.734375\n",
      "2018-12-07T16:48:43.817246: step 2488, loss 0.819697, acc 0.703125\n",
      "2018-12-07T16:48:44.923530: step 2489, loss 0.775151, acc 0.734375\n",
      "2018-12-07T16:48:46.130858: step 2490, loss 0.859497, acc 0.703125\n",
      "2018-12-07T16:48:47.213215: step 2491, loss 0.793305, acc 0.65625\n",
      "2018-12-07T16:48:48.278049: step 2492, loss 0.634784, acc 0.75\n",
      "2018-12-07T16:48:49.574130: step 2493, loss 0.754747, acc 0.75\n",
      "2018-12-07T16:48:50.813829: step 2494, loss 0.938068, acc 0.65625\n",
      "2018-12-07T16:48:51.903418: step 2495, loss 0.830817, acc 0.671875\n",
      "2018-12-07T16:48:53.126808: step 2496, loss 0.832182, acc 0.6875\n",
      "2018-12-07T16:48:54.261531: step 2497, loss 0.593148, acc 0.78125\n",
      "2018-12-07T16:48:55.363273: step 2498, loss 0.908757, acc 0.65625\n",
      "2018-12-07T16:48:56.414848: step 2499, loss 0.757169, acc 0.625\n",
      "2018-12-07T16:48:57.722642: step 2500, loss 0.858259, acc 0.6875\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T16:48:59.063058: step 2500, loss 0.729795, acc 0.736\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-2500\n",
      "\n",
      "2018-12-07T16:49:01.090574: step 2501, loss 0.897356, acc 0.6875\n",
      "2018-12-07T16:49:02.193406: step 2502, loss 0.664257, acc 0.75\n",
      "2018-12-07T16:49:03.442020: step 2503, loss 0.839135, acc 0.65625\n",
      "2018-12-07T16:49:04.809634: step 2504, loss 0.906067, acc 0.640625\n",
      "2018-12-07T16:49:06.028556: step 2505, loss 0.685021, acc 0.765625\n",
      "2018-12-07T16:49:07.485611: step 2506, loss 1.07818, acc 0.625\n",
      "2018-12-07T16:49:08.696780: step 2507, loss 0.723979, acc 0.671875\n",
      "2018-12-07T16:49:09.808373: step 2508, loss 0.781764, acc 0.6875\n",
      "2018-12-07T16:49:11.099918: step 2509, loss 0.988693, acc 0.546875\n",
      "2018-12-07T16:49:12.157930: step 2510, loss 0.76969, acc 0.75\n",
      "2018-12-07T16:49:13.293408: step 2511, loss 1.0062, acc 0.59375\n",
      "2018-12-07T16:49:14.625313: step 2512, loss 0.74946, acc 0.75\n",
      "2018-12-07T16:49:15.872163: step 2513, loss 0.905663, acc 0.6875\n",
      "2018-12-07T16:49:17.210631: step 2514, loss 0.916383, acc 0.671875\n",
      "2018-12-07T16:49:18.499934: step 2515, loss 0.730596, acc 0.78125\n",
      "2018-12-07T16:49:19.600188: step 2516, loss 0.930194, acc 0.671875\n",
      "2018-12-07T16:49:20.704984: step 2517, loss 0.825215, acc 0.703125\n",
      "2018-12-07T16:49:21.797080: step 2518, loss 0.835078, acc 0.6875\n",
      "2018-12-07T16:49:22.890012: step 2519, loss 0.766335, acc 0.703125\n",
      "2018-12-07T16:49:24.153975: step 2520, loss 0.810565, acc 0.71875\n",
      "2018-12-07T16:49:25.625153: step 2521, loss 0.809013, acc 0.640625\n",
      "2018-12-07T16:49:26.928806: step 2522, loss 0.910904, acc 0.65625\n",
      "2018-12-07T16:49:28.168168: step 2523, loss 0.946893, acc 0.671875\n",
      "2018-12-07T16:49:29.380217: step 2524, loss 0.822314, acc 0.640625\n",
      "2018-12-07T16:49:30.463807: step 2525, loss 0.79101, acc 0.6875\n",
      "2018-12-07T16:49:31.893197: step 2526, loss 0.673786, acc 0.765625\n",
      "2018-12-07T16:49:33.167021: step 2527, loss 0.832714, acc 0.59375\n",
      "2018-12-07T16:49:34.326579: step 2528, loss 0.773501, acc 0.703125\n",
      "2018-12-07T16:49:35.517844: step 2529, loss 0.811084, acc 0.6875\n",
      "2018-12-07T16:49:37.136631: step 2530, loss 0.793384, acc 0.703125\n",
      "2018-12-07T16:49:38.420914: step 2531, loss 0.704069, acc 0.765625\n",
      "2018-12-07T16:49:39.756693: step 2532, loss 0.727456, acc 0.734375\n",
      "2018-12-07T16:49:40.803151: step 2533, loss 0.730132, acc 0.765625\n",
      "2018-12-07T16:49:42.087026: step 2534, loss 0.848664, acc 0.703125\n",
      "2018-12-07T16:49:43.301133: step 2535, loss 0.725206, acc 0.765625\n",
      "2018-12-07T16:49:44.483790: step 2536, loss 0.819982, acc 0.765625\n",
      "2018-12-07T16:49:45.890354: step 2537, loss 0.888896, acc 0.609375\n",
      "2018-12-07T16:49:47.988146: step 2538, loss 0.907122, acc 0.65625\n",
      "2018-12-07T16:49:49.105039: step 2539, loss 0.931368, acc 0.65625\n",
      "2018-12-07T16:49:50.485447: step 2540, loss 0.97053, acc 0.640625\n",
      "2018-12-07T16:49:51.583460: step 2541, loss 0.773218, acc 0.703125\n",
      "2018-12-07T16:49:52.973428: step 2542, loss 0.835842, acc 0.609375\n",
      "2018-12-07T16:49:54.114219: step 2543, loss 0.781772, acc 0.671875\n",
      "2018-12-07T16:49:55.217205: step 2544, loss 0.840792, acc 0.671875\n",
      "2018-12-07T16:49:56.456784: step 2545, loss 0.847093, acc 0.625\n",
      "2018-12-07T16:49:58.057952: step 2546, loss 0.896204, acc 0.625\n",
      "2018-12-07T16:49:59.509702: step 2547, loss 0.832819, acc 0.703125\n",
      "2018-12-07T16:50:00.573004: step 2548, loss 0.765626, acc 0.703125\n",
      "2018-12-07T16:50:02.046866: step 2549, loss 0.652217, acc 0.75\n",
      "2018-12-07T16:50:03.342012: step 2550, loss 0.956721, acc 0.671875\n",
      "2018-12-07T16:50:04.417862: step 2551, loss 0.774684, acc 0.71875\n",
      "2018-12-07T16:50:05.659120: step 2552, loss 0.825622, acc 0.625\n",
      "2018-12-07T16:50:07.011217: step 2553, loss 0.897782, acc 0.703125\n",
      "2018-12-07T16:50:08.335463: step 2554, loss 0.986609, acc 0.65625\n",
      "2018-12-07T16:50:09.553357: step 2555, loss 1.11727, acc 0.5625\n",
      "2018-12-07T16:50:11.251749: step 2556, loss 0.744071, acc 0.71875\n",
      "2018-12-07T16:50:12.312778: step 2557, loss 0.855573, acc 0.640625\n",
      "2018-12-07T16:50:13.462227: step 2558, loss 0.804176, acc 0.734375\n",
      "2018-12-07T16:50:14.759737: step 2559, loss 0.804405, acc 0.734375\n",
      "2018-12-07T16:50:15.970498: step 2560, loss 0.735951, acc 0.734375\n",
      "2018-12-07T16:50:17.417729: step 2561, loss 0.921164, acc 0.640625\n",
      "2018-12-07T16:50:18.624472: step 2562, loss 0.891413, acc 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07T16:50:19.840419: step 2563, loss 0.916548, acc 0.625\n",
      "2018-12-07T16:50:21.430951: step 2564, loss 0.841812, acc 0.640625\n",
      "2018-12-07T16:50:22.912055: step 2565, loss 0.683803, acc 0.765625\n",
      "2018-12-07T16:50:24.007599: step 2566, loss 0.782131, acc 0.734375\n",
      "2018-12-07T16:50:25.271880: step 2567, loss 0.622446, acc 0.78125\n",
      "2018-12-07T16:50:26.540555: step 2568, loss 0.915511, acc 0.640625\n",
      "2018-12-07T16:50:27.710450: step 2569, loss 0.717539, acc 0.71875\n",
      "2018-12-07T16:50:28.781216: step 2570, loss 0.657431, acc 0.71875\n",
      "2018-12-07T16:50:29.893310: step 2571, loss 0.917002, acc 0.65625\n",
      "2018-12-07T16:50:31.384275: step 2572, loss 1.04278, acc 0.578125\n",
      "2018-12-07T16:50:32.645824: step 2573, loss 0.586889, acc 0.78125\n",
      "2018-12-07T16:50:33.738454: step 2574, loss 0.957835, acc 0.65625\n",
      "2018-12-07T16:50:34.973532: step 2575, loss 0.738994, acc 0.640625\n",
      "2018-12-07T16:50:36.117316: step 2576, loss 0.74922, acc 0.75\n",
      "2018-12-07T16:50:37.613302: step 2577, loss 0.812534, acc 0.71875\n",
      "2018-12-07T16:50:38.763652: step 2578, loss 0.850834, acc 0.671875\n",
      "2018-12-07T16:50:39.952437: step 2579, loss 0.958922, acc 0.609375\n",
      "2018-12-07T16:50:41.204630: step 2580, loss 0.926128, acc 0.640625\n",
      "2018-12-07T16:50:42.586347: step 2581, loss 0.693042, acc 0.71875\n",
      "2018-12-07T16:50:43.934564: step 2582, loss 0.771503, acc 0.734375\n",
      "2018-12-07T16:50:45.329548: step 2583, loss 0.817867, acc 0.671875\n",
      "2018-12-07T16:50:46.372220: step 2584, loss 0.882383, acc 0.671875\n",
      "2018-12-07T16:50:47.684587: step 2585, loss 0.816559, acc 0.671875\n",
      "2018-12-07T16:50:48.847278: step 2586, loss 0.821281, acc 0.65625\n",
      "2018-12-07T16:50:50.174105: step 2587, loss 0.842769, acc 0.703125\n",
      "2018-12-07T16:50:51.355081: step 2588, loss 0.758504, acc 0.734375\n",
      "2018-12-07T16:50:52.612964: step 2589, loss 0.914103, acc 0.59375\n",
      "2018-12-07T16:50:54.263359: step 2590, loss 0.803176, acc 0.703125\n",
      "2018-12-07T16:50:55.777107: step 2591, loss 1.04395, acc 0.65625\n",
      "2018-12-07T16:50:57.592081: step 2592, loss 0.870554, acc 0.609375\n",
      "2018-12-07T16:50:58.784576: step 2593, loss 0.862043, acc 0.609375\n",
      "2018-12-07T16:50:59.939879: step 2594, loss 0.739523, acc 0.6875\n",
      "2018-12-07T16:51:01.397483: step 2595, loss 0.823909, acc 0.671875\n",
      "2018-12-07T16:51:03.136767: step 2596, loss 0.763374, acc 0.734375\n",
      "2018-12-07T16:51:04.383048: step 2597, loss 0.837367, acc 0.640625\n",
      "2018-12-07T16:51:05.574875: step 2598, loss 0.803744, acc 0.625\n",
      "2018-12-07T16:51:06.928537: step 2599, loss 0.727045, acc 0.796875\n",
      "2018-12-07T16:51:08.398156: step 2600, loss 0.657995, acc 0.765625\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T16:51:09.636844: step 2600, loss 0.713453, acc 0.748\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-2600\n",
      "\n",
      "2018-12-07T16:51:11.744306: step 2601, loss 1.02607, acc 0.578125\n",
      "2018-12-07T16:51:13.201780: step 2602, loss 0.947839, acc 0.65625\n",
      "2018-12-07T16:51:14.907266: step 2603, loss 0.787129, acc 0.65625\n",
      "2018-12-07T16:51:16.321234: step 2604, loss 0.781594, acc 0.703125\n",
      "2018-12-07T16:51:17.861895: step 2605, loss 0.875543, acc 0.640625\n",
      "2018-12-07T16:51:19.293260: step 2606, loss 0.729082, acc 0.6875\n",
      "2018-12-07T16:51:20.911966: step 2607, loss 0.981862, acc 0.640625\n",
      "2018-12-07T16:51:22.383238: step 2608, loss 0.917576, acc 0.625\n",
      "2018-12-07T16:51:23.742684: step 2609, loss 0.863969, acc 0.609375\n",
      "2018-12-07T16:51:25.200575: step 2610, loss 0.714879, acc 0.71875\n",
      "2018-12-07T16:51:26.462988: step 2611, loss 0.666335, acc 0.703125\n",
      "2018-12-07T16:51:28.092098: step 2612, loss 0.728598, acc 0.71875\n",
      "2018-12-07T16:51:29.304353: step 2613, loss 0.808024, acc 0.65625\n",
      "2018-12-07T16:51:30.639162: step 2614, loss 0.909732, acc 0.578125\n",
      "2018-12-07T16:51:31.970645: step 2615, loss 1.02244, acc 0.609375\n",
      "2018-12-07T16:51:33.635871: step 2616, loss 0.761753, acc 0.75\n",
      "2018-12-07T16:51:34.799347: step 2617, loss 0.733237, acc 0.71875\n",
      "2018-12-07T16:51:36.208127: step 2618, loss 0.770143, acc 0.71875\n",
      "2018-12-07T16:51:37.870117: step 2619, loss 0.827842, acc 0.671875\n",
      "2018-12-07T16:51:39.313525: step 2620, loss 0.821132, acc 0.703125\n",
      "2018-12-07T16:51:40.633821: step 2621, loss 0.717812, acc 0.71875\n",
      "2018-12-07T16:51:42.207353: step 2622, loss 0.880736, acc 0.65625\n",
      "2018-12-07T16:51:44.098111: step 2623, loss 0.898093, acc 0.625\n",
      "2018-12-07T16:51:45.748497: step 2624, loss 0.664775, acc 0.75\n",
      "2018-12-07T16:51:47.386010: step 2625, loss 0.848963, acc 0.640625\n",
      "2018-12-07T16:51:48.734912: step 2626, loss 0.965664, acc 0.703125\n",
      "2018-12-07T16:51:50.533326: step 2627, loss 0.849158, acc 0.640625\n",
      "2018-12-07T16:51:52.242930: step 2628, loss 0.785987, acc 0.6875\n",
      "2018-12-07T16:51:53.827567: step 2629, loss 0.973587, acc 0.65625\n",
      "2018-12-07T16:51:54.995708: step 2630, loss 0.852669, acc 0.6875\n",
      "2018-12-07T16:51:56.244089: step 2631, loss 0.979508, acc 0.53125\n",
      "2018-12-07T16:51:57.860133: step 2632, loss 0.835846, acc 0.6875\n",
      "2018-12-07T16:51:59.700914: step 2633, loss 0.823359, acc 0.640625\n",
      "2018-12-07T16:52:01.100216: step 2634, loss 0.609, acc 0.796875\n",
      "2018-12-07T16:52:02.410024: step 2635, loss 0.995064, acc 0.515625\n",
      "2018-12-07T16:52:03.819520: step 2636, loss 0.755666, acc 0.6875\n",
      "2018-12-07T16:52:05.290186: step 2637, loss 0.892446, acc 0.609375\n",
      "2018-12-07T16:52:06.518220: step 2638, loss 1.01446, acc 0.578125\n",
      "2018-12-07T16:52:08.153446: step 2639, loss 0.866604, acc 0.671875\n",
      "2018-12-07T16:52:09.539064: step 2640, loss 0.807798, acc 0.703125\n",
      "2018-12-07T16:52:11.103328: step 2641, loss 0.811198, acc 0.6875\n",
      "2018-12-07T16:52:12.450785: step 2642, loss 0.929201, acc 0.625\n",
      "2018-12-07T16:52:14.356364: step 2643, loss 0.65798, acc 0.8125\n",
      "2018-12-07T16:52:15.996508: step 2644, loss 0.92408, acc 0.5625\n",
      "2018-12-07T16:52:17.541019: step 2645, loss 0.757754, acc 0.6875\n",
      "2018-12-07T16:52:18.991003: step 2646, loss 0.667322, acc 0.71875\n",
      "2018-12-07T16:52:20.383617: step 2647, loss 0.794146, acc 0.6875\n",
      "2018-12-07T16:52:21.719035: step 2648, loss 0.913308, acc 0.625\n",
      "2018-12-07T16:52:23.281994: step 2649, loss 0.848404, acc 0.75\n",
      "2018-12-07T16:52:25.147149: step 2650, loss 0.896167, acc 0.703125\n",
      "2018-12-07T16:52:26.611770: step 2651, loss 1.01141, acc 0.609375\n",
      "2018-12-07T16:52:28.412996: step 2652, loss 0.660329, acc 0.78125\n",
      "2018-12-07T16:52:30.250498: step 2653, loss 0.767152, acc 0.71875\n",
      "2018-12-07T16:52:31.780807: step 2654, loss 1.00941, acc 0.65625\n",
      "2018-12-07T16:52:33.230408: step 2655, loss 0.691673, acc 0.734375\n",
      "2018-12-07T16:52:34.891499: step 2656, loss 0.908789, acc 0.65625\n",
      "2018-12-07T16:52:36.999930: step 2657, loss 0.985138, acc 0.578125\n",
      "2018-12-07T16:52:38.839634: step 2658, loss 0.763077, acc 0.734375\n",
      "2018-12-07T16:52:40.449344: step 2659, loss 0.645847, acc 0.734375\n",
      "2018-12-07T16:52:42.217990: step 2660, loss 0.849925, acc 0.640625\n",
      "2018-12-07T16:52:43.942656: step 2661, loss 0.765591, acc 0.703125\n",
      "2018-12-07T16:52:45.538265: step 2662, loss 0.904609, acc 0.640625\n",
      "2018-12-07T16:52:47.559447: step 2663, loss 0.821386, acc 0.6875\n",
      "2018-12-07T16:52:49.305333: step 2664, loss 0.792159, acc 0.65625\n",
      "2018-12-07T16:52:51.000278: step 2665, loss 0.870276, acc 0.5625\n",
      "2018-12-07T16:52:52.456177: step 2666, loss 0.932985, acc 0.6875\n",
      "2018-12-07T16:52:54.125061: step 2667, loss 0.631582, acc 0.75\n",
      "2018-12-07T16:52:55.829842: step 2668, loss 0.775168, acc 0.71875\n",
      "2018-12-07T16:52:57.613590: step 2669, loss 0.893214, acc 0.640625\n",
      "2018-12-07T16:52:59.236940: step 2670, loss 0.984435, acc 0.59375\n",
      "2018-12-07T16:53:01.072279: step 2671, loss 0.880718, acc 0.65625\n",
      "2018-12-07T16:53:03.196284: step 2672, loss 0.779932, acc 0.703125\n",
      "2018-12-07T16:53:04.986794: step 2673, loss 0.906231, acc 0.6875\n",
      "2018-12-07T16:53:06.491249: step 2674, loss 0.766792, acc 0.671875\n",
      "2018-12-07T16:53:08.070895: step 2675, loss 0.730648, acc 0.703125\n",
      "2018-12-07T16:53:09.597635: step 2676, loss 0.93202, acc 0.6875\n",
      "2018-12-07T16:53:11.489130: step 2677, loss 0.868924, acc 0.671875\n",
      "2018-12-07T16:53:13.036573: step 2678, loss 0.938778, acc 0.6875\n",
      "2018-12-07T16:53:14.997944: step 2679, loss 0.935914, acc 0.625\n",
      "2018-12-07T16:53:16.618444: step 2680, loss 0.945839, acc 0.609375\n",
      "2018-12-07T16:53:18.166307: step 2681, loss 0.9143, acc 0.609375\n",
      "2018-12-07T16:53:19.704096: step 2682, loss 0.875428, acc 0.640625\n",
      "2018-12-07T16:53:21.306073: step 2683, loss 0.866632, acc 0.71875\n",
      "2018-12-07T16:53:23.239769: step 2684, loss 0.850217, acc 0.703125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07T16:53:25.276496: step 2685, loss 0.759551, acc 0.71875\n",
      "2018-12-07T16:53:26.878964: step 2686, loss 0.937296, acc 0.703125\n",
      "2018-12-07T16:53:28.482541: step 2687, loss 0.911673, acc 0.625\n",
      "2018-12-07T16:53:30.247400: step 2688, loss 0.722161, acc 0.671875\n",
      "2018-12-07T16:53:31.925606: step 2689, loss 0.796599, acc 0.640625\n",
      "2018-12-07T16:53:33.431249: step 2690, loss 0.874449, acc 0.609375\n",
      "2018-12-07T16:53:35.589481: step 2691, loss 0.733009, acc 0.734375\n",
      "2018-12-07T16:53:37.635773: step 2692, loss 0.674776, acc 0.734375\n",
      "2018-12-07T16:53:39.436739: step 2693, loss 0.7496, acc 0.65625\n",
      "2018-12-07T16:53:41.423009: step 2694, loss 0.95341, acc 0.609375\n",
      "2018-12-07T16:53:43.637852: step 2695, loss 0.805891, acc 0.71875\n",
      "2018-12-07T16:53:45.141087: step 2696, loss 0.679832, acc 0.765625\n",
      "2018-12-07T16:53:46.928699: step 2697, loss 0.668003, acc 0.71875\n",
      "2018-12-07T16:53:48.628673: step 2698, loss 0.825438, acc 0.640625\n",
      "2018-12-07T16:53:50.203403: step 2699, loss 0.943319, acc 0.609375\n",
      "2018-12-07T16:53:51.966487: step 2700, loss 0.74367, acc 0.71875\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T16:53:53.203180: step 2700, loss 0.695847, acc 0.768\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-2700\n",
      "\n",
      "2018-12-07T16:53:55.351013: step 2701, loss 0.972961, acc 0.640625\n",
      "2018-12-07T16:53:57.186950: step 2702, loss 0.651668, acc 0.765625\n",
      "2018-12-07T16:53:58.998413: step 2703, loss 0.884239, acc 0.71875\n",
      "2018-12-07T16:54:00.934617: step 2704, loss 1.04503, acc 0.625\n",
      "2018-12-07T16:54:02.397928: step 2705, loss 0.76385, acc 0.65625\n",
      "2018-12-07T16:54:04.570382: step 2706, loss 0.879414, acc 0.6875\n",
      "2018-12-07T16:54:06.231844: step 2707, loss 0.744712, acc 0.703125\n",
      "2018-12-07T16:54:08.146994: step 2708, loss 0.865808, acc 0.59375\n",
      "2018-12-07T16:54:09.285813: step 2709, loss 0.916492, acc 0.652174\n",
      "2018-12-07T16:54:10.283777: step 2710, loss 0.74253, acc 0.703125\n",
      "2018-12-07T16:54:11.315966: step 2711, loss 0.811671, acc 0.625\n",
      "2018-12-07T16:54:12.402229: step 2712, loss 0.89589, acc 0.65625\n",
      "2018-12-07T16:54:13.421388: step 2713, loss 0.829186, acc 0.703125\n",
      "2018-12-07T16:54:14.431151: step 2714, loss 0.676125, acc 0.734375\n",
      "2018-12-07T16:54:15.569371: step 2715, loss 0.826801, acc 0.6875\n",
      "2018-12-07T16:54:16.870117: step 2716, loss 0.812268, acc 0.75\n",
      "2018-12-07T16:54:18.152434: step 2717, loss 0.785598, acc 0.75\n",
      "2018-12-07T16:54:19.183926: step 2718, loss 0.990229, acc 0.609375\n",
      "2018-12-07T16:54:20.235370: step 2719, loss 0.933158, acc 0.65625\n",
      "2018-12-07T16:54:21.597801: step 2720, loss 0.738347, acc 0.6875\n",
      "2018-12-07T16:54:22.639680: step 2721, loss 0.667027, acc 0.78125\n",
      "2018-12-07T16:54:23.668107: step 2722, loss 0.71215, acc 0.75\n",
      "2018-12-07T16:54:24.673403: step 2723, loss 0.737706, acc 0.671875\n",
      "2018-12-07T16:54:25.675950: step 2724, loss 0.816067, acc 0.703125\n",
      "2018-12-07T16:54:26.776457: step 2725, loss 0.538981, acc 0.84375\n",
      "2018-12-07T16:54:27.832944: step 2726, loss 0.74536, acc 0.671875\n",
      "2018-12-07T16:54:28.808851: step 2727, loss 0.782739, acc 0.625\n",
      "2018-12-07T16:54:29.831655: step 2728, loss 0.578255, acc 0.765625\n",
      "2018-12-07T16:54:30.972060: step 2729, loss 0.82467, acc 0.6875\n",
      "2018-12-07T16:54:31.987376: step 2730, loss 0.882968, acc 0.609375\n",
      "2018-12-07T16:54:33.183665: step 2731, loss 0.730038, acc 0.75\n",
      "2018-12-07T16:54:34.206527: step 2732, loss 0.655401, acc 0.75\n",
      "2018-12-07T16:54:35.534605: step 2733, loss 0.832411, acc 0.6875\n",
      "2018-12-07T16:54:36.552846: step 2734, loss 0.879499, acc 0.625\n",
      "2018-12-07T16:54:37.636888: step 2735, loss 0.689717, acc 0.703125\n",
      "2018-12-07T16:54:38.740106: step 2736, loss 0.639737, acc 0.703125\n",
      "2018-12-07T16:54:39.756522: step 2737, loss 0.802634, acc 0.671875\n",
      "2018-12-07T16:54:40.757131: step 2738, loss 0.646058, acc 0.765625\n",
      "2018-12-07T16:54:41.749834: step 2739, loss 0.709747, acc 0.71875\n",
      "2018-12-07T16:54:42.898332: step 2740, loss 0.779552, acc 0.6875\n",
      "2018-12-07T16:54:44.064639: step 2741, loss 0.8455, acc 0.625\n",
      "2018-12-07T16:54:45.138873: step 2742, loss 0.896138, acc 0.65625\n",
      "2018-12-07T16:54:46.117890: step 2743, loss 1.02796, acc 0.609375\n",
      "2018-12-07T16:54:47.095062: step 2744, loss 0.713311, acc 0.703125\n",
      "2018-12-07T16:54:48.289236: step 2745, loss 0.924839, acc 0.625\n",
      "2018-12-07T16:54:49.365028: step 2746, loss 0.853615, acc 0.671875\n",
      "2018-12-07T16:54:50.418095: step 2747, loss 0.540518, acc 0.828125\n",
      "2018-12-07T16:54:51.421404: step 2748, loss 0.775951, acc 0.65625\n",
      "2018-12-07T16:54:52.470751: step 2749, loss 0.714979, acc 0.6875\n",
      "2018-12-07T16:54:53.635090: step 2750, loss 0.708689, acc 0.703125\n",
      "2018-12-07T16:54:54.677333: step 2751, loss 0.792106, acc 0.671875\n",
      "2018-12-07T16:54:55.705161: step 2752, loss 0.798619, acc 0.765625\n",
      "2018-12-07T16:54:56.713090: step 2753, loss 0.907164, acc 0.640625\n",
      "2018-12-07T16:54:58.064738: step 2754, loss 0.874287, acc 0.671875\n",
      "2018-12-07T16:54:59.256864: step 2755, loss 0.781909, acc 0.71875\n",
      "2018-12-07T16:55:00.318519: step 2756, loss 0.554693, acc 0.84375\n",
      "2018-12-07T16:55:01.442929: step 2757, loss 0.798055, acc 0.6875\n",
      "2018-12-07T16:55:02.619947: step 2758, loss 0.797836, acc 0.703125\n",
      "2018-12-07T16:55:03.601702: step 2759, loss 0.649848, acc 0.734375\n",
      "2018-12-07T16:55:04.600669: step 2760, loss 0.719925, acc 0.734375\n",
      "2018-12-07T16:55:05.676588: step 2761, loss 0.935014, acc 0.6875\n",
      "2018-12-07T16:55:06.724686: step 2762, loss 0.881167, acc 0.640625\n",
      "2018-12-07T16:55:08.057177: step 2763, loss 0.846792, acc 0.59375\n",
      "2018-12-07T16:55:09.206383: step 2764, loss 0.663529, acc 0.765625\n",
      "2018-12-07T16:55:10.257717: step 2765, loss 0.777236, acc 0.6875\n",
      "2018-12-07T16:55:11.447522: step 2766, loss 0.675309, acc 0.75\n",
      "2018-12-07T16:55:12.503704: step 2767, loss 0.680358, acc 0.734375\n",
      "2018-12-07T16:55:13.669215: step 2768, loss 0.853691, acc 0.703125\n",
      "2018-12-07T16:55:15.028094: step 2769, loss 0.701162, acc 0.75\n",
      "2018-12-07T16:55:16.145481: step 2770, loss 0.844785, acc 0.65625\n",
      "2018-12-07T16:55:17.301164: step 2771, loss 0.79475, acc 0.71875\n",
      "2018-12-07T16:55:18.349161: step 2772, loss 0.618885, acc 0.796875\n",
      "2018-12-07T16:55:19.430488: step 2773, loss 0.960415, acc 0.609375\n",
      "2018-12-07T16:55:20.595859: step 2774, loss 0.682608, acc 0.75\n",
      "2018-12-07T16:55:21.721795: step 2775, loss 0.863694, acc 0.625\n",
      "2018-12-07T16:55:22.712289: step 2776, loss 0.815732, acc 0.65625\n",
      "2018-12-07T16:55:23.729751: step 2777, loss 0.87739, acc 0.640625\n",
      "2018-12-07T16:55:24.733154: step 2778, loss 0.804116, acc 0.765625\n",
      "2018-12-07T16:55:25.780008: step 2779, loss 0.763993, acc 0.703125\n",
      "2018-12-07T16:55:26.799618: step 2780, loss 0.916052, acc 0.671875\n",
      "2018-12-07T16:55:27.940023: step 2781, loss 0.794775, acc 0.671875\n",
      "2018-12-07T16:55:28.972184: step 2782, loss 0.753226, acc 0.640625\n",
      "2018-12-07T16:55:29.937213: step 2783, loss 0.818536, acc 0.734375\n",
      "2018-12-07T16:55:30.957821: step 2784, loss 0.796944, acc 0.671875\n",
      "2018-12-07T16:55:32.056465: step 2785, loss 0.691938, acc 0.75\n",
      "2018-12-07T16:55:33.079472: step 2786, loss 0.789378, acc 0.640625\n",
      "2018-12-07T16:55:34.107820: step 2787, loss 0.601033, acc 0.765625\n",
      "2018-12-07T16:55:35.130897: step 2788, loss 0.687397, acc 0.734375\n",
      "2018-12-07T16:55:36.142513: step 2789, loss 0.702358, acc 0.71875\n",
      "2018-12-07T16:55:37.189115: step 2790, loss 0.845076, acc 0.6875\n",
      "2018-12-07T16:55:38.552688: step 2791, loss 0.664189, acc 0.828125\n",
      "2018-12-07T16:55:39.556489: step 2792, loss 0.626078, acc 0.796875\n",
      "2018-12-07T16:55:40.700630: step 2793, loss 0.784994, acc 0.6875\n",
      "2018-12-07T16:55:41.895953: step 2794, loss 0.610906, acc 0.796875\n",
      "2018-12-07T16:55:42.893423: step 2795, loss 0.713165, acc 0.71875\n",
      "2018-12-07T16:55:44.023575: step 2796, loss 0.819973, acc 0.671875\n",
      "2018-12-07T16:55:45.045103: step 2797, loss 0.695527, acc 0.71875\n",
      "2018-12-07T16:55:46.209354: step 2798, loss 0.864145, acc 0.671875\n",
      "2018-12-07T16:55:47.376774: step 2799, loss 0.750371, acc 0.6875\n",
      "2018-12-07T16:55:48.536048: step 2800, loss 0.587649, acc 0.78125\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T16:55:49.761769: step 2800, loss 0.710012, acc 0.756\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-2800\n",
      "\n",
      "2018-12-07T16:55:51.566226: step 2801, loss 0.718747, acc 0.75\n",
      "2018-12-07T16:55:52.716913: step 2802, loss 0.76922, acc 0.734375\n",
      "2018-12-07T16:55:53.918189: step 2803, loss 0.701346, acc 0.671875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07T16:55:54.939773: step 2804, loss 0.859866, acc 0.65625\n",
      "2018-12-07T16:55:55.978542: step 2805, loss 0.778303, acc 0.71875\n",
      "2018-12-07T16:55:57.198068: step 2806, loss 0.632392, acc 0.734375\n",
      "2018-12-07T16:55:58.429774: step 2807, loss 0.837841, acc 0.71875\n",
      "2018-12-07T16:55:59.497478: step 2808, loss 0.721794, acc 0.65625\n",
      "2018-12-07T16:56:00.522577: step 2809, loss 0.787937, acc 0.6875\n",
      "2018-12-07T16:56:01.554707: step 2810, loss 0.692985, acc 0.703125\n",
      "2018-12-07T16:56:02.700022: step 2811, loss 0.778164, acc 0.6875\n",
      "2018-12-07T16:56:03.894924: step 2812, loss 0.786674, acc 0.640625\n",
      "2018-12-07T16:56:05.006522: step 2813, loss 0.732558, acc 0.703125\n",
      "2018-12-07T16:56:06.039114: step 2814, loss 0.809223, acc 0.671875\n",
      "2018-12-07T16:56:07.067460: step 2815, loss 0.739735, acc 0.734375\n",
      "2018-12-07T16:56:08.300649: step 2816, loss 0.726884, acc 0.703125\n",
      "2018-12-07T16:56:09.392502: step 2817, loss 0.507118, acc 0.828125\n",
      "2018-12-07T16:56:10.369824: step 2818, loss 0.631586, acc 0.8125\n",
      "2018-12-07T16:56:11.380230: step 2819, loss 0.813815, acc 0.71875\n",
      "2018-12-07T16:56:12.359912: step 2820, loss 0.814289, acc 0.703125\n",
      "2018-12-07T16:56:13.431108: step 2821, loss 0.720806, acc 0.703125\n",
      "2018-12-07T16:56:14.756007: step 2822, loss 0.742877, acc 0.734375\n",
      "2018-12-07T16:56:15.800621: step 2823, loss 0.674838, acc 0.71875\n",
      "2018-12-07T16:56:16.835936: step 2824, loss 0.855829, acc 0.703125\n",
      "2018-12-07T16:56:18.005269: step 2825, loss 0.781613, acc 0.734375\n",
      "2018-12-07T16:56:19.245360: step 2826, loss 0.858046, acc 0.671875\n",
      "2018-12-07T16:56:20.303294: step 2827, loss 0.688694, acc 0.765625\n",
      "2018-12-07T16:56:21.462815: step 2828, loss 0.921597, acc 0.65625\n",
      "2018-12-07T16:56:22.504270: step 2829, loss 0.716271, acc 0.765625\n",
      "2018-12-07T16:56:23.691622: step 2830, loss 0.992363, acc 0.59375\n",
      "2018-12-07T16:56:24.732889: step 2831, loss 0.853737, acc 0.578125\n",
      "2018-12-07T16:56:25.837987: step 2832, loss 0.806953, acc 0.65625\n",
      "2018-12-07T16:56:26.923474: step 2833, loss 0.749986, acc 0.6875\n",
      "2018-12-07T16:56:28.242078: step 2834, loss 0.853183, acc 0.625\n",
      "2018-12-07T16:56:29.347582: step 2835, loss 0.816984, acc 0.6875\n",
      "2018-12-07T16:56:30.502508: step 2836, loss 0.735992, acc 0.65625\n",
      "2018-12-07T16:56:31.594142: step 2837, loss 0.686246, acc 0.703125\n",
      "2018-12-07T16:56:32.601253: step 2838, loss 0.695368, acc 0.765625\n",
      "2018-12-07T16:56:33.642333: step 2839, loss 0.756435, acc 0.75\n",
      "2018-12-07T16:56:34.665608: step 2840, loss 0.889324, acc 0.6875\n",
      "2018-12-07T16:56:35.854492: step 2841, loss 0.766899, acc 0.6875\n",
      "2018-12-07T16:56:36.900916: step 2842, loss 0.625868, acc 0.765625\n",
      "2018-12-07T16:56:38.066445: step 2843, loss 0.842144, acc 0.640625\n",
      "2018-12-07T16:56:39.107886: step 2844, loss 0.91509, acc 0.640625\n",
      "2018-12-07T16:56:40.170180: step 2845, loss 0.668402, acc 0.71875\n",
      "2018-12-07T16:56:41.405803: step 2846, loss 0.81884, acc 0.65625\n",
      "2018-12-07T16:56:42.676633: step 2847, loss 0.462405, acc 0.890625\n",
      "2018-12-07T16:56:43.717710: step 2848, loss 0.771891, acc 0.765625\n",
      "2018-12-07T16:56:44.895468: step 2849, loss 0.938276, acc 0.625\n",
      "2018-12-07T16:56:46.053239: step 2850, loss 1.00122, acc 0.59375\n",
      "2018-12-07T16:56:47.421221: step 2851, loss 0.720693, acc 0.75\n",
      "2018-12-07T16:56:48.507797: step 2852, loss 0.797418, acc 0.671875\n",
      "2018-12-07T16:56:49.569353: step 2853, loss 0.692008, acc 0.703125\n",
      "2018-12-07T16:56:50.581665: step 2854, loss 0.66349, acc 0.78125\n",
      "2018-12-07T16:56:51.773138: step 2855, loss 0.810904, acc 0.703125\n",
      "2018-12-07T16:56:52.839539: step 2856, loss 0.778387, acc 0.703125\n",
      "2018-12-07T16:56:53.932120: step 2857, loss 0.69499, acc 0.65625\n",
      "2018-12-07T16:56:54.933571: step 2858, loss 0.705257, acc 0.71875\n",
      "2018-12-07T16:56:56.061609: step 2859, loss 0.928658, acc 0.640625\n",
      "2018-12-07T16:56:57.327934: step 2860, loss 0.801224, acc 0.71875\n",
      "2018-12-07T16:56:58.941894: step 2861, loss 0.567122, acc 0.8125\n",
      "2018-12-07T16:57:00.077617: step 2862, loss 0.808494, acc 0.65625\n",
      "2018-12-07T16:57:01.113358: step 2863, loss 0.81422, acc 0.640625\n",
      "2018-12-07T16:57:02.185542: step 2864, loss 0.805249, acc 0.671875\n",
      "2018-12-07T16:57:03.272799: step 2865, loss 0.867829, acc 0.609375\n",
      "2018-12-07T16:57:04.459889: step 2866, loss 0.902609, acc 0.671875\n",
      "2018-12-07T16:57:05.613904: step 2867, loss 0.926694, acc 0.65625\n",
      "2018-12-07T16:57:06.661671: step 2868, loss 0.67727, acc 0.734375\n",
      "2018-12-07T16:57:07.958551: step 2869, loss 0.631566, acc 0.734375\n",
      "2018-12-07T16:57:09.178760: step 2870, loss 0.604027, acc 0.796875\n",
      "2018-12-07T16:57:10.231277: step 2871, loss 0.765708, acc 0.75\n",
      "2018-12-07T16:57:11.563012: step 2872, loss 0.920586, acc 0.65625\n",
      "2018-12-07T16:57:12.630744: step 2873, loss 0.666908, acc 0.71875\n",
      "2018-12-07T16:57:13.901548: step 2874, loss 0.733693, acc 0.671875\n",
      "2018-12-07T16:57:15.390297: step 2875, loss 0.967008, acc 0.671875\n",
      "2018-12-07T16:57:16.393149: step 2876, loss 0.750912, acc 0.71875\n",
      "2018-12-07T16:57:17.534808: step 2877, loss 0.677147, acc 0.796875\n",
      "2018-12-07T16:57:18.830249: step 2878, loss 0.837919, acc 0.609375\n",
      "2018-12-07T16:57:20.003805: step 2879, loss 0.8341, acc 0.65625\n",
      "2018-12-07T16:57:21.109492: step 2880, loss 0.66558, acc 0.71875\n",
      "2018-12-07T16:57:22.232223: step 2881, loss 0.756644, acc 0.75\n",
      "2018-12-07T16:57:23.378971: step 2882, loss 0.599614, acc 0.796875\n",
      "2018-12-07T16:57:24.669105: step 2883, loss 0.554706, acc 0.828125\n",
      "2018-12-07T16:57:25.957778: step 2884, loss 0.92411, acc 0.65625\n",
      "2018-12-07T16:57:27.024679: step 2885, loss 0.851989, acc 0.65625\n",
      "2018-12-07T16:57:28.206361: step 2886, loss 0.737147, acc 0.6875\n",
      "2018-12-07T16:57:29.466439: step 2887, loss 0.715824, acc 0.734375\n",
      "2018-12-07T16:57:30.493264: step 2888, loss 0.742318, acc 0.65625\n",
      "2018-12-07T16:57:31.722434: step 2889, loss 0.808615, acc 0.703125\n",
      "2018-12-07T16:57:33.051022: step 2890, loss 0.691246, acc 0.765625\n",
      "2018-12-07T16:57:34.079253: step 2891, loss 0.735941, acc 0.71875\n",
      "2018-12-07T16:57:35.272158: step 2892, loss 0.81958, acc 0.703125\n",
      "2018-12-07T16:57:36.362799: step 2893, loss 0.567567, acc 0.796875\n",
      "2018-12-07T16:57:37.482522: step 2894, loss 0.89367, acc 0.640625\n",
      "2018-12-07T16:57:38.575754: step 2895, loss 0.7236, acc 0.734375\n",
      "2018-12-07T16:57:39.907391: step 2896, loss 0.697758, acc 0.765625\n",
      "2018-12-07T16:57:41.208016: step 2897, loss 1.04176, acc 0.609375\n",
      "2018-12-07T16:57:42.231673: step 2898, loss 0.705629, acc 0.734375\n",
      "2018-12-07T16:57:43.339751: step 2899, loss 0.729312, acc 0.75\n",
      "2018-12-07T16:57:44.364911: step 2900, loss 0.609485, acc 0.796875\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T16:57:45.595621: step 2900, loss 0.656363, acc 0.78\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-2900\n",
      "\n",
      "2018-12-07T16:57:47.386048: step 2901, loss 0.77375, acc 0.625\n",
      "2018-12-07T16:57:49.094696: step 2902, loss 0.850103, acc 0.6875\n",
      "2018-12-07T16:57:50.352029: step 2903, loss 0.825939, acc 0.625\n",
      "2018-12-07T16:57:51.736053: step 2904, loss 0.801558, acc 0.65625\n",
      "2018-12-07T16:57:52.827763: step 2905, loss 0.768328, acc 0.6875\n",
      "2018-12-07T16:57:53.953203: step 2906, loss 0.624456, acc 0.75\n",
      "2018-12-07T16:57:55.230388: step 2907, loss 0.805416, acc 0.71875\n",
      "2018-12-07T16:57:56.309489: step 2908, loss 0.787727, acc 0.6875\n",
      "2018-12-07T16:57:57.673295: step 2909, loss 0.744453, acc 0.65625\n",
      "2018-12-07T16:57:58.941306: step 2910, loss 0.745532, acc 0.75\n",
      "2018-12-07T16:58:00.156084: step 2911, loss 0.608012, acc 0.78125\n",
      "2018-12-07T16:58:01.612480: step 2912, loss 0.61369, acc 0.796875\n",
      "2018-12-07T16:58:02.873972: step 2913, loss 0.750779, acc 0.75\n",
      "2018-12-07T16:58:04.323640: step 2914, loss 0.69563, acc 0.78125\n",
      "2018-12-07T16:58:05.501265: step 2915, loss 0.962266, acc 0.640625\n",
      "2018-12-07T16:58:07.070105: step 2916, loss 0.689703, acc 0.75\n",
      "2018-12-07T16:58:08.261508: step 2917, loss 0.793274, acc 0.71875\n",
      "2018-12-07T16:58:09.592555: step 2918, loss 0.799528, acc 0.6875\n",
      "2018-12-07T16:58:10.928528: step 2919, loss 0.837441, acc 0.6875\n",
      "2018-12-07T16:58:12.649116: step 2920, loss 0.696541, acc 0.75\n",
      "2018-12-07T16:58:13.785391: step 2921, loss 0.92071, acc 0.640625\n",
      "2018-12-07T16:58:15.241709: step 2922, loss 0.864696, acc 0.671875\n",
      "2018-12-07T16:58:16.731925: step 2923, loss 0.746898, acc 0.703125\n",
      "2018-12-07T16:58:17.989398: step 2924, loss 0.761312, acc 0.71875\n",
      "2018-12-07T16:58:19.105098: step 2925, loss 0.802239, acc 0.609375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07T16:58:20.497019: step 2926, loss 0.656326, acc 0.78125\n",
      "2018-12-07T16:58:21.615132: step 2927, loss 0.814142, acc 0.6875\n",
      "2018-12-07T16:58:22.890307: step 2928, loss 0.815773, acc 0.734375\n",
      "2018-12-07T16:58:24.248802: step 2929, loss 0.742874, acc 0.75\n",
      "2018-12-07T16:58:25.457890: step 2930, loss 0.580583, acc 0.78125\n",
      "2018-12-07T16:58:26.581952: step 2931, loss 0.712463, acc 0.734375\n",
      "2018-12-07T16:58:28.296563: step 2932, loss 0.663111, acc 0.6875\n",
      "2018-12-07T16:58:29.612428: step 2933, loss 0.841725, acc 0.671875\n",
      "2018-12-07T16:58:31.000377: step 2934, loss 0.783506, acc 0.703125\n",
      "2018-12-07T16:58:32.188214: step 2935, loss 0.812041, acc 0.65625\n",
      "2018-12-07T16:58:33.765892: step 2936, loss 0.726751, acc 0.703125\n",
      "2018-12-07T16:58:34.789561: step 2937, loss 0.80824, acc 0.71875\n",
      "2018-12-07T16:58:35.969461: step 2938, loss 0.782792, acc 0.671875\n",
      "2018-12-07T16:58:37.333487: step 2939, loss 0.86935, acc 0.671875\n",
      "2018-12-07T16:58:38.482411: step 2940, loss 0.7508, acc 0.71875\n",
      "2018-12-07T16:58:40.163969: step 2941, loss 0.707318, acc 0.71875\n",
      "2018-12-07T16:58:41.425584: step 2942, loss 0.707229, acc 0.703125\n",
      "2018-12-07T16:58:42.774056: step 2943, loss 0.686164, acc 0.75\n",
      "2018-12-07T16:58:44.013363: step 2944, loss 0.789625, acc 0.75\n",
      "2018-12-07T16:58:45.276140: step 2945, loss 0.707947, acc 0.671875\n",
      "2018-12-07T16:58:46.408001: step 2946, loss 0.643103, acc 0.796875\n",
      "2018-12-07T16:58:47.739173: step 2947, loss 0.568704, acc 0.78125\n",
      "2018-12-07T16:58:48.806557: step 2948, loss 0.953658, acc 0.734375\n",
      "2018-12-07T16:58:50.064283: step 2949, loss 0.772375, acc 0.6875\n",
      "2018-12-07T16:58:51.319842: step 2950, loss 0.541756, acc 0.84375\n",
      "2018-12-07T16:58:52.446565: step 2951, loss 0.812077, acc 0.703125\n",
      "2018-12-07T16:58:53.540283: step 2952, loss 0.713269, acc 0.671875\n",
      "2018-12-07T16:58:54.827187: step 2953, loss 0.930575, acc 0.65625\n",
      "2018-12-07T16:58:55.968582: step 2954, loss 0.735746, acc 0.71875\n",
      "2018-12-07T16:58:57.291213: step 2955, loss 0.810183, acc 0.65625\n",
      "2018-12-07T16:58:58.579066: step 2956, loss 0.745652, acc 0.78125\n",
      "2018-12-07T16:58:59.999202: step 2957, loss 0.601057, acc 0.828125\n",
      "2018-12-07T16:59:01.268350: step 2958, loss 0.863657, acc 0.6875\n",
      "2018-12-07T16:59:02.602634: step 2959, loss 0.71142, acc 0.6875\n",
      "2018-12-07T16:59:03.938502: step 2960, loss 0.694699, acc 0.75\n",
      "2018-12-07T16:59:05.299443: step 2961, loss 0.681171, acc 0.6875\n",
      "2018-12-07T16:59:06.730338: step 2962, loss 0.870796, acc 0.6875\n",
      "2018-12-07T16:59:08.240828: step 2963, loss 0.877424, acc 0.640625\n",
      "2018-12-07T16:59:09.610021: step 2964, loss 0.721897, acc 0.78125\n",
      "2018-12-07T16:59:11.040709: step 2965, loss 0.816047, acc 0.65625\n",
      "2018-12-07T16:59:12.196039: step 2966, loss 0.613215, acc 0.78125\n",
      "2018-12-07T16:59:13.553131: step 2967, loss 0.762523, acc 0.625\n",
      "2018-12-07T16:59:14.654803: step 2968, loss 0.782832, acc 0.671875\n",
      "2018-12-07T16:59:15.773939: step 2969, loss 0.632667, acc 0.765625\n",
      "2018-12-07T16:59:17.154521: step 2970, loss 0.738704, acc 0.71875\n",
      "2018-12-07T16:59:18.510328: step 2971, loss 0.68726, acc 0.71875\n",
      "2018-12-07T16:59:19.787096: step 2972, loss 0.69674, acc 0.734375\n",
      "2018-12-07T16:59:21.207543: step 2973, loss 0.779936, acc 0.703125\n",
      "2018-12-07T16:59:22.324723: step 2974, loss 0.570297, acc 0.78125\n",
      "2018-12-07T16:59:23.610573: step 2975, loss 0.799043, acc 0.71875\n",
      "2018-12-07T16:59:24.793813: step 2976, loss 0.787317, acc 0.71875\n",
      "2018-12-07T16:59:26.183500: step 2977, loss 0.690709, acc 0.75\n",
      "2018-12-07T16:59:27.729216: step 2978, loss 0.763724, acc 0.703125\n",
      "2018-12-07T16:59:28.957466: step 2979, loss 0.806894, acc 0.734375\n",
      "2018-12-07T16:59:30.397962: step 2980, loss 0.839789, acc 0.671875\n",
      "2018-12-07T16:59:31.500986: step 2981, loss 0.714282, acc 0.734375\n",
      "2018-12-07T16:59:32.971524: step 2982, loss 0.750544, acc 0.6875\n",
      "2018-12-07T16:59:34.107281: step 2983, loss 0.89122, acc 0.65625\n",
      "2018-12-07T16:59:35.383807: step 2984, loss 0.829355, acc 0.640625\n",
      "2018-12-07T16:59:36.682936: step 2985, loss 0.836101, acc 0.671875\n",
      "2018-12-07T16:59:38.249314: step 2986, loss 0.82779, acc 0.6875\n",
      "2018-12-07T16:59:39.498888: step 2987, loss 0.603394, acc 0.8125\n",
      "2018-12-07T16:59:40.751756: step 2988, loss 0.949903, acc 0.640625\n",
      "2018-12-07T16:59:41.959776: step 2989, loss 0.596246, acc 0.8125\n",
      "2018-12-07T16:59:43.137001: step 2990, loss 0.745526, acc 0.703125\n",
      "2018-12-07T16:59:44.787365: step 2991, loss 0.736684, acc 0.6875\n",
      "2018-12-07T16:59:46.067192: step 2992, loss 0.600585, acc 0.765625\n",
      "2018-12-07T16:59:47.285159: step 2993, loss 0.933942, acc 0.640625\n",
      "2018-12-07T16:59:49.035618: step 2994, loss 0.8069, acc 0.640625\n",
      "2018-12-07T16:59:50.605109: step 2995, loss 0.8292, acc 0.671875\n",
      "2018-12-07T16:59:51.822334: step 2996, loss 0.870878, acc 0.65625\n",
      "2018-12-07T16:59:52.946149: step 2997, loss 0.653665, acc 0.71875\n",
      "2018-12-07T16:59:54.555671: step 2998, loss 0.687349, acc 0.71875\n",
      "2018-12-07T16:59:55.733328: step 2999, loss 0.748454, acc 0.65625\n",
      "2018-12-07T16:59:57.127567: step 3000, loss 0.956066, acc 0.640625\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T16:59:58.397172: step 3000, loss 0.697436, acc 0.74\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-3000\n",
      "\n",
      "2018-12-07T17:00:00.521852: step 3001, loss 0.69704, acc 0.75\n",
      "2018-12-07T17:00:01.921381: step 3002, loss 0.65719, acc 0.765625\n",
      "2018-12-07T17:00:03.581395: step 3003, loss 0.777796, acc 0.703125\n",
      "2018-12-07T17:00:04.935341: step 3004, loss 0.834702, acc 0.640625\n",
      "2018-12-07T17:00:06.672671: step 3005, loss 0.734347, acc 0.765625\n",
      "2018-12-07T17:00:08.352291: step 3006, loss 0.771196, acc 0.703125\n",
      "2018-12-07T17:00:09.899314: step 3007, loss 0.836348, acc 0.703125\n",
      "2018-12-07T17:00:11.928275: step 3008, loss 0.741011, acc 0.703125\n",
      "2018-12-07T17:00:13.685962: step 3009, loss 0.797798, acc 0.703125\n",
      "2018-12-07T17:00:15.301227: step 3010, loss 0.872692, acc 0.625\n",
      "2018-12-07T17:00:16.915702: step 3011, loss 0.708851, acc 0.65625\n",
      "2018-12-07T17:00:18.385377: step 3012, loss 0.853974, acc 0.640625\n",
      "2018-12-07T17:00:20.239169: step 3013, loss 0.753154, acc 0.65625\n",
      "2018-12-07T17:00:22.034431: step 3014, loss 0.832075, acc 0.65625\n",
      "2018-12-07T17:00:23.476548: step 3015, loss 0.811224, acc 0.6875\n",
      "2018-12-07T17:00:25.304804: step 3016, loss 0.765939, acc 0.671875\n",
      "2018-12-07T17:00:26.947646: step 3017, loss 0.961568, acc 0.609375\n",
      "2018-12-07T17:00:28.848461: step 3018, loss 0.690925, acc 0.796875\n",
      "2018-12-07T17:00:30.201200: step 3019, loss 0.929418, acc 0.703125\n",
      "2018-12-07T17:00:31.665684: step 3020, loss 0.618739, acc 0.765625\n",
      "2018-12-07T17:00:32.990777: step 3021, loss 0.555838, acc 0.8125\n",
      "2018-12-07T17:00:34.382185: step 3022, loss 0.69334, acc 0.78125\n",
      "2018-12-07T17:00:35.771273: step 3023, loss 0.696143, acc 0.71875\n",
      "2018-12-07T17:00:37.479302: step 3024, loss 0.864799, acc 0.703125\n",
      "2018-12-07T17:00:39.528967: step 3025, loss 0.57547, acc 0.765625\n",
      "2018-12-07T17:00:41.555997: step 3026, loss 0.772087, acc 0.703125\n",
      "2018-12-07T17:00:43.326459: step 3027, loss 0.689118, acc 0.765625\n",
      "2018-12-07T17:00:44.820742: step 3028, loss 0.845779, acc 0.671875\n",
      "2018-12-07T17:00:46.542512: step 3029, loss 0.78926, acc 0.6875\n",
      "2018-12-07T17:00:48.231398: step 3030, loss 0.785887, acc 0.703125\n",
      "2018-12-07T17:00:50.071782: step 3031, loss 0.817749, acc 0.6875\n",
      "2018-12-07T17:00:51.729770: step 3032, loss 0.811272, acc 0.671875\n",
      "2018-12-07T17:00:53.431568: step 3033, loss 0.649724, acc 0.78125\n",
      "2018-12-07T17:00:55.051581: step 3034, loss 0.822986, acc 0.65625\n",
      "2018-12-07T17:00:57.090120: step 3035, loss 0.745759, acc 0.703125\n",
      "2018-12-07T17:00:58.659774: step 3036, loss 0.672216, acc 0.765625\n",
      "2018-12-07T17:01:00.543405: step 3037, loss 0.69996, acc 0.765625\n",
      "2018-12-07T17:01:01.985089: step 3038, loss 0.971163, acc 0.609375\n",
      "2018-12-07T17:01:03.873762: step 3039, loss 0.823385, acc 0.65625\n",
      "2018-12-07T17:01:05.350130: step 3040, loss 0.719663, acc 0.71875\n",
      "2018-12-07T17:01:07.162199: step 3041, loss 0.531827, acc 0.78125\n",
      "2018-12-07T17:01:09.346863: step 3042, loss 0.939632, acc 0.640625\n",
      "2018-12-07T17:01:11.059911: step 3043, loss 0.846413, acc 0.6875\n",
      "2018-12-07T17:01:12.816112: step 3044, loss 0.783412, acc 0.734375\n",
      "2018-12-07T17:01:14.213827: step 3045, loss 0.996321, acc 0.609375\n",
      "2018-12-07T17:01:15.800800: step 3046, loss 0.650885, acc 0.8125\n",
      "2018-12-07T17:01:18.180501: step 3047, loss 0.739797, acc 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07T17:01:19.706755: step 3048, loss 0.880624, acc 0.65625\n",
      "2018-12-07T17:01:21.399177: step 3049, loss 0.760733, acc 0.65625\n",
      "2018-12-07T17:01:23.107503: step 3050, loss 0.806355, acc 0.671875\n",
      "2018-12-07T17:01:24.850132: step 3051, loss 0.896155, acc 0.640625\n",
      "2018-12-07T17:01:26.371062: step 3052, loss 0.794104, acc 0.734375\n",
      "2018-12-07T17:01:28.066083: step 3053, loss 0.851434, acc 0.609375\n",
      "2018-12-07T17:01:29.782500: step 3054, loss 0.788338, acc 0.671875\n",
      "2018-12-07T17:01:31.221846: step 3055, loss 0.655561, acc 0.71875\n",
      "2018-12-07T17:01:33.026762: step 3056, loss 0.832546, acc 0.75\n",
      "2018-12-07T17:01:34.519898: step 3057, loss 0.727564, acc 0.71875\n",
      "2018-12-07T17:01:36.278983: step 3058, loss 0.805696, acc 0.625\n",
      "2018-12-07T17:01:37.937765: step 3059, loss 0.728615, acc 0.6875\n",
      "2018-12-07T17:01:39.710248: step 3060, loss 0.717779, acc 0.671875\n",
      "2018-12-07T17:01:41.603648: step 3061, loss 0.844605, acc 0.625\n",
      "2018-12-07T17:01:43.193061: step 3062, loss 0.658095, acc 0.78125\n",
      "2018-12-07T17:01:44.986763: step 3063, loss 0.804677, acc 0.703125\n",
      "2018-12-07T17:01:46.552884: step 3064, loss 0.725246, acc 0.734375\n",
      "2018-12-07T17:01:48.454026: step 3065, loss 0.792619, acc 0.671875\n",
      "2018-12-07T17:01:49.988850: step 3066, loss 0.913727, acc 0.625\n",
      "2018-12-07T17:01:51.884214: step 3067, loss 0.807413, acc 0.609375\n",
      "2018-12-07T17:01:53.552892: step 3068, loss 0.688167, acc 0.71875\n",
      "2018-12-07T17:01:55.382958: step 3069, loss 0.723288, acc 0.78125\n",
      "2018-12-07T17:01:57.168798: step 3070, loss 0.969895, acc 0.609375\n",
      "2018-12-07T17:01:58.717602: step 3071, loss 0.824711, acc 0.703125\n",
      "2018-12-07T17:02:00.344844: step 3072, loss 0.745927, acc 0.6875\n",
      "2018-12-07T17:02:01.896919: step 3073, loss 0.835183, acc 0.625\n",
      "2018-12-07T17:02:03.702465: step 3074, loss 0.623436, acc 0.75\n",
      "2018-12-07T17:02:05.220350: step 3075, loss 0.792619, acc 0.703125\n",
      "2018-12-07T17:02:07.279743: step 3076, loss 0.823292, acc 0.703125\n",
      "2018-12-07T17:02:08.895558: step 3077, loss 0.599671, acc 0.84375\n",
      "2018-12-07T17:02:10.963308: step 3078, loss 0.845193, acc 0.734375\n",
      "2018-12-07T17:02:12.602887: step 3079, loss 0.744143, acc 0.6875\n",
      "2018-12-07T17:02:15.156181: step 3080, loss 0.75127, acc 0.734375\n",
      "2018-12-07T17:02:16.849804: step 3081, loss 0.787161, acc 0.734375\n",
      "2018-12-07T17:02:18.354464: step 3082, loss 0.807228, acc 0.703125\n",
      "2018-12-07T17:02:19.847193: step 3083, loss 0.894022, acc 0.609375\n",
      "2018-12-07T17:02:22.137736: step 3084, loss 0.828819, acc 0.640625\n",
      "2018-12-07T17:02:23.642258: step 3085, loss 0.659715, acc 0.8125\n",
      "2018-12-07T17:02:25.694808: step 3086, loss 0.729805, acc 0.71875\n",
      "2018-12-07T17:02:27.446003: step 3087, loss 0.776529, acc 0.703125\n",
      "2018-12-07T17:02:28.966095: step 3088, loss 0.716427, acc 0.703125\n",
      "2018-12-07T17:02:30.360110: step 3089, loss 0.777471, acc 0.734375\n",
      "2018-12-07T17:02:32.352005: step 3090, loss 0.780469, acc 0.6875\n",
      "2018-12-07T17:02:34.260558: step 3091, loss 0.786137, acc 0.703125\n",
      "2018-12-07T17:02:36.077701: step 3092, loss 0.869732, acc 0.671875\n",
      "2018-12-07T17:02:38.198946: step 3093, loss 0.800326, acc 0.703125\n",
      "2018-12-07T17:02:39.964426: step 3094, loss 0.656246, acc 0.71875\n",
      "2018-12-07T17:02:41.388488: step 3095, loss 0.723495, acc 0.765625\n",
      "2018-12-07T17:02:42.381591: step 3096, loss 0.618501, acc 0.826087\n",
      "2018-12-07T17:02:43.449119: step 3097, loss 0.63042, acc 0.6875\n",
      "2018-12-07T17:02:44.466839: step 3098, loss 0.693034, acc 0.765625\n",
      "2018-12-07T17:02:45.470712: step 3099, loss 0.501725, acc 0.796875\n",
      "2018-12-07T17:02:46.439882: step 3100, loss 0.665506, acc 0.734375\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T17:02:47.754369: step 3100, loss 0.655215, acc 0.776\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-3100\n",
      "\n",
      "2018-12-07T17:02:49.238409: step 3101, loss 0.746275, acc 0.703125\n",
      "2018-12-07T17:02:50.428411: step 3102, loss 0.618132, acc 0.75\n",
      "2018-12-07T17:02:51.469164: step 3103, loss 0.758189, acc 0.703125\n",
      "2018-12-07T17:02:52.614692: step 3104, loss 0.580005, acc 0.765625\n",
      "2018-12-07T17:02:53.931862: step 3105, loss 0.580692, acc 0.75\n",
      "2018-12-07T17:02:55.089865: step 3106, loss 0.651316, acc 0.71875\n",
      "2018-12-07T17:02:56.124948: step 3107, loss 0.585833, acc 0.78125\n",
      "2018-12-07T17:02:57.174173: step 3108, loss 0.599796, acc 0.75\n",
      "2018-12-07T17:02:58.438072: step 3109, loss 0.621061, acc 0.75\n",
      "2018-12-07T17:02:59.485063: step 3110, loss 0.624924, acc 0.828125\n",
      "2018-12-07T17:03:00.758352: step 3111, loss 1.01033, acc 0.640625\n",
      "2018-12-07T17:03:01.788467: step 3112, loss 0.744786, acc 0.71875\n",
      "2018-12-07T17:03:02.758569: step 3113, loss 0.628499, acc 0.6875\n",
      "2018-12-07T17:03:03.842042: step 3114, loss 0.755691, acc 0.6875\n",
      "2018-12-07T17:03:04.799348: step 3115, loss 0.734724, acc 0.703125\n",
      "2018-12-07T17:03:05.787773: step 3116, loss 0.579569, acc 0.78125\n",
      "2018-12-07T17:03:06.903116: step 3117, loss 0.651923, acc 0.71875\n",
      "2018-12-07T17:03:08.132606: step 3118, loss 0.533716, acc 0.78125\n",
      "2018-12-07T17:03:09.154931: step 3119, loss 0.739295, acc 0.6875\n",
      "2018-12-07T17:03:10.160452: step 3120, loss 0.694485, acc 0.75\n",
      "2018-12-07T17:03:11.288701: step 3121, loss 0.663563, acc 0.765625\n",
      "2018-12-07T17:03:12.639230: step 3122, loss 0.707121, acc 0.734375\n",
      "2018-12-07T17:03:13.689210: step 3123, loss 0.629755, acc 0.75\n",
      "2018-12-07T17:03:14.801871: step 3124, loss 0.843511, acc 0.703125\n",
      "2018-12-07T17:03:15.820216: step 3125, loss 0.671296, acc 0.75\n",
      "2018-12-07T17:03:17.011870: step 3126, loss 0.771801, acc 0.71875\n",
      "2018-12-07T17:03:18.287109: step 3127, loss 0.604652, acc 0.734375\n",
      "2018-12-07T17:03:19.502364: step 3128, loss 0.712071, acc 0.734375\n",
      "2018-12-07T17:03:20.616499: step 3129, loss 0.759038, acc 0.6875\n",
      "2018-12-07T17:03:21.762079: step 3130, loss 0.695927, acc 0.703125\n",
      "2018-12-07T17:03:23.035347: step 3131, loss 0.632356, acc 0.75\n",
      "2018-12-07T17:03:24.329240: step 3132, loss 0.696812, acc 0.71875\n",
      "2018-12-07T17:03:25.418465: step 3133, loss 0.589249, acc 0.765625\n",
      "2018-12-07T17:03:26.409931: step 3134, loss 0.635361, acc 0.734375\n",
      "2018-12-07T17:03:27.495571: step 3135, loss 0.661009, acc 0.6875\n",
      "2018-12-07T17:03:28.503916: step 3136, loss 0.680833, acc 0.734375\n",
      "2018-12-07T17:03:29.515499: step 3137, loss 0.562744, acc 0.765625\n",
      "2018-12-07T17:03:30.660385: step 3138, loss 0.619748, acc 0.78125\n",
      "2018-12-07T17:03:31.723137: step 3139, loss 0.561197, acc 0.765625\n",
      "2018-12-07T17:03:32.717228: step 3140, loss 0.574029, acc 0.796875\n",
      "2018-12-07T17:03:33.864096: step 3141, loss 0.563684, acc 0.8125\n",
      "2018-12-07T17:03:35.087873: step 3142, loss 0.811864, acc 0.640625\n",
      "2018-12-07T17:03:36.558800: step 3143, loss 0.781888, acc 0.65625\n",
      "2018-12-07T17:03:38.429587: step 3144, loss 0.785428, acc 0.734375\n",
      "2018-12-07T17:03:39.712964: step 3145, loss 0.689205, acc 0.75\n",
      "2018-12-07T17:03:40.826899: step 3146, loss 0.887515, acc 0.6875\n",
      "2018-12-07T17:03:41.985730: step 3147, loss 0.482173, acc 0.84375\n",
      "2018-12-07T17:03:43.366492: step 3148, loss 0.641529, acc 0.734375\n",
      "2018-12-07T17:03:44.723448: step 3149, loss 0.482815, acc 0.796875\n",
      "2018-12-07T17:03:46.023532: step 3150, loss 0.663854, acc 0.78125\n",
      "2018-12-07T17:03:47.087296: step 3151, loss 0.464545, acc 0.890625\n",
      "2018-12-07T17:03:48.309025: step 3152, loss 0.668496, acc 0.8125\n",
      "2018-12-07T17:03:49.345650: step 3153, loss 0.787676, acc 0.703125\n",
      "2018-12-07T17:03:50.351282: step 3154, loss 0.785664, acc 0.671875\n",
      "2018-12-07T17:03:51.435796: step 3155, loss 0.767627, acc 0.671875\n",
      "2018-12-07T17:03:52.438097: step 3156, loss 0.617737, acc 0.78125\n",
      "2018-12-07T17:03:53.520234: step 3157, loss 0.687978, acc 0.765625\n",
      "2018-12-07T17:03:54.538081: step 3158, loss 0.741352, acc 0.71875\n",
      "2018-12-07T17:03:55.581336: step 3159, loss 0.752728, acc 0.734375\n",
      "2018-12-07T17:03:56.743302: step 3160, loss 0.509358, acc 0.828125\n",
      "2018-12-07T17:03:57.997057: step 3161, loss 0.748468, acc 0.6875\n",
      "2018-12-07T17:03:59.029146: step 3162, loss 0.603594, acc 0.8125\n",
      "2018-12-07T17:04:00.090832: step 3163, loss 0.630107, acc 0.796875\n",
      "2018-12-07T17:04:01.115041: step 3164, loss 0.907891, acc 0.65625\n",
      "2018-12-07T17:04:02.195102: step 3165, loss 0.729645, acc 0.671875\n",
      "2018-12-07T17:04:03.361757: step 3166, loss 0.815514, acc 0.671875\n",
      "2018-12-07T17:04:04.366424: step 3167, loss 0.66997, acc 0.765625\n",
      "2018-12-07T17:04:05.408622: step 3168, loss 0.59021, acc 0.765625\n",
      "2018-12-07T17:04:06.428907: step 3169, loss 0.758116, acc 0.703125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07T17:04:07.502652: step 3170, loss 0.481569, acc 0.796875\n",
      "2018-12-07T17:04:08.489536: step 3171, loss 0.58308, acc 0.796875\n",
      "2018-12-07T17:04:09.614567: step 3172, loss 0.687275, acc 0.6875\n",
      "2018-12-07T17:04:10.796485: step 3173, loss 0.598421, acc 0.75\n",
      "2018-12-07T17:04:11.812638: step 3174, loss 0.897115, acc 0.640625\n",
      "2018-12-07T17:04:12.950638: step 3175, loss 0.589238, acc 0.78125\n",
      "2018-12-07T17:04:13.978452: step 3176, loss 0.575385, acc 0.78125\n",
      "2018-12-07T17:04:15.145363: step 3177, loss 0.715275, acc 0.734375\n",
      "2018-12-07T17:04:16.171643: step 3178, loss 0.766318, acc 0.703125\n",
      "2018-12-07T17:04:17.193131: step 3179, loss 0.765324, acc 0.6875\n",
      "2018-12-07T17:04:18.519691: step 3180, loss 0.686127, acc 0.75\n",
      "2018-12-07T17:04:19.712436: step 3181, loss 0.69444, acc 0.71875\n",
      "2018-12-07T17:04:20.834075: step 3182, loss 0.805597, acc 0.640625\n",
      "2018-12-07T17:04:22.007050: step 3183, loss 0.870302, acc 0.640625\n",
      "2018-12-07T17:04:23.071814: step 3184, loss 0.684215, acc 0.796875\n",
      "2018-12-07T17:04:24.218431: step 3185, loss 0.818254, acc 0.65625\n",
      "2018-12-07T17:04:25.257103: step 3186, loss 0.921018, acc 0.65625\n",
      "2018-12-07T17:04:26.296909: step 3187, loss 0.631175, acc 0.75\n",
      "2018-12-07T17:04:27.408595: step 3188, loss 0.74265, acc 0.71875\n",
      "2018-12-07T17:04:28.799471: step 3189, loss 0.643656, acc 0.796875\n",
      "2018-12-07T17:04:29.962615: step 3190, loss 0.6835, acc 0.71875\n",
      "2018-12-07T17:04:31.125923: step 3191, loss 0.748859, acc 0.75\n",
      "2018-12-07T17:04:32.227797: step 3192, loss 0.683589, acc 0.734375\n",
      "2018-12-07T17:04:33.776587: step 3193, loss 0.566654, acc 0.796875\n",
      "2018-12-07T17:04:34.762960: step 3194, loss 0.6249, acc 0.8125\n",
      "2018-12-07T17:04:35.807529: step 3195, loss 0.688511, acc 0.75\n",
      "2018-12-07T17:04:37.116407: step 3196, loss 0.626835, acc 0.765625\n",
      "2018-12-07T17:04:38.162567: step 3197, loss 0.490205, acc 0.859375\n",
      "2018-12-07T17:04:39.177020: step 3198, loss 0.640507, acc 0.6875\n",
      "2018-12-07T17:04:40.402079: step 3199, loss 0.597668, acc 0.78125\n",
      "2018-12-07T17:04:41.572168: step 3200, loss 0.98357, acc 0.640625\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T17:04:42.822824: step 3200, loss 0.621653, acc 0.792\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-3200\n",
      "\n",
      "2018-12-07T17:04:44.783148: step 3201, loss 0.59945, acc 0.765625\n",
      "2018-12-07T17:04:46.164359: step 3202, loss 0.705581, acc 0.734375\n",
      "2018-12-07T17:04:47.285598: step 3203, loss 0.746734, acc 0.703125\n",
      "2018-12-07T17:04:48.428979: step 3204, loss 0.585114, acc 0.828125\n",
      "2018-12-07T17:04:49.605858: step 3205, loss 0.665221, acc 0.75\n",
      "2018-12-07T17:04:50.792651: step 3206, loss 0.551103, acc 0.8125\n",
      "2018-12-07T17:04:52.395559: step 3207, loss 0.813893, acc 0.6875\n",
      "2018-12-07T17:04:53.501845: step 3208, loss 0.773017, acc 0.65625\n",
      "2018-12-07T17:04:55.017924: step 3209, loss 0.731527, acc 0.703125\n",
      "2018-12-07T17:04:56.403290: step 3210, loss 0.695874, acc 0.75\n",
      "2018-12-07T17:04:57.592038: step 3211, loss 0.712144, acc 0.75\n",
      "2018-12-07T17:04:59.153328: step 3212, loss 0.709049, acc 0.765625\n",
      "2018-12-07T17:05:00.334298: step 3213, loss 0.593055, acc 0.828125\n",
      "2018-12-07T17:05:01.729244: step 3214, loss 0.73908, acc 0.765625\n",
      "2018-12-07T17:05:02.919451: step 3215, loss 0.774604, acc 0.6875\n",
      "2018-12-07T17:05:04.305256: step 3216, loss 0.773907, acc 0.65625\n",
      "2018-12-07T17:05:05.395322: step 3217, loss 0.547911, acc 0.765625\n",
      "2018-12-07T17:05:06.473104: step 3218, loss 0.788982, acc 0.671875\n",
      "2018-12-07T17:05:07.797598: step 3219, loss 0.518422, acc 0.84375\n",
      "2018-12-07T17:05:09.156177: step 3220, loss 0.623161, acc 0.796875\n",
      "2018-12-07T17:05:10.304251: step 3221, loss 0.689705, acc 0.734375\n",
      "2018-12-07T17:05:11.399705: step 3222, loss 0.69633, acc 0.78125\n",
      "2018-12-07T17:05:12.565960: step 3223, loss 0.680106, acc 0.703125\n",
      "2018-12-07T17:05:13.802317: step 3224, loss 0.659273, acc 0.75\n",
      "2018-12-07T17:05:15.088648: step 3225, loss 0.614393, acc 0.796875\n",
      "2018-12-07T17:05:16.173065: step 3226, loss 0.78703, acc 0.71875\n",
      "2018-12-07T17:05:17.267178: step 3227, loss 0.688997, acc 0.75\n",
      "2018-12-07T17:05:18.427270: step 3228, loss 0.6032, acc 0.78125\n",
      "2018-12-07T17:05:19.560169: step 3229, loss 0.715306, acc 0.6875\n",
      "2018-12-07T17:05:20.799729: step 3230, loss 0.636577, acc 0.765625\n",
      "2018-12-07T17:05:21.935288: step 3231, loss 0.837794, acc 0.6875\n",
      "2018-12-07T17:05:23.377343: step 3232, loss 0.791348, acc 0.6875\n",
      "2018-12-07T17:05:24.417201: step 3233, loss 0.580487, acc 0.796875\n",
      "2018-12-07T17:05:25.547484: step 3234, loss 0.684639, acc 0.71875\n",
      "2018-12-07T17:05:26.773188: step 3235, loss 0.711287, acc 0.75\n",
      "2018-12-07T17:05:28.263332: step 3236, loss 0.729051, acc 0.75\n",
      "2018-12-07T17:05:29.489098: step 3237, loss 0.594668, acc 0.765625\n",
      "2018-12-07T17:05:30.981922: step 3238, loss 0.738215, acc 0.734375\n",
      "2018-12-07T17:05:31.997578: step 3239, loss 0.603653, acc 0.71875\n",
      "2018-12-07T17:05:33.178707: step 3240, loss 0.753442, acc 0.625\n",
      "2018-12-07T17:05:34.402768: step 3241, loss 0.799638, acc 0.671875\n",
      "2018-12-07T17:05:35.645309: step 3242, loss 0.943127, acc 0.609375\n",
      "2018-12-07T17:05:36.713230: step 3243, loss 0.563729, acc 0.828125\n",
      "2018-12-07T17:05:38.212930: step 3244, loss 0.622345, acc 0.796875\n",
      "2018-12-07T17:05:39.647164: step 3245, loss 0.636729, acc 0.734375\n",
      "2018-12-07T17:05:40.765831: step 3246, loss 0.712826, acc 0.703125\n",
      "2018-12-07T17:05:41.960733: step 3247, loss 0.498691, acc 0.828125\n",
      "2018-12-07T17:05:43.307156: step 3248, loss 0.741531, acc 0.6875\n",
      "2018-12-07T17:05:44.478304: step 3249, loss 0.74085, acc 0.75\n",
      "2018-12-07T17:05:45.547919: step 3250, loss 0.573147, acc 0.75\n",
      "2018-12-07T17:05:46.639221: step 3251, loss 0.629764, acc 0.78125\n",
      "2018-12-07T17:05:47.734918: step 3252, loss 0.741594, acc 0.6875\n",
      "2018-12-07T17:05:48.785745: step 3253, loss 0.60693, acc 0.71875\n",
      "2018-12-07T17:05:50.202513: step 3254, loss 0.584818, acc 0.75\n",
      "2018-12-07T17:05:51.259479: step 3255, loss 0.927632, acc 0.671875\n",
      "2018-12-07T17:05:52.330373: step 3256, loss 0.753422, acc 0.6875\n",
      "2018-12-07T17:05:53.760855: step 3257, loss 0.656245, acc 0.75\n",
      "2018-12-07T17:05:54.814672: step 3258, loss 0.703421, acc 0.765625\n",
      "2018-12-07T17:05:56.129259: step 3259, loss 0.695155, acc 0.75\n",
      "2018-12-07T17:05:57.292210: step 3260, loss 0.80171, acc 0.65625\n",
      "2018-12-07T17:05:58.440112: step 3261, loss 0.834883, acc 0.65625\n",
      "2018-12-07T17:05:59.492940: step 3262, loss 0.660552, acc 0.71875\n",
      "2018-12-07T17:06:00.549921: step 3263, loss 0.755821, acc 0.6875\n",
      "2018-12-07T17:06:01.753510: step 3264, loss 0.748794, acc 0.734375\n",
      "2018-12-07T17:06:03.304735: step 3265, loss 0.789477, acc 0.6875\n",
      "2018-12-07T17:06:04.369925: step 3266, loss 0.794129, acc 0.703125\n",
      "2018-12-07T17:06:05.404624: step 3267, loss 0.548281, acc 0.765625\n",
      "2018-12-07T17:06:06.487613: step 3268, loss 0.692387, acc 0.71875\n",
      "2018-12-07T17:06:07.648310: step 3269, loss 0.585748, acc 0.796875\n",
      "2018-12-07T17:06:08.731915: step 3270, loss 0.462557, acc 0.859375\n",
      "2018-12-07T17:06:09.763056: step 3271, loss 0.609473, acc 0.765625\n",
      "2018-12-07T17:06:10.882457: step 3272, loss 0.676888, acc 0.75\n",
      "2018-12-07T17:06:12.101271: step 3273, loss 0.660843, acc 0.765625\n",
      "2018-12-07T17:06:13.683689: step 3274, loss 0.587995, acc 0.765625\n",
      "2018-12-07T17:06:14.979878: step 3275, loss 0.737062, acc 0.71875\n",
      "2018-12-07T17:06:16.161193: step 3276, loss 0.717878, acc 0.75\n",
      "2018-12-07T17:06:17.574028: step 3277, loss 0.885108, acc 0.640625\n",
      "2018-12-07T17:06:18.713326: step 3278, loss 0.840833, acc 0.71875\n",
      "2018-12-07T17:06:19.986493: step 3279, loss 0.766042, acc 0.71875\n",
      "2018-12-07T17:06:21.367762: step 3280, loss 0.65656, acc 0.734375\n",
      "2018-12-07T17:06:22.380466: step 3281, loss 0.717866, acc 0.75\n",
      "2018-12-07T17:06:23.616853: step 3282, loss 0.854158, acc 0.6875\n",
      "2018-12-07T17:06:24.924379: step 3283, loss 0.90607, acc 0.65625\n",
      "2018-12-07T17:06:25.974386: step 3284, loss 0.681849, acc 0.71875\n",
      "2018-12-07T17:06:27.271658: step 3285, loss 0.721452, acc 0.75\n",
      "2018-12-07T17:06:28.712246: step 3286, loss 0.712398, acc 0.75\n",
      "2018-12-07T17:06:29.796156: step 3287, loss 0.78279, acc 0.6875\n",
      "2018-12-07T17:06:31.093672: step 3288, loss 0.70445, acc 0.765625\n",
      "2018-12-07T17:06:32.227679: step 3289, loss 0.61841, acc 0.78125\n",
      "2018-12-07T17:06:33.405611: step 3290, loss 0.802234, acc 0.640625\n",
      "2018-12-07T17:06:34.496416: step 3291, loss 0.655047, acc 0.71875\n",
      "2018-12-07T17:06:35.626664: step 3292, loss 0.646275, acc 0.796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07T17:06:36.824739: step 3293, loss 0.757389, acc 0.71875\n",
      "2018-12-07T17:06:38.043255: step 3294, loss 0.818154, acc 0.625\n",
      "2018-12-07T17:06:39.303781: step 3295, loss 0.560879, acc 0.78125\n",
      "2018-12-07T17:06:40.393149: step 3296, loss 0.779433, acc 0.6875\n",
      "2018-12-07T17:06:41.467295: step 3297, loss 0.781352, acc 0.734375\n",
      "2018-12-07T17:06:42.551261: step 3298, loss 0.789344, acc 0.734375\n",
      "2018-12-07T17:06:43.943380: step 3299, loss 0.684742, acc 0.703125\n",
      "2018-12-07T17:06:45.075374: step 3300, loss 0.614667, acc 0.828125\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T17:06:46.316057: step 3300, loss 0.610425, acc 0.76\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-3300\n",
      "\n",
      "2018-12-07T17:06:48.526288: step 3301, loss 0.810205, acc 0.703125\n",
      "2018-12-07T17:06:50.063161: step 3302, loss 0.464908, acc 0.875\n",
      "2018-12-07T17:06:51.365857: step 3303, loss 0.656904, acc 0.796875\n",
      "2018-12-07T17:06:52.560096: step 3304, loss 0.696129, acc 0.6875\n",
      "2018-12-07T17:06:53.746343: step 3305, loss 0.751946, acc 0.734375\n",
      "2018-12-07T17:06:55.020324: step 3306, loss 0.791444, acc 0.640625\n",
      "2018-12-07T17:06:56.296595: step 3307, loss 0.679893, acc 0.71875\n",
      "2018-12-07T17:06:57.617002: step 3308, loss 0.829, acc 0.65625\n",
      "2018-12-07T17:06:59.075222: step 3309, loss 0.74373, acc 0.640625\n",
      "2018-12-07T17:07:00.332168: step 3310, loss 0.860693, acc 0.65625\n",
      "2018-12-07T17:07:01.731548: step 3311, loss 0.60268, acc 0.84375\n",
      "2018-12-07T17:07:03.187522: step 3312, loss 0.600687, acc 0.828125\n",
      "2018-12-07T17:07:04.693739: step 3313, loss 0.799776, acc 0.65625\n",
      "2018-12-07T17:07:06.461937: step 3314, loss 0.718381, acc 0.6875\n",
      "2018-12-07T17:07:07.830713: step 3315, loss 0.749552, acc 0.703125\n",
      "2018-12-07T17:07:09.500914: step 3316, loss 0.575612, acc 0.78125\n",
      "2018-12-07T17:07:10.674640: step 3317, loss 0.836035, acc 0.734375\n",
      "2018-12-07T17:07:12.034118: step 3318, loss 0.720051, acc 0.765625\n",
      "2018-12-07T17:07:13.180941: step 3319, loss 0.600732, acc 0.8125\n",
      "2018-12-07T17:07:14.473801: step 3320, loss 0.675949, acc 0.6875\n",
      "2018-12-07T17:07:15.767578: step 3321, loss 0.485546, acc 0.859375\n",
      "2018-12-07T17:07:17.090428: step 3322, loss 0.62195, acc 0.734375\n",
      "2018-12-07T17:07:18.467174: step 3323, loss 0.907119, acc 0.609375\n",
      "2018-12-07T17:07:19.752926: step 3324, loss 0.594241, acc 0.78125\n",
      "2018-12-07T17:07:21.139597: step 3325, loss 0.706709, acc 0.71875\n",
      "2018-12-07T17:07:22.715357: step 3326, loss 0.731507, acc 0.765625\n",
      "2018-12-07T17:07:23.939889: step 3327, loss 0.557948, acc 0.796875\n",
      "2018-12-07T17:07:25.405770: step 3328, loss 0.741017, acc 0.71875\n",
      "2018-12-07T17:07:26.666709: step 3329, loss 0.626168, acc 0.71875\n",
      "2018-12-07T17:07:28.139696: step 3330, loss 0.885969, acc 0.6875\n",
      "2018-12-07T17:07:29.873502: step 3331, loss 0.806734, acc 0.71875\n",
      "2018-12-07T17:07:31.336814: step 3332, loss 0.857023, acc 0.671875\n",
      "2018-12-07T17:07:32.743589: step 3333, loss 0.559767, acc 0.8125\n",
      "2018-12-07T17:07:34.181989: step 3334, loss 0.914936, acc 0.71875\n",
      "2018-12-07T17:07:35.848579: step 3335, loss 0.609006, acc 0.796875\n",
      "2018-12-07T17:07:37.323609: step 3336, loss 0.551313, acc 0.796875\n",
      "2018-12-07T17:07:38.584910: step 3337, loss 0.666453, acc 0.75\n",
      "2018-12-07T17:07:39.812889: step 3338, loss 0.701362, acc 0.734375\n",
      "2018-12-07T17:07:40.988412: step 3339, loss 0.70323, acc 0.71875\n",
      "2018-12-07T17:07:42.295703: step 3340, loss 0.653125, acc 0.796875\n",
      "2018-12-07T17:07:44.174329: step 3341, loss 0.842404, acc 0.6875\n",
      "2018-12-07T17:07:45.461049: step 3342, loss 0.746732, acc 0.703125\n",
      "2018-12-07T17:07:46.672951: step 3343, loss 0.89936, acc 0.671875\n",
      "2018-12-07T17:07:48.103869: step 3344, loss 0.673952, acc 0.765625\n",
      "2018-12-07T17:07:49.260418: step 3345, loss 0.758364, acc 0.671875\n",
      "2018-12-07T17:07:50.540123: step 3346, loss 0.902644, acc 0.609375\n",
      "2018-12-07T17:07:51.824708: step 3347, loss 0.721292, acc 0.71875\n",
      "2018-12-07T17:07:52.961686: step 3348, loss 1.04413, acc 0.625\n",
      "2018-12-07T17:07:54.161117: step 3349, loss 0.529606, acc 0.796875\n",
      "2018-12-07T17:07:55.446725: step 3350, loss 0.699644, acc 0.75\n",
      "2018-12-07T17:07:56.795232: step 3351, loss 0.77039, acc 0.6875\n",
      "2018-12-07T17:07:58.311315: step 3352, loss 0.686194, acc 0.75\n",
      "2018-12-07T17:07:59.607327: step 3353, loss 0.650288, acc 0.796875\n",
      "2018-12-07T17:08:01.180818: step 3354, loss 0.754719, acc 0.71875\n",
      "2018-12-07T17:08:02.508439: step 3355, loss 0.890957, acc 0.609375\n",
      "2018-12-07T17:08:04.220966: step 3356, loss 0.867421, acc 0.59375\n",
      "2018-12-07T17:08:05.710493: step 3357, loss 0.707388, acc 0.6875\n",
      "2018-12-07T17:08:07.020774: step 3358, loss 0.678844, acc 0.796875\n",
      "2018-12-07T17:08:08.719461: step 3359, loss 0.765922, acc 0.65625\n",
      "2018-12-07T17:08:10.302497: step 3360, loss 0.873923, acc 0.71875\n",
      "2018-12-07T17:08:11.577383: step 3361, loss 0.737826, acc 0.765625\n",
      "2018-12-07T17:08:12.719814: step 3362, loss 0.81686, acc 0.6875\n",
      "2018-12-07T17:08:14.461902: step 3363, loss 0.563238, acc 0.796875\n",
      "2018-12-07T17:08:15.646814: step 3364, loss 0.668735, acc 0.71875\n",
      "2018-12-07T17:08:17.317097: step 3365, loss 0.624084, acc 0.765625\n",
      "2018-12-07T17:08:18.549132: step 3366, loss 0.807816, acc 0.65625\n",
      "2018-12-07T17:08:19.865402: step 3367, loss 0.728864, acc 0.765625\n",
      "2018-12-07T17:08:20.998542: step 3368, loss 0.66734, acc 0.765625\n",
      "2018-12-07T17:08:22.408261: step 3369, loss 0.678015, acc 0.8125\n",
      "2018-12-07T17:08:23.588097: step 3370, loss 0.697185, acc 0.703125\n",
      "2018-12-07T17:08:24.851183: step 3371, loss 0.526028, acc 0.796875\n",
      "2018-12-07T17:08:26.281202: step 3372, loss 0.595567, acc 0.796875\n",
      "2018-12-07T17:08:27.514059: step 3373, loss 0.694647, acc 0.734375\n",
      "2018-12-07T17:08:28.743817: step 3374, loss 0.896657, acc 0.71875\n",
      "2018-12-07T17:08:30.079076: step 3375, loss 0.852915, acc 0.734375\n",
      "2018-12-07T17:08:31.397951: step 3376, loss 0.657923, acc 0.75\n",
      "2018-12-07T17:08:32.665246: step 3377, loss 0.515884, acc 0.8125\n",
      "2018-12-07T17:08:34.159272: step 3378, loss 0.582924, acc 0.75\n",
      "2018-12-07T17:08:35.494780: step 3379, loss 0.710383, acc 0.734375\n",
      "2018-12-07T17:08:36.804960: step 3380, loss 0.898224, acc 0.734375\n",
      "2018-12-07T17:08:38.422764: step 3381, loss 0.752017, acc 0.703125\n",
      "2018-12-07T17:08:39.706833: step 3382, loss 0.787711, acc 0.71875\n",
      "2018-12-07T17:08:40.814483: step 3383, loss 0.59473, acc 0.796875\n",
      "2018-12-07T17:08:42.493713: step 3384, loss 0.59593, acc 0.796875\n",
      "2018-12-07T17:08:44.077913: step 3385, loss 0.828982, acc 0.65625\n",
      "2018-12-07T17:08:45.385378: step 3386, loss 0.70721, acc 0.734375\n",
      "2018-12-07T17:08:46.807811: step 3387, loss 0.870913, acc 0.625\n",
      "2018-12-07T17:08:48.232677: step 3388, loss 0.681662, acc 0.6875\n",
      "2018-12-07T17:08:49.478326: step 3389, loss 0.635853, acc 0.78125\n",
      "2018-12-07T17:08:50.752087: step 3390, loss 0.671529, acc 0.671875\n",
      "2018-12-07T17:08:52.165783: step 3391, loss 0.62772, acc 0.796875\n",
      "2018-12-07T17:08:53.701922: step 3392, loss 0.699107, acc 0.734375\n",
      "2018-12-07T17:08:55.394463: step 3393, loss 0.752898, acc 0.609375\n",
      "2018-12-07T17:08:56.666549: step 3394, loss 0.646862, acc 0.765625\n",
      "2018-12-07T17:08:58.151165: step 3395, loss 0.605613, acc 0.734375\n",
      "2018-12-07T17:08:59.596118: step 3396, loss 0.630639, acc 0.75\n",
      "2018-12-07T17:09:00.994762: step 3397, loss 0.895764, acc 0.640625\n",
      "2018-12-07T17:09:02.419575: step 3398, loss 0.703711, acc 0.71875\n",
      "2018-12-07T17:09:04.074876: step 3399, loss 0.781072, acc 0.734375\n",
      "2018-12-07T17:09:05.760383: step 3400, loss 0.613331, acc 0.859375\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T17:09:06.999071: step 3400, loss 0.605502, acc 0.784\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-3400\n",
      "\n",
      "2018-12-07T17:09:09.033589: step 3401, loss 0.814968, acc 0.65625\n",
      "2018-12-07T17:09:10.600485: step 3402, loss 0.727167, acc 0.75\n",
      "2018-12-07T17:09:12.370939: step 3403, loss 0.516423, acc 0.796875\n",
      "2018-12-07T17:09:14.394919: step 3404, loss 0.598865, acc 0.78125\n",
      "2018-12-07T17:09:16.233914: step 3405, loss 0.738326, acc 0.71875\n",
      "2018-12-07T17:09:18.044656: step 3406, loss 0.705796, acc 0.78125\n",
      "2018-12-07T17:09:19.494955: step 3407, loss 0.819836, acc 0.71875\n",
      "2018-12-07T17:09:21.257580: step 3408, loss 0.710803, acc 0.6875\n",
      "2018-12-07T17:09:23.330092: step 3409, loss 0.799639, acc 0.6875\n",
      "2018-12-07T17:09:25.279438: step 3410, loss 0.829275, acc 0.71875\n",
      "2018-12-07T17:09:27.143303: step 3411, loss 0.756397, acc 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07T17:09:28.715209: step 3412, loss 0.684649, acc 0.734375\n",
      "2018-12-07T17:09:30.227719: step 3413, loss 0.670214, acc 0.734375\n",
      "2018-12-07T17:09:31.932243: step 3414, loss 0.784455, acc 0.734375\n",
      "2018-12-07T17:09:33.459326: step 3415, loss 0.532873, acc 0.796875\n",
      "2018-12-07T17:09:35.113994: step 3416, loss 0.871306, acc 0.609375\n",
      "2018-12-07T17:09:36.757175: step 3417, loss 0.608016, acc 0.796875\n",
      "2018-12-07T17:09:38.257468: step 3418, loss 0.644827, acc 0.765625\n",
      "2018-12-07T17:09:39.756408: step 3419, loss 0.73189, acc 0.671875\n",
      "2018-12-07T17:09:41.370967: step 3420, loss 0.673624, acc 0.765625\n",
      "2018-12-07T17:09:43.202159: step 3421, loss 0.7128, acc 0.6875\n",
      "2018-12-07T17:09:44.922763: step 3422, loss 0.843609, acc 0.640625\n",
      "2018-12-07T17:09:46.394299: step 3423, loss 0.782843, acc 0.703125\n",
      "2018-12-07T17:09:48.213989: step 3424, loss 0.504753, acc 0.828125\n",
      "2018-12-07T17:09:50.025686: step 3425, loss 0.802909, acc 0.734375\n",
      "2018-12-07T17:09:51.418167: step 3426, loss 0.746989, acc 0.75\n",
      "2018-12-07T17:09:53.211409: step 3427, loss 0.819924, acc 0.6875\n",
      "2018-12-07T17:09:55.020160: step 3428, loss 0.69636, acc 0.734375\n",
      "2018-12-07T17:09:56.636909: step 3429, loss 0.709525, acc 0.703125\n",
      "2018-12-07T17:09:58.633566: step 3430, loss 0.671036, acc 0.78125\n",
      "2018-12-07T17:10:00.625025: step 3431, loss 0.678358, acc 0.796875\n",
      "2018-12-07T17:10:02.226710: step 3432, loss 0.814789, acc 0.671875\n",
      "2018-12-07T17:10:04.046783: step 3433, loss 0.681281, acc 0.734375\n",
      "2018-12-07T17:10:05.548386: step 3434, loss 0.804883, acc 0.703125\n",
      "2018-12-07T17:10:07.264352: step 3435, loss 0.719406, acc 0.734375\n",
      "2018-12-07T17:10:09.263837: step 3436, loss 0.614588, acc 0.765625\n",
      "2018-12-07T17:10:10.802259: step 3437, loss 0.67414, acc 0.6875\n",
      "2018-12-07T17:10:12.338119: step 3438, loss 0.737641, acc 0.71875\n",
      "2018-12-07T17:10:13.849000: step 3439, loss 0.631894, acc 0.671875\n",
      "2018-12-07T17:10:15.488720: step 3440, loss 0.762427, acc 0.703125\n",
      "2018-12-07T17:10:17.184073: step 3441, loss 0.806782, acc 0.640625\n",
      "2018-12-07T17:10:18.982349: step 3442, loss 0.6233, acc 0.765625\n",
      "2018-12-07T17:10:20.709794: step 3443, loss 0.623222, acc 0.6875\n",
      "2018-12-07T17:10:22.673539: step 3444, loss 0.620255, acc 0.796875\n",
      "2018-12-07T17:10:24.220215: step 3445, loss 0.833928, acc 0.671875\n",
      "2018-12-07T17:10:25.888821: step 3446, loss 0.608933, acc 0.765625\n",
      "2018-12-07T17:10:27.471725: step 3447, loss 0.960226, acc 0.59375\n",
      "2018-12-07T17:10:29.381289: step 3448, loss 0.899886, acc 0.734375\n",
      "2018-12-07T17:10:31.053366: step 3449, loss 0.53122, acc 0.796875\n",
      "2018-12-07T17:10:32.981887: step 3450, loss 0.847425, acc 0.640625\n",
      "2018-12-07T17:10:34.649373: step 3451, loss 0.633261, acc 0.796875\n",
      "2018-12-07T17:10:36.398046: step 3452, loss 0.729263, acc 0.703125\n",
      "2018-12-07T17:10:38.439213: step 3453, loss 0.719171, acc 0.734375\n",
      "2018-12-07T17:10:40.063362: step 3454, loss 0.838174, acc 0.65625\n",
      "2018-12-07T17:10:41.844586: step 3455, loss 0.789762, acc 0.625\n",
      "2018-12-07T17:10:43.365404: step 3456, loss 0.697743, acc 0.765625\n",
      "2018-12-07T17:10:45.180687: step 3457, loss 0.769638, acc 0.6875\n",
      "2018-12-07T17:10:47.083635: step 3458, loss 0.842277, acc 0.75\n",
      "2018-12-07T17:10:48.539690: step 3459, loss 0.820582, acc 0.71875\n",
      "2018-12-07T17:10:50.356926: step 3460, loss 0.691573, acc 0.703125\n",
      "2018-12-07T17:10:51.759621: step 3461, loss 0.676817, acc 0.734375\n",
      "2018-12-07T17:10:53.589821: step 3462, loss 0.689171, acc 0.6875\n",
      "2018-12-07T17:10:55.472845: step 3463, loss 0.63291, acc 0.765625\n",
      "2018-12-07T17:10:57.159623: step 3464, loss 0.899879, acc 0.609375\n",
      "2018-12-07T17:10:58.878254: step 3465, loss 0.612412, acc 0.71875\n",
      "2018-12-07T17:11:00.362129: step 3466, loss 0.530079, acc 0.8125\n",
      "2018-12-07T17:11:01.836979: step 3467, loss 0.936807, acc 0.640625\n",
      "2018-12-07T17:11:03.774299: step 3468, loss 0.64583, acc 0.765625\n",
      "2018-12-07T17:11:05.449267: step 3469, loss 0.696936, acc 0.734375\n",
      "2018-12-07T17:11:07.580505: step 3470, loss 0.556801, acc 0.75\n",
      "2018-12-07T17:11:09.314470: step 3471, loss 0.729123, acc 0.71875\n",
      "2018-12-07T17:11:10.952747: step 3472, loss 0.926771, acc 0.640625\n",
      "2018-12-07T17:11:12.602686: step 3473, loss 0.663059, acc 0.71875\n",
      "2018-12-07T17:11:14.518695: step 3474, loss 0.743498, acc 0.6875\n",
      "2018-12-07T17:11:16.254338: step 3475, loss 0.844544, acc 0.625\n",
      "2018-12-07T17:11:18.223186: step 3476, loss 0.65369, acc 0.796875\n",
      "2018-12-07T17:11:19.786153: step 3477, loss 0.768847, acc 0.671875\n",
      "2018-12-07T17:11:21.582540: step 3478, loss 0.569276, acc 0.8125\n",
      "2018-12-07T17:11:23.089315: step 3479, loss 0.637474, acc 0.78125\n",
      "2018-12-07T17:11:25.291488: step 3480, loss 0.629166, acc 0.6875\n",
      "2018-12-07T17:11:27.071421: step 3481, loss 0.778456, acc 0.625\n",
      "2018-12-07T17:11:28.850736: step 3482, loss 0.544767, acc 0.828125\n",
      "2018-12-07T17:11:30.368568: step 3483, loss 0.703547, acc 0.673913\n",
      "2018-12-07T17:11:31.377581: step 3484, loss 0.554824, acc 0.78125\n",
      "2018-12-07T17:11:32.363480: step 3485, loss 0.665138, acc 0.75\n",
      "2018-12-07T17:11:33.380531: step 3486, loss 0.661133, acc 0.734375\n",
      "2018-12-07T17:11:34.398509: step 3487, loss 0.795022, acc 0.734375\n",
      "2018-12-07T17:11:35.447429: step 3488, loss 0.624917, acc 0.796875\n",
      "2018-12-07T17:11:36.573720: step 3489, loss 0.640799, acc 0.828125\n",
      "2018-12-07T17:11:37.643725: step 3490, loss 0.67059, acc 0.78125\n",
      "2018-12-07T17:11:38.910749: step 3491, loss 0.520415, acc 0.859375\n",
      "2018-12-07T17:11:39.912169: step 3492, loss 0.495468, acc 0.78125\n",
      "2018-12-07T17:11:41.094154: step 3493, loss 0.887765, acc 0.6875\n",
      "2018-12-07T17:11:42.215051: step 3494, loss 0.533178, acc 0.796875\n",
      "2018-12-07T17:11:43.197351: step 3495, loss 0.769488, acc 0.734375\n",
      "2018-12-07T17:11:44.279476: step 3496, loss 0.733449, acc 0.671875\n",
      "2018-12-07T17:11:45.356246: step 3497, loss 0.599223, acc 0.765625\n",
      "2018-12-07T17:11:46.465918: step 3498, loss 0.720778, acc 0.78125\n",
      "2018-12-07T17:11:47.540939: step 3499, loss 0.655754, acc 0.796875\n",
      "2018-12-07T17:11:48.531053: step 3500, loss 0.67291, acc 0.765625\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T17:11:49.777719: step 3500, loss 0.587504, acc 0.792\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-3500\n",
      "\n",
      "2018-12-07T17:11:51.513865: step 3501, loss 0.797923, acc 0.75\n",
      "2018-12-07T17:11:52.779124: step 3502, loss 0.599634, acc 0.796875\n",
      "2018-12-07T17:11:53.840300: step 3503, loss 0.700771, acc 0.78125\n",
      "2018-12-07T17:11:55.024707: step 3504, loss 0.675268, acc 0.75\n",
      "2018-12-07T17:11:56.347594: step 3505, loss 0.67519, acc 0.703125\n",
      "2018-12-07T17:11:57.646535: step 3506, loss 0.765756, acc 0.71875\n",
      "2018-12-07T17:11:58.665480: step 3507, loss 0.626312, acc 0.8125\n",
      "2018-12-07T17:11:59.994737: step 3508, loss 0.676145, acc 0.71875\n",
      "2018-12-07T17:12:01.454753: step 3509, loss 0.65301, acc 0.734375\n",
      "2018-12-07T17:12:02.590757: step 3510, loss 0.735872, acc 0.703125\n",
      "2018-12-07T17:12:03.653088: step 3511, loss 0.625068, acc 0.796875\n",
      "2018-12-07T17:12:04.667493: step 3512, loss 0.691866, acc 0.703125\n",
      "2018-12-07T17:12:05.714050: step 3513, loss 0.641816, acc 0.78125\n",
      "2018-12-07T17:12:06.757079: step 3514, loss 0.568824, acc 0.765625\n",
      "2018-12-07T17:12:07.922618: step 3515, loss 0.54325, acc 0.765625\n",
      "2018-12-07T17:12:09.020437: step 3516, loss 0.767383, acc 0.703125\n",
      "2018-12-07T17:12:10.069754: step 3517, loss 0.662198, acc 0.75\n",
      "2018-12-07T17:12:11.276725: step 3518, loss 0.531635, acc 0.828125\n",
      "2018-12-07T17:12:12.300056: step 3519, loss 0.610269, acc 0.78125\n",
      "2018-12-07T17:12:13.330713: step 3520, loss 0.842573, acc 0.671875\n",
      "2018-12-07T17:12:14.351223: step 3521, loss 0.585219, acc 0.828125\n",
      "2018-12-07T17:12:15.510258: step 3522, loss 0.64973, acc 0.78125\n",
      "2018-12-07T17:12:16.916252: step 3523, loss 0.664961, acc 0.734375\n",
      "2018-12-07T17:12:18.073149: step 3524, loss 0.799264, acc 0.6875\n",
      "2018-12-07T17:12:19.139958: step 3525, loss 0.708669, acc 0.734375\n",
      "2018-12-07T17:12:20.169690: step 3526, loss 0.753463, acc 0.703125\n",
      "2018-12-07T17:12:21.315016: step 3527, loss 0.677057, acc 0.75\n",
      "2018-12-07T17:12:22.345067: step 3528, loss 0.810631, acc 0.796875\n",
      "2018-12-07T17:12:23.538472: step 3529, loss 0.615108, acc 0.828125\n",
      "2018-12-07T17:12:24.693497: step 3530, loss 0.672759, acc 0.734375\n",
      "2018-12-07T17:12:25.983617: step 3531, loss 0.543979, acc 0.78125\n",
      "2018-12-07T17:12:27.170907: step 3532, loss 0.604883, acc 0.75\n",
      "2018-12-07T17:12:28.284452: step 3533, loss 0.575491, acc 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07T17:12:29.394717: step 3534, loss 0.589946, acc 0.734375\n",
      "2018-12-07T17:12:30.394193: step 3535, loss 0.729462, acc 0.75\n",
      "2018-12-07T17:12:31.621454: step 3536, loss 0.570139, acc 0.78125\n",
      "2018-12-07T17:12:32.652310: step 3537, loss 0.563468, acc 0.8125\n",
      "2018-12-07T17:12:33.738175: step 3538, loss 0.545506, acc 0.8125\n",
      "2018-12-07T17:12:35.083281: step 3539, loss 0.690312, acc 0.671875\n",
      "2018-12-07T17:12:36.091369: step 3540, loss 0.757328, acc 0.75\n",
      "2018-12-07T17:12:37.168361: step 3541, loss 0.703853, acc 0.75\n",
      "2018-12-07T17:12:38.276282: step 3542, loss 0.672489, acc 0.75\n",
      "2018-12-07T17:12:39.296347: step 3543, loss 0.52234, acc 0.84375\n",
      "2018-12-07T17:12:40.328817: step 3544, loss 0.699579, acc 0.703125\n",
      "2018-12-07T17:12:41.387029: step 3545, loss 0.667189, acc 0.78125\n",
      "2018-12-07T17:12:42.413353: step 3546, loss 0.399984, acc 0.90625\n",
      "2018-12-07T17:12:43.492927: step 3547, loss 0.566697, acc 0.78125\n",
      "2018-12-07T17:12:44.543319: step 3548, loss 0.603638, acc 0.71875\n",
      "2018-12-07T17:12:45.670699: step 3549, loss 0.658632, acc 0.703125\n",
      "2018-12-07T17:12:46.686999: step 3550, loss 0.718457, acc 0.703125\n",
      "2018-12-07T17:12:48.020458: step 3551, loss 0.54228, acc 0.75\n",
      "2018-12-07T17:12:49.215079: step 3552, loss 0.816625, acc 0.671875\n",
      "2018-12-07T17:12:50.221008: step 3553, loss 0.659094, acc 0.796875\n",
      "2018-12-07T17:12:51.275394: step 3554, loss 0.698383, acc 0.71875\n",
      "2018-12-07T17:12:52.281705: step 3555, loss 0.580431, acc 0.78125\n",
      "2018-12-07T17:12:53.291660: step 3556, loss 0.617949, acc 0.859375\n",
      "2018-12-07T17:12:54.664626: step 3557, loss 0.679589, acc 0.765625\n",
      "2018-12-07T17:12:55.807244: step 3558, loss 0.670605, acc 0.78125\n",
      "2018-12-07T17:12:56.886522: step 3559, loss 0.6773, acc 0.75\n",
      "2018-12-07T17:12:58.144250: step 3560, loss 0.575964, acc 0.75\n",
      "2018-12-07T17:12:59.381305: step 3561, loss 0.792453, acc 0.703125\n",
      "2018-12-07T17:13:00.533132: step 3562, loss 0.814506, acc 0.75\n",
      "2018-12-07T17:13:01.909940: step 3563, loss 0.612481, acc 0.75\n",
      "2018-12-07T17:13:03.137511: step 3564, loss 0.708603, acc 0.75\n",
      "2018-12-07T17:13:04.429839: step 3565, loss 0.588873, acc 0.78125\n",
      "2018-12-07T17:13:05.624191: step 3566, loss 0.64572, acc 0.75\n",
      "2018-12-07T17:13:06.623394: step 3567, loss 0.653461, acc 0.734375\n",
      "2018-12-07T17:13:07.717303: step 3568, loss 0.67016, acc 0.71875\n",
      "2018-12-07T17:13:09.210148: step 3569, loss 0.675547, acc 0.71875\n",
      "2018-12-07T17:13:10.202909: step 3570, loss 0.558781, acc 0.859375\n",
      "2018-12-07T17:13:11.338296: step 3571, loss 0.621695, acc 0.75\n",
      "2018-12-07T17:13:12.512420: step 3572, loss 0.630718, acc 0.75\n",
      "2018-12-07T17:13:14.051245: step 3573, loss 0.628372, acc 0.71875\n",
      "2018-12-07T17:13:15.121728: step 3574, loss 0.841658, acc 0.578125\n",
      "2018-12-07T17:13:16.374353: step 3575, loss 0.583806, acc 0.8125\n",
      "2018-12-07T17:13:17.508258: step 3576, loss 0.662075, acc 0.703125\n",
      "2018-12-07T17:13:18.696684: step 3577, loss 0.633431, acc 0.734375\n",
      "2018-12-07T17:13:19.764777: step 3578, loss 0.675938, acc 0.703125\n",
      "2018-12-07T17:13:21.073785: step 3579, loss 0.616178, acc 0.765625\n",
      "2018-12-07T17:13:22.076930: step 3580, loss 0.521655, acc 0.796875\n",
      "2018-12-07T17:13:23.291958: step 3581, loss 0.582735, acc 0.8125\n",
      "2018-12-07T17:13:24.472609: step 3582, loss 0.578366, acc 0.8125\n",
      "2018-12-07T17:13:25.518712: step 3583, loss 0.80845, acc 0.65625\n",
      "2018-12-07T17:13:26.545784: step 3584, loss 0.650711, acc 0.703125\n",
      "2018-12-07T17:13:27.665315: step 3585, loss 0.653819, acc 0.75\n",
      "2018-12-07T17:13:28.740374: step 3586, loss 0.583506, acc 0.75\n",
      "2018-12-07T17:13:29.827218: step 3587, loss 0.68282, acc 0.734375\n",
      "2018-12-07T17:13:30.811873: step 3588, loss 0.480078, acc 0.84375\n",
      "2018-12-07T17:13:31.856859: step 3589, loss 0.678902, acc 0.828125\n",
      "2018-12-07T17:13:32.839637: step 3590, loss 0.726609, acc 0.734375\n",
      "2018-12-07T17:13:33.879708: step 3591, loss 0.693044, acc 0.703125\n",
      "2018-12-07T17:13:35.497317: step 3592, loss 0.695652, acc 0.71875\n",
      "2018-12-07T17:13:36.547655: step 3593, loss 0.648015, acc 0.75\n",
      "2018-12-07T17:13:37.689412: step 3594, loss 0.623842, acc 0.765625\n",
      "2018-12-07T17:13:38.672678: step 3595, loss 0.649147, acc 0.78125\n",
      "2018-12-07T17:13:40.071307: step 3596, loss 0.442802, acc 0.875\n",
      "2018-12-07T17:13:41.267945: step 3597, loss 0.602906, acc 0.78125\n",
      "2018-12-07T17:13:42.421292: step 3598, loss 0.834145, acc 0.6875\n",
      "2018-12-07T17:13:43.477427: step 3599, loss 0.758977, acc 0.71875\n",
      "2018-12-07T17:13:44.829528: step 3600, loss 0.786807, acc 0.65625\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T17:13:46.074199: step 3600, loss 0.58217, acc 0.788\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-3600\n",
      "\n",
      "2018-12-07T17:13:47.931891: step 3601, loss 0.624025, acc 0.75\n",
      "2018-12-07T17:13:48.997972: step 3602, loss 0.670164, acc 0.75\n",
      "2018-12-07T17:13:50.100640: step 3603, loss 0.460914, acc 0.875\n",
      "2018-12-07T17:13:51.458716: step 3604, loss 0.762159, acc 0.6875\n",
      "2018-12-07T17:13:52.514422: step 3605, loss 0.594337, acc 0.78125\n",
      "2018-12-07T17:13:53.887945: step 3606, loss 0.676142, acc 0.75\n",
      "2018-12-07T17:13:55.097666: step 3607, loss 0.727944, acc 0.703125\n",
      "2018-12-07T17:13:56.477245: step 3608, loss 0.59968, acc 0.796875\n",
      "2018-12-07T17:13:57.745780: step 3609, loss 0.666447, acc 0.8125\n",
      "2018-12-07T17:13:59.116129: step 3610, loss 0.57021, acc 0.796875\n",
      "2018-12-07T17:14:00.477318: step 3611, loss 0.868583, acc 0.703125\n",
      "2018-12-07T17:14:01.528350: step 3612, loss 0.574669, acc 0.796875\n",
      "2018-12-07T17:14:02.769543: step 3613, loss 0.69097, acc 0.765625\n",
      "2018-12-07T17:14:03.875615: step 3614, loss 0.513864, acc 0.84375\n",
      "2018-12-07T17:14:05.343531: step 3615, loss 0.628559, acc 0.765625\n",
      "2018-12-07T17:14:06.460513: step 3616, loss 0.750128, acc 0.640625\n",
      "2018-12-07T17:14:07.697757: step 3617, loss 0.645092, acc 0.75\n",
      "2018-12-07T17:14:08.945896: step 3618, loss 0.684306, acc 0.703125\n",
      "2018-12-07T17:14:10.027937: step 3619, loss 0.637942, acc 0.78125\n",
      "2018-12-07T17:14:11.121558: step 3620, loss 0.825481, acc 0.65625\n",
      "2018-12-07T17:14:12.298318: step 3621, loss 0.577985, acc 0.765625\n",
      "2018-12-07T17:14:13.745843: step 3622, loss 0.607158, acc 0.75\n",
      "2018-12-07T17:14:14.752072: step 3623, loss 0.58078, acc 0.78125\n",
      "2018-12-07T17:14:15.807543: step 3624, loss 0.557538, acc 0.765625\n",
      "2018-12-07T17:14:16.910230: step 3625, loss 0.58432, acc 0.765625\n",
      "2018-12-07T17:14:18.245404: step 3626, loss 0.815703, acc 0.625\n",
      "2018-12-07T17:14:19.375359: step 3627, loss 0.700687, acc 0.796875\n",
      "2018-12-07T17:14:20.426628: step 3628, loss 0.556297, acc 0.8125\n",
      "2018-12-07T17:14:21.481994: step 3629, loss 0.571521, acc 0.84375\n",
      "2018-12-07T17:14:22.552824: step 3630, loss 0.665115, acc 0.71875\n",
      "2018-12-07T17:14:23.822244: step 3631, loss 0.736996, acc 0.734375\n",
      "2018-12-07T17:14:25.056624: step 3632, loss 0.648006, acc 0.765625\n",
      "2018-12-07T17:14:26.062928: step 3633, loss 0.746459, acc 0.703125\n",
      "2018-12-07T17:14:27.230405: step 3634, loss 0.668121, acc 0.703125\n",
      "2018-12-07T17:14:29.038174: step 3635, loss 0.671742, acc 0.71875\n",
      "2018-12-07T17:14:30.107316: step 3636, loss 0.625058, acc 0.765625\n",
      "2018-12-07T17:14:31.335482: step 3637, loss 0.743896, acc 0.71875\n",
      "2018-12-07T17:14:32.445502: step 3638, loss 0.738804, acc 0.734375\n",
      "2018-12-07T17:14:33.699637: step 3639, loss 0.630296, acc 0.703125\n",
      "2018-12-07T17:14:34.706815: step 3640, loss 0.527514, acc 0.8125\n",
      "2018-12-07T17:14:35.756352: step 3641, loss 0.615802, acc 0.765625\n",
      "2018-12-07T17:14:36.770007: step 3642, loss 0.742594, acc 0.734375\n",
      "2018-12-07T17:14:37.873389: step 3643, loss 0.64481, acc 0.75\n",
      "2018-12-07T17:14:39.082880: step 3644, loss 0.581271, acc 0.8125\n",
      "2018-12-07T17:14:40.117014: step 3645, loss 0.692802, acc 0.75\n",
      "2018-12-07T17:14:41.344738: step 3646, loss 0.506345, acc 0.875\n",
      "2018-12-07T17:14:42.361278: step 3647, loss 0.549008, acc 0.796875\n",
      "2018-12-07T17:14:43.667101: step 3648, loss 0.697734, acc 0.734375\n",
      "2018-12-07T17:14:44.718489: step 3649, loss 0.829286, acc 0.671875\n",
      "2018-12-07T17:14:45.998842: step 3650, loss 0.619231, acc 0.78125\n",
      "2018-12-07T17:14:47.386338: step 3651, loss 0.535504, acc 0.828125\n",
      "2018-12-07T17:14:48.762825: step 3652, loss 0.573911, acc 0.828125\n",
      "2018-12-07T17:14:49.861762: step 3653, loss 0.447109, acc 0.796875\n",
      "2018-12-07T17:14:51.222158: step 3654, loss 0.694737, acc 0.765625\n",
      "2018-12-07T17:14:52.310800: step 3655, loss 0.691487, acc 0.78125\n",
      "2018-12-07T17:14:53.547776: step 3656, loss 0.583351, acc 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07T17:14:54.901754: step 3657, loss 0.764317, acc 0.671875\n",
      "2018-12-07T17:14:56.216649: step 3658, loss 0.64463, acc 0.734375\n",
      "2018-12-07T17:14:57.408918: step 3659, loss 0.563164, acc 0.796875\n",
      "2018-12-07T17:14:58.774940: step 3660, loss 0.669868, acc 0.6875\n",
      "2018-12-07T17:14:59.854959: step 3661, loss 0.66523, acc 0.765625\n",
      "2018-12-07T17:15:01.250174: step 3662, loss 0.624489, acc 0.765625\n",
      "2018-12-07T17:15:02.611822: step 3663, loss 0.639616, acc 0.703125\n",
      "2018-12-07T17:15:03.665747: step 3664, loss 0.831593, acc 0.65625\n",
      "2018-12-07T17:15:04.841332: step 3665, loss 0.550293, acc 0.78125\n",
      "2018-12-07T17:15:06.057795: step 3666, loss 0.593595, acc 0.796875\n",
      "2018-12-07T17:15:07.234568: step 3667, loss 0.607727, acc 0.734375\n",
      "2018-12-07T17:15:08.349471: step 3668, loss 0.584476, acc 0.78125\n",
      "2018-12-07T17:15:09.881041: step 3669, loss 0.648814, acc 0.71875\n",
      "2018-12-07T17:15:10.964515: step 3670, loss 0.591075, acc 0.78125\n",
      "2018-12-07T17:15:12.185616: step 3671, loss 0.577834, acc 0.796875\n",
      "2018-12-07T17:15:13.285687: step 3672, loss 0.679055, acc 0.75\n",
      "2018-12-07T17:15:14.358220: step 3673, loss 0.764168, acc 0.6875\n",
      "2018-12-07T17:15:15.546126: step 3674, loss 0.752363, acc 0.671875\n",
      "2018-12-07T17:15:16.796296: step 3675, loss 0.612194, acc 0.734375\n",
      "2018-12-07T17:15:18.065855: step 3676, loss 0.756239, acc 0.65625\n",
      "2018-12-07T17:15:19.259443: step 3677, loss 0.694032, acc 0.703125\n",
      "2018-12-07T17:15:20.317534: step 3678, loss 0.486995, acc 0.84375\n",
      "2018-12-07T17:15:21.506391: step 3679, loss 0.776321, acc 0.6875\n",
      "2018-12-07T17:15:22.881749: step 3680, loss 0.699389, acc 0.71875\n",
      "2018-12-07T17:15:23.935418: step 3681, loss 0.541573, acc 0.75\n",
      "2018-12-07T17:15:25.028987: step 3682, loss 0.540896, acc 0.828125\n",
      "2018-12-07T17:15:26.399798: step 3683, loss 0.490236, acc 0.828125\n",
      "2018-12-07T17:15:27.595320: step 3684, loss 0.640493, acc 0.765625\n",
      "2018-12-07T17:15:29.214859: step 3685, loss 0.680979, acc 0.765625\n",
      "2018-12-07T17:15:30.253397: step 3686, loss 0.541227, acc 0.796875\n",
      "2018-12-07T17:15:31.561409: step 3687, loss 0.804411, acc 0.703125\n",
      "2018-12-07T17:15:32.652727: step 3688, loss 0.605326, acc 0.78125\n",
      "2018-12-07T17:15:33.908405: step 3689, loss 0.788485, acc 0.71875\n",
      "2018-12-07T17:15:35.413233: step 3690, loss 0.655564, acc 0.75\n",
      "2018-12-07T17:15:36.927089: step 3691, loss 0.487776, acc 0.84375\n",
      "2018-12-07T17:15:38.153838: step 3692, loss 0.610961, acc 0.765625\n",
      "2018-12-07T17:15:40.276413: step 3693, loss 0.717944, acc 0.734375\n",
      "2018-12-07T17:15:41.420277: step 3694, loss 0.575412, acc 0.765625\n",
      "2018-12-07T17:15:42.489742: step 3695, loss 0.585497, acc 0.796875\n",
      "2018-12-07T17:15:43.739052: step 3696, loss 0.584471, acc 0.78125\n",
      "2018-12-07T17:15:45.057063: step 3697, loss 0.477457, acc 0.78125\n",
      "2018-12-07T17:15:46.396629: step 3698, loss 0.602667, acc 0.765625\n",
      "2018-12-07T17:15:47.448994: step 3699, loss 0.675369, acc 0.71875\n",
      "2018-12-07T17:15:48.491221: step 3700, loss 0.720032, acc 0.6875\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T17:15:49.729909: step 3700, loss 0.589994, acc 0.796\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-3700\n",
      "\n",
      "2018-12-07T17:15:51.402226: step 3701, loss 0.76561, acc 0.75\n",
      "2018-12-07T17:15:53.001084: step 3702, loss 0.633346, acc 0.765625\n",
      "2018-12-07T17:15:54.090941: step 3703, loss 0.642812, acc 0.796875\n",
      "2018-12-07T17:15:55.388678: step 3704, loss 0.707718, acc 0.734375\n",
      "2018-12-07T17:15:56.618219: step 3705, loss 0.622471, acc 0.78125\n",
      "2018-12-07T17:15:57.939152: step 3706, loss 0.596848, acc 0.75\n",
      "2018-12-07T17:15:59.409447: step 3707, loss 0.582399, acc 0.71875\n",
      "2018-12-07T17:16:00.815508: step 3708, loss 0.725872, acc 0.78125\n",
      "2018-12-07T17:16:01.994268: step 3709, loss 0.435726, acc 0.84375\n",
      "2018-12-07T17:16:03.101037: step 3710, loss 0.500338, acc 0.8125\n",
      "2018-12-07T17:16:04.319259: step 3711, loss 0.694847, acc 0.734375\n",
      "2018-12-07T17:16:05.414608: step 3712, loss 0.588963, acc 0.796875\n",
      "2018-12-07T17:16:06.808000: step 3713, loss 0.746715, acc 0.703125\n",
      "2018-12-07T17:16:07.955274: step 3714, loss 0.74227, acc 0.671875\n",
      "2018-12-07T17:16:09.399187: step 3715, loss 0.681591, acc 0.703125\n",
      "2018-12-07T17:16:10.661178: step 3716, loss 0.615521, acc 0.78125\n",
      "2018-12-07T17:16:11.779201: step 3717, loss 0.487441, acc 0.828125\n",
      "2018-12-07T17:16:12.918920: step 3718, loss 0.842979, acc 0.703125\n",
      "2018-12-07T17:16:14.138845: step 3719, loss 0.666184, acc 0.671875\n",
      "2018-12-07T17:16:15.820087: step 3720, loss 0.583008, acc 0.75\n",
      "2018-12-07T17:16:17.357628: step 3721, loss 0.701207, acc 0.71875\n",
      "2018-12-07T17:16:18.732822: step 3722, loss 0.485713, acc 0.84375\n",
      "2018-12-07T17:16:20.270870: step 3723, loss 0.547337, acc 0.84375\n",
      "2018-12-07T17:16:21.759159: step 3724, loss 0.611464, acc 0.78125\n",
      "2018-12-07T17:16:23.109662: step 3725, loss 0.684538, acc 0.765625\n",
      "2018-12-07T17:16:24.419770: step 3726, loss 0.656752, acc 0.71875\n",
      "2018-12-07T17:16:25.511676: step 3727, loss 0.564637, acc 0.78125\n",
      "2018-12-07T17:16:26.816067: step 3728, loss 0.575462, acc 0.765625\n",
      "2018-12-07T17:16:27.964713: step 3729, loss 0.845154, acc 0.703125\n",
      "2018-12-07T17:16:29.490719: step 3730, loss 0.67751, acc 0.75\n",
      "2018-12-07T17:16:30.999552: step 3731, loss 0.638115, acc 0.78125\n",
      "2018-12-07T17:16:32.269011: step 3732, loss 0.712986, acc 0.734375\n",
      "2018-12-07T17:16:33.407650: step 3733, loss 0.681096, acc 0.734375\n",
      "2018-12-07T17:16:34.502536: step 3734, loss 0.575059, acc 0.796875\n",
      "2018-12-07T17:16:35.849998: step 3735, loss 0.697299, acc 0.703125\n",
      "2018-12-07T17:16:37.012443: step 3736, loss 0.677425, acc 0.734375\n",
      "2018-12-07T17:16:38.133719: step 3737, loss 0.8307, acc 0.734375\n",
      "2018-12-07T17:16:39.616658: step 3738, loss 0.792665, acc 0.765625\n",
      "2018-12-07T17:16:40.737800: step 3739, loss 0.672877, acc 0.71875\n",
      "2018-12-07T17:16:41.977132: step 3740, loss 0.663697, acc 0.71875\n",
      "2018-12-07T17:16:43.299486: step 3741, loss 0.782913, acc 0.640625\n",
      "2018-12-07T17:16:44.593843: step 3742, loss 0.552259, acc 0.75\n",
      "2018-12-07T17:16:46.067232: step 3743, loss 0.718138, acc 0.703125\n",
      "2018-12-07T17:16:47.381221: step 3744, loss 0.509928, acc 0.796875\n",
      "2018-12-07T17:16:48.521488: step 3745, loss 0.82237, acc 0.703125\n",
      "2018-12-07T17:16:49.879189: step 3746, loss 0.684772, acc 0.6875\n",
      "2018-12-07T17:16:51.482419: step 3747, loss 0.73085, acc 0.765625\n",
      "2018-12-07T17:16:52.744994: step 3748, loss 0.585496, acc 0.8125\n",
      "2018-12-07T17:16:54.198104: step 3749, loss 0.736694, acc 0.71875\n",
      "2018-12-07T17:16:55.464805: step 3750, loss 0.601682, acc 0.765625\n",
      "2018-12-07T17:16:56.682627: step 3751, loss 0.577608, acc 0.78125\n",
      "2018-12-07T17:16:58.114291: step 3752, loss 0.672348, acc 0.703125\n",
      "2018-12-07T17:16:59.480917: step 3753, loss 0.729938, acc 0.8125\n",
      "2018-12-07T17:17:00.787099: step 3754, loss 0.504972, acc 0.796875\n",
      "2018-12-07T17:17:02.050095: step 3755, loss 0.690549, acc 0.75\n",
      "2018-12-07T17:17:03.211164: step 3756, loss 0.651593, acc 0.71875\n",
      "2018-12-07T17:17:04.466326: step 3757, loss 0.831125, acc 0.65625\n",
      "2018-12-07T17:17:05.606227: step 3758, loss 0.66873, acc 0.78125\n",
      "2018-12-07T17:17:06.735178: step 3759, loss 0.748339, acc 0.671875\n",
      "2018-12-07T17:17:07.858766: step 3760, loss 0.857112, acc 0.65625\n",
      "2018-12-07T17:17:09.419601: step 3761, loss 0.714697, acc 0.75\n",
      "2018-12-07T17:17:10.553448: step 3762, loss 0.540637, acc 0.859375\n",
      "2018-12-07T17:17:11.740098: step 3763, loss 0.742871, acc 0.71875\n",
      "2018-12-07T17:17:13.248488: step 3764, loss 0.576163, acc 0.796875\n",
      "2018-12-07T17:17:14.597672: step 3765, loss 0.705984, acc 0.734375\n",
      "2018-12-07T17:17:15.985906: step 3766, loss 0.620603, acc 0.703125\n",
      "2018-12-07T17:17:17.459173: step 3767, loss 0.750002, acc 0.765625\n",
      "2018-12-07T17:17:18.819694: step 3768, loss 0.743873, acc 0.671875\n",
      "2018-12-07T17:17:19.989605: step 3769, loss 0.688511, acc 0.734375\n",
      "2018-12-07T17:17:21.529125: step 3770, loss 0.698634, acc 0.734375\n",
      "2018-12-07T17:17:22.762871: step 3771, loss 0.520835, acc 0.8125\n",
      "2018-12-07T17:17:24.286450: step 3772, loss 0.66882, acc 0.75\n",
      "2018-12-07T17:17:25.485687: step 3773, loss 0.567291, acc 0.8125\n",
      "2018-12-07T17:17:26.896017: step 3774, loss 0.589548, acc 0.78125\n",
      "2018-12-07T17:17:29.270838: step 3775, loss 0.598378, acc 0.765625\n",
      "2018-12-07T17:17:31.287379: step 3776, loss 0.546511, acc 0.796875\n",
      "2018-12-07T17:17:32.796921: step 3777, loss 0.604522, acc 0.796875\n",
      "2018-12-07T17:17:34.591359: step 3778, loss 0.603979, acc 0.765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07T17:17:36.089933: step 3779, loss 0.575449, acc 0.78125\n",
      "2018-12-07T17:17:37.917072: step 3780, loss 0.713827, acc 0.734375\n",
      "2018-12-07T17:17:39.397058: step 3781, loss 0.695843, acc 0.75\n",
      "2018-12-07T17:17:40.935274: step 3782, loss 0.663774, acc 0.75\n",
      "2018-12-07T17:17:42.328031: step 3783, loss 0.616439, acc 0.71875\n",
      "2018-12-07T17:17:43.462773: step 3784, loss 0.661896, acc 0.75\n",
      "2018-12-07T17:17:44.716545: step 3785, loss 0.668068, acc 0.71875\n",
      "2018-12-07T17:17:46.061256: step 3786, loss 0.68344, acc 0.734375\n",
      "2018-12-07T17:17:47.577468: step 3787, loss 0.768148, acc 0.71875\n",
      "2018-12-07T17:17:48.977200: step 3788, loss 0.812513, acc 0.6875\n",
      "2018-12-07T17:17:50.355928: step 3789, loss 0.677032, acc 0.734375\n",
      "2018-12-07T17:17:52.057364: step 3790, loss 0.530892, acc 0.84375\n",
      "2018-12-07T17:17:53.492901: step 3791, loss 0.63235, acc 0.703125\n",
      "2018-12-07T17:17:54.674801: step 3792, loss 0.653669, acc 0.71875\n",
      "2018-12-07T17:17:56.169338: step 3793, loss 0.673092, acc 0.734375\n",
      "2018-12-07T17:17:57.516137: step 3794, loss 0.623803, acc 0.71875\n",
      "2018-12-07T17:17:58.911815: step 3795, loss 0.784732, acc 0.65625\n",
      "2018-12-07T17:18:00.405848: step 3796, loss 0.609763, acc 0.765625\n",
      "2018-12-07T17:18:01.920107: step 3797, loss 0.589612, acc 0.796875\n",
      "2018-12-07T17:18:03.274480: step 3798, loss 0.795692, acc 0.65625\n",
      "2018-12-07T17:18:04.840977: step 3799, loss 0.713826, acc 0.6875\n",
      "2018-12-07T17:18:06.221920: step 3800, loss 0.617551, acc 0.78125\n",
      "\n",
      "Evaluation:\n",
      "2018-12-07T17:18:07.462603: step 3800, loss 0.552401, acc 0.808\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\hooke\\Desktop\\ecen765_Project\\runs\\1544219414\\checkpoints\\model-3800\n",
      "\n",
      "2018-12-07T17:18:09.723865: step 3801, loss 0.999515, acc 0.625\n",
      "2018-12-07T17:18:11.372672: step 3802, loss 0.601159, acc 0.78125\n",
      "2018-12-07T17:18:12.957134: step 3803, loss 0.693147, acc 0.703125\n",
      "2018-12-07T17:18:14.526582: step 3804, loss 0.423822, acc 0.8125\n",
      "2018-12-07T17:18:16.198480: step 3805, loss 0.673008, acc 0.75\n",
      "2018-12-07T17:18:17.819991: step 3806, loss 0.751979, acc 0.765625\n",
      "2018-12-07T17:18:19.587235: step 3807, loss 0.66749, acc 0.765625\n",
      "2018-12-07T17:18:21.026978: step 3808, loss 0.614697, acc 0.71875\n",
      "2018-12-07T17:18:22.599375: step 3809, loss 0.83434, acc 0.671875\n",
      "2018-12-07T17:18:24.363166: step 3810, loss 0.608714, acc 0.796875\n",
      "2018-12-07T17:18:26.755547: step 3811, loss 0.640139, acc 0.796875\n",
      "2018-12-07T17:18:28.424052: step 3812, loss 0.465672, acc 0.84375\n",
      "2018-12-07T17:18:30.104123: step 3813, loss 0.525534, acc 0.796875\n",
      "2018-12-07T17:18:31.590878: step 3814, loss 0.636205, acc 0.734375\n",
      "2018-12-07T17:18:33.333882: step 3815, loss 0.677098, acc 0.765625\n",
      "2018-12-07T17:18:35.075964: step 3816, loss 0.629123, acc 0.75\n",
      "2018-12-07T17:18:36.947544: step 3817, loss 0.819679, acc 0.671875\n",
      "2018-12-07T17:18:38.496321: step 3818, loss 0.67207, acc 0.78125\n",
      "2018-12-07T17:18:40.006671: step 3819, loss 0.523869, acc 0.8125\n",
      "2018-12-07T17:18:41.678793: step 3820, loss 0.671296, acc 0.75\n",
      "2018-12-07T17:18:43.464843: step 3821, loss 0.684348, acc 0.6875\n",
      "2018-12-07T17:18:45.184840: step 3822, loss 0.534797, acc 0.875\n",
      "2018-12-07T17:18:47.227693: step 3823, loss 0.690074, acc 0.75\n",
      "2018-12-07T17:18:48.884566: step 3824, loss 0.537468, acc 0.796875\n",
      "2018-12-07T17:18:50.496476: step 3825, loss 0.733307, acc 0.703125\n",
      "2018-12-07T17:18:52.220218: step 3826, loss 0.938993, acc 0.703125\n",
      "2018-12-07T17:18:54.111171: step 3827, loss 0.704161, acc 0.765625\n",
      "2018-12-07T17:18:55.920882: step 3828, loss 0.749245, acc 0.78125\n",
      "2018-12-07T17:18:57.614071: step 3829, loss 0.612742, acc 0.78125\n",
      "2018-12-07T17:18:59.393806: step 3830, loss 0.634899, acc 0.734375\n",
      "2018-12-07T17:19:01.003707: step 3831, loss 0.592508, acc 0.796875\n",
      "2018-12-07T17:19:02.596848: step 3832, loss 0.788219, acc 0.640625\n",
      "2018-12-07T17:19:04.174025: step 3833, loss 0.66037, acc 0.71875\n",
      "2018-12-07T17:19:05.774092: step 3834, loss 0.670806, acc 0.71875\n",
      "2018-12-07T17:19:07.495439: step 3835, loss 0.597138, acc 0.765625\n",
      "2018-12-07T17:19:09.693575: step 3836, loss 0.667711, acc 0.78125\n",
      "2018-12-07T17:19:11.100264: step 3837, loss 0.855308, acc 0.625\n",
      "2018-12-07T17:19:12.937173: step 3838, loss 0.546753, acc 0.765625\n",
      "2018-12-07T17:19:14.562689: step 3839, loss 0.819808, acc 0.6875\n",
      "2018-12-07T17:19:16.346690: step 3840, loss 0.548473, acc 0.765625\n",
      "2018-12-07T17:19:18.003932: step 3841, loss 0.612378, acc 0.828125\n",
      "2018-12-07T17:19:19.761811: step 3842, loss 0.71542, acc 0.640625\n",
      "2018-12-07T17:19:21.684632: step 3843, loss 0.604182, acc 0.703125\n",
      "2018-12-07T17:19:23.193489: step 3844, loss 0.642518, acc 0.78125\n",
      "2018-12-07T17:19:24.587010: step 3845, loss 0.753412, acc 0.671875\n",
      "2018-12-07T17:19:26.212947: step 3846, loss 0.605669, acc 0.75\n",
      "2018-12-07T17:19:27.645025: step 3847, loss 0.839791, acc 0.6875\n",
      "2018-12-07T17:19:29.297787: step 3848, loss 0.782587, acc 0.71875\n",
      "2018-12-07T17:19:31.142614: step 3849, loss 0.743496, acc 0.765625\n",
      "2018-12-07T17:19:32.834902: step 3850, loss 0.67509, acc 0.734375\n",
      "2018-12-07T17:19:34.448400: step 3851, loss 0.805688, acc 0.65625\n",
      "2018-12-07T17:19:35.990325: step 3852, loss 0.768399, acc 0.703125\n",
      "2018-12-07T17:19:38.000355: step 3853, loss 0.552754, acc 0.8125\n",
      "2018-12-07T17:19:39.856429: step 3854, loss 0.762853, acc 0.671875\n",
      "2018-12-07T17:19:41.628669: step 3855, loss 0.569398, acc 0.78125\n",
      "2018-12-07T17:19:43.285353: step 3856, loss 0.717888, acc 0.71875\n",
      "2018-12-07T17:19:44.786084: step 3857, loss 0.690562, acc 0.671875\n",
      "2018-12-07T17:19:46.777865: step 3858, loss 0.553818, acc 0.78125\n",
      "2018-12-07T17:19:48.468391: step 3859, loss 0.771404, acc 0.75\n",
      "2018-12-07T17:19:50.187246: step 3860, loss 0.57057, acc 0.796875\n",
      "2018-12-07T17:19:51.895673: step 3861, loss 0.76038, acc 0.6875\n",
      "2018-12-07T17:19:54.025950: step 3862, loss 0.604485, acc 0.71875\n",
      "2018-12-07T17:19:55.874590: step 3863, loss 0.647278, acc 0.734375\n",
      "2018-12-07T17:19:57.383879: step 3864, loss 0.604067, acc 0.734375\n",
      "2018-12-07T17:19:59.385756: step 3865, loss 0.580225, acc 0.75\n",
      "2018-12-07T17:20:01.149397: step 3866, loss 0.63908, acc 0.71875\n",
      "2018-12-07T17:20:02.885278: step 3867, loss 0.614352, acc 0.765625\n",
      "2018-12-07T17:20:04.885419: step 3868, loss 0.534634, acc 0.78125\n",
      "2018-12-07T17:20:06.656497: step 3869, loss 0.558528, acc 0.796875\n",
      "2018-12-07T17:20:08.143457: step 3870, loss 0.661447, acc 0.782609\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "testFile = \"test.txt\"\n",
    "trainFile = \"train.txt\"\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "\n",
    "    with sess.as_default():\n",
    "        cnn = CNN()\n",
    "\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        \n",
    "        # optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # record the gradient values\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # the output directory to save 1)summaries and 2)models/checkpoints\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "        \n",
    "        # 1)save the summary\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        test_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        test_summary_dir = os.path.join(out_dir, \"summaries\", \"test\")\n",
    "        test_summary_writer = tf.summary.FileWriter(test_summary_dir, sess.graph)\n",
    "\n",
    "        # 2)save the checkpoint\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
    "\n",
    "        \n",
    "        # Initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        file1 = open(testFile, 'w') \n",
    "        file2 = open(trainFile, 'w')\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iterator(list(zip(x_train, y_train)), batch_size, num_epochs)\n",
    "        \n",
    "        # for each batch, train\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            \n",
    "            if current_step % eval_interval == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                test_step(x_test, y_test, writer=test_summary_writer)\n",
    "                print(\"\")\n",
    "                \n",
    "            if current_step % checkpoint_interval == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "\n",
    "        file1.close()\n",
    "        file2.close()\n",
    "\n",
    "print(\"finish\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
